---
layout: post
title:  "keras to pytorch"
date:   2020-01-12 10:05:05 +0900
tag: [pytorch]
comments: true
---
# 케라스에서 파이토치로
- 같이 공부하는분들이 대부분 케라스로 시작하셨기에 곧 쓸일이 많이 생길 것 같아 정리해둘겸 겸사겸사 만듬
- 기본적으로 Keras에 비해 파이썬 공부가 더 필요하다...
     

# 처음 접할때 느끼는 차이점
- 불편하다
  - Keras에선 compile, fit 으로 쉽게 사용하던 부분을 전부 직접 코딩해야함
    - 미니배치나 에폭당 결과 출력 메시지 등도 전부 직접 해야됨
    - 그래서 케라스는 신경망의 구조를 잘몰라도 돌리는건 가능하지만 pytorch의 경우 조금 어려움
  - CPU, GPU 사용을 할당하는것도 직접 해야함
    - 모델 중간에서의 텐서 생성 등에서 주의가 필요
  - conv 연산방식이 기본적으로 b, c, h, w 의 channel first 방식을 사용해 불편
    
- 좋아보인다
  - 매우 직관적인 구조를 가짐
    - keras의 경우 생략되고 자동으로 처리하는 부분이많은데 반해 실제 모델이 돌아가면서 해야할 일을 직접 코딩하는 느낌이 듬
    - 예전 tf 1.x 대의 경우 효율을 위해 해야하는 작업들(Session)이 있었으나 pytorch에서는 대부분 내부적으로 전부 해결하고 모델 설계에 필요한부분 위주의 코딩을 사용
  - GPU, CPU의 이동이 자유로움
    - 구조가 잘 짜여져 있다면 torch.cuda.is_available() 을 사용해 gpu 유무를 확인하고 모듈 전체를 이동시킬수 있음


# 필수 클래스
- Tensor 클래스
  - torch.empty, .randn, .ones, .full, .from_numpy 등으로 생성 가능
  - Numpy에서 지원되는 기본적인 연산들은 거의 전부 사용가능 
    - 함수명도 거의 동일해 그대로 사용할 수 있음
    - 생각나는 차이점
      - max 함수가 (value, index) 튜플형식으로 같이 반환해 argmax가 없음
      - 다차원 Transpose의 경우 permute 라는 함수를 사용 (Transpose는 2차원만)
  - .cuda(), .cpu()나 .to(device) 를 이용해 cpu와 gpu사이에서 텐서를 전송가능

- torch.nn.Module 클래스
   - 케라스의 layer와 model을 모두 포함하는 개념
   - 기본적으로 Torch에선 레이어, 모델, 손실함수 등 전부 이 클래스를 상속받아 재정의 하여 생성
   - print() 로 출력시 내부 모듈을 전부 출력해줌
   - 예시
        ```
        class MyModel(nn.Module) :
            def __init__(self) :
                super(MyModel, self).__init__()
                self.fcl_1 = nn.Linear(10, 10)
                ...

            def forward(self, x) :
                ...
                return y

        ```
      - init에 self에 변수로 선언된 레이어에 한해서만 iteration 실행
        - parameters 등 대부분의 메소드가 반복자에 의해 실행되므로 가중치가 있는 레이어는 반드시 선언 후 사용
        - 비슷한 레이어 여러개를 선언할때는 nn.ModuleList 혹은 nn.ModuleDict 를 사용하면 편함(포함된 모든 레이어가 선언된것처럼 작동)
        - 가중치가 존재하지 않는 레이어는 굳이 미리 선언할 필요는 없음
          - 기존 소스에는 Functional F 를 선언해서 하는 소스가많은데 추천하지 않는방식이라 함

    - loss 함수도 일반 모델처럼 모듈로 생성
      - mse, crossentropy 등 기본적인 loss는 제공됨 
      - 모델의 출력과 Label을 입력으로받아 forward 하고 나온 결과 텐서에 backward를 실시 하여 연결된 모듈 전체에 기울기를 계산(업데이트는 하지않음)
      - 당연히 inference엔 사용되지 않으므로 모델과 분리하여 생성
      - 예시
        ```
        criterion = torch.nn.MSELoss()
        loss = criterion(y_pred, y_true)
        loss.backward()
        ```
      - 참고로 기록을 위해 loss 를 저장할떄는 꼭 detach 한 후에 저장해야함

- torch.optim 클래스
  - 옵티마이저를 생성하는 클래스
  - 사실 일반적으로 옵티마이저는 네트워크와 관련없이 각파라미터에 변화기록만 있으면 되기에 선언시 파라미터만 연결하고 step 함수를 이용해 업데이트 실시
    - 파라미터를 연결해야하므로 모델 생성 후 선언해야함
  - 기존 이력이 남아있을 수 있으니 새 학습시 zero_grad()를 호출해 이력 삭제
  - 예시
    ```
    # SGD 옵티마이저 생성
    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)
    optimizer.zero_grad()
    optimizer.step()
    ```



# 그래서 간단히 하나 만들어보자
```

# 모듈 임포트
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# 모델 생성
class FCLModel(nn.Module) :
    def __init__(self) :
        super(FCLModel, self).__init__()
        
        # FCL 2층 후 분류기
        self.linear_1 = nn.Linear(784, 128)
        self.linear_2 = nn.Linear(128, 128)
        self.out = nn.Linear(128, 10)

    # 여기에서 네트워크의 순서를 정의    
    def forward(self, x) :

        x = self.linear_1(x)
        x = nn.ReLU()(x)
        x = self.linear_2(x)
        x = nn.ReLU()(x)
        y = self.out(x)
        return y


# 실행부분 정의
def main() :

    # 모델생성
    model = FCLModel()

    # 손실모듈 생성 pytorch의 CrossEntropy는 log softmax를 포함
    criterion = nn.CrossEntropyLoss()

    # 옵티마이저 생성
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # 임시 랜덤 데이터 생성
    x_train = torch.randn((5, 784), dtype=torch.float32)
    y_train = torch.randint(10, size =(5,))

    y_pred = model(x_train)
    
    # loss 계산 (원핫이 아닌 그냥 레이블을 집어넣음)
    loss = criterion(y_pred, y_train)

    # 역전파 계산 (내부에 지정된 grad_fn 사용)
    loss.backward()

    # 옵티마이저 초기화 및 업데이트
    optimizer.zero_grad()
    optimizer.step()

    # 그냥 놔두면 썰렁하니 출력
    print(loss)

```

- 모델 자체를 만드는것은 비교적 간단하다. 이제 어려운 학습시키는 쪽으로 가보자


# DataLoader

- 이부분을 따로 뗀 이유는 조금 복잡하다(내생각엔 모델구축보다 복잡하다)
- 데이터를 불러올때 사용하고 torch.utils.data 안에 들어있다
- 기본구조 (구조를 먼저 보는게 이해가 쉬울듯 했다)
  ```
  from torch.utils.data import Dataset, DataLoader

  # 데이터 불러오는 클래스 정의
  class MyDataSet(Dataset) : 
    # 여기서 데이터를 불러오거나 데이터 불러오기 위한 준비를한다
    def __init__(self) :
      ...

    # 여기서 인덱스와 데이터를 연결해서 반환해줌
    def __getitem__(self, index) :
      ...
      return x, y

    # 데이터 전체 길이 반환하는 함수 정의
    def __len__(self) :
      length = ...
      return length

  # 위에서 정의한 클래스 객체 생성
  dataset = MyDataSet()

  # 데이터셋과 연결된 Loader 생성
  train_loader = DataLoader(dataset = dataset, 
                            batch_size = 32, 
                            shuffle = True, 
                            num_workers = 0)

  # 학습
  for epoch in range(1, epochs+1) :
    for i, (input, label) in enumerate(train_loader) :
      ...

  ```
- DataSet
  - 이 클래스를 상속받아 데이터를 불러오는 클래스를 생성한다
  - 데이터가 적을떼(메모리에 전부올리기 무리가 없을때)
    - init 에서 데이터를 전부 불러와서 인덱스만 넣으면 값을 가져올수 있도록 설정하고 전체 길이를 변수로 저장해둠
    - getitem 에서는 그저 인덱스에 해당하는 자료 반환만 함
    - len 에서는 위에서 저장한 길이를 출력
  
  - 데이터가 클때
    - init 에서 전부 불러오는게 불가능하므로 패스 설정같은 데이터를 불러오기 위한 준비만 시행, 전체크기를 파악해 저장해둠
    - getitem 에서 인덱스에 해당하는 데이터를 파일에서 직접 불러오고 전처리하여 만들어 반환
    - len 에서는 마찬가지로 저장해둔 전체 길이 출력
  - 생성된 데이터는 DataLoader를 통해 불러오게 됨
  - 참고로 torchvision에 들어있는 데이터셋은 대부분 이방식으로 생성되어 있음

- Dataloader
  - 데이터를 불러오는 generator
  - 위에서 생성한 데이터셋에서 데이터를 가져오는 방식을 설정
    - batch_size, shuffle 등을 설정하면 알아서 데이터를 뽑아와줌
  - 반환값은 위에 정의한 getitem의 반환값으로 같이 x, y 순으로 반환
  - 마지막 반환값의 배치는 len 을 batch로 나눈 나머지값이되니 주의
  - 보통 for 문을 통해 반복시키나 iterator를 만들고 next를 사용하여 수동으로 가져올 수도 있음
