---
layout: post
title:  "keras to pytorch"
date:   2020-01-12 10:05:05 +0900
tag: [pytorch]
comments: true
---
# ???
- 어쩌다보니 만들게 된 페이지
   
     



# 차이점
- 불편하다
  - Keras에선 compile, fit 으로 쉽게 사용하던 부분을 전부 직접 코딩해야함
    - 미니배치나 에폭당 결과 출력 메시지 등도 전부 직접 해야됨
    - 그래서 케라스는 신경망의 구조를 잘몰라도 돌리는건 가능하지만 pytorch의 경우 조금 어려움
  - CPU, GPU 사용을 할당하는것도 직접 해야함
    - 모델 중간에서의 텐서 생성 등에서 주의가 필요
  - conv 연산방식이 기본적으로 b, c, h, w 의 channel first 방식을 사용해 불편
    
- 강력하다
  - 매우 직관적인 구조를 가짐
    - keras의 경우 생략되고 자동으로 처리하는 부분이많은데 반해 실제 모델이 돌아가면서 해야할 일을 직접 코딩하는 느낌이 듬
    - 예전 tf 1.x 대의 경우 효율을 위해 해야하는 작업들(Session)이 있었으나 pytorch에서는 대부분 내부적으로 전부 해결하고 모델 설계에 필요한부분 위주의 코딩을 사용
  - GPU, CPU의 이동이 자유로움
    - 구조가 잘 짜여져 있다면 torch.cuda.is_available() 을 사용해 gpu 유무를 확인하고 모듈 전체를 이동시킬수 있음


# 필수 클래스
- Tensor 클래스
  - torch.empty, .randn, .ones, .full, .from_numpy 등으로 생성 가능
  - 기본적인 add, 
  - .cuda(), .cpu()나 .to(device) 를 이용해 cpu와 gpu사이에서 텐서를 전송가능

- torch.nn.Module 클래스
   - 케라스의 layer와 model을 모두 포함하는 개념
   - 기본적으로 Torch에선 레이어, 모델, 손실함수 등 전부 이 클래스를 상속받아 재정의 하여 생성
   - print() 로 출력시 내부 모듈을 전부 출력해줌
   - 예시
        ```
        class MyModel(nn.Module) :
            def __init__(self) :
                super(MyModel, self).__init__()
                self.fcl_1 = nn.Linear(10, 10)
                ...

            def forward(self, x) :
                ...
                return y

        ```
      - init에 self에 변수로 선언된 레이어에 한해서만 iteration 실행
        - parameters 등 대부분의 메소드가 반복자에 의해 실행되므로 가중치가 있는 레이어는 반드시 선언 후 사용
        - 비슷한 레이어 여러개를 선언할때는 nn.ModuleList 혹은 nn.ModuleDict 를 사용하면 편함(포함된 모든 레이어가 선언된것처럼 작동)
        - 가중치가 존재하지 않는 레이어는 굳이 미리 선언할 필요는 없음
          - 기존 소스에는 Functional F 를 선언해서 하는 소스가많은데 추천하지 않는방식이라 함

    - loss 함수도 일반 모델처럼 모듈로 생성
      - mse, crossentropy 등 기본적인 loss는 제공됨 
      - 모델의 출력과 Label을 입력으로받아 forward 하고 나온 결과 텐서에 backward를 실시 하여 연결된 모듈 전체에 기울기를 계산(업데이트는 하지않음)
      - 당연히 inference엔 사용되지 않으므로 모델과 분리하여 생성
      - 예시
        ```
        criterion = torch.nn.MSELoss()
        loss = criterion(y_pred, y_true)
        loss.backward()
        ```

- torch.optim 클래스
  - 옵티마이저를 생성하는 클래스
  - 사실 일반적으로 옵티마이저는 네트워크와 관련없이 각파라미터에 변화기록만 있으면 되기에 선언시 파라미터만 연결하고 step 함수를 이용해 업데이트 실시
    - 파라미터를 연결해야하므로 모델 생성 후 선언해야함
  - 기존 이력이 남아있을 수 있으니 새 학습시 zero_grad()를 호출해 이력 삭제
  - 예시
    ```
    # SGD 옵티마이저 생성
    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)
    optimizer.zero_grad()
    optimizer.step()
    ```

# 그래서 간단히 하나 만들어보자
```

# 모듈 임포트
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# 모델 생성
class FCLModel(nn.Module) :
    def __init__(self) :
        super(FCLModel, self).__init__()
        
        # FCL 2층 후 분류기
        self.linear_1 = nn.Linear(784, 128)
        self.linear_2 = nn.Linear(128, 128)
        self.out = nn.Linear(128, 10)

    # 여기에서 네트워크의 순서를 정의    
    def forward(self, x) :

        x = self.linear_1(x)
        x = nn.ReLU()(x)
        x = self.linear_2(x)
        x = nn.ReLU()(x)
        y = self.out(x)
        return y


# 실행부분 정의
def main() :

    # 모델생성
    model = FCLModel()

    # 손실모듈 생성 pytorch의 CrossEntropy는 log softmax를 포함
    criterion = nn.CrossEntropyLoss()

    # 옵티마이저 생성
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # 임시 랜덤 데이터 생성
    x_train = torch.randn((5, 784), dtype=torch.float32)
    y_train = torch.randint(10, size =(5,))

    y_pred = model(x_train)
    
    # loss 계산 (원핫이 아닌 그냥 레이블을 집어넣음)
    loss = criterion(y_pred, y_train)

    # 역전파 계산 (내부에 지정된 grad_fn 사용)
    loss.backward()

    # 옵티마이저 초기화 및 업데이트
    optimizer.zero_grad()
    optimizer.step()

    # 그냥 놔두면 썰렁하니 출력
    print(loss)

```