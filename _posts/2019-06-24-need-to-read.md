---
layout: post
title:  "need to read"
categories: paper
date : 2019-06-24 10:05:05 +0900
comments: true
---




## BERT : Bidirectinal Encoder Representations from Transformer

이분 블로그를 참조 
감사합니다 ㅠ  

https://mino-park7.github.io/nlp/2018/12/12/bert-%EB%85%BC%EB%AC%B8%EC%A0%95%EB%A6%AC/?fbclid=IwAR3S-8iLWEVG6FGUVxoYdwQyA-zG0GpOUzVEsFBd0ARFg4eFXqCyGLznu7w






## Attention is all you need 

트랜스포머 구조 




##Constrastive Predictive coding
[https://arxiv.org/abs/1807.03748]

일단 초록만 써놓고 BERT 다음에 봐야겠습니다 ㅠㅠ
아 할게 너무많 ㅠ

### abstract

지도학습이 많은 진보가 있는동안, 비지도학습은 광범위하게 적용할수 있는것을 보이지 못했다
그리고 인공지능을 위한 중요하고 도전적인 노력이 계속되고 있다
이 작업에선 고차원 데이터에서 유용한 표현을 추출 할 수있는 universal 한 비지도 학습을 제안한다
Constrastive Predictive Coding 이라 불리는

우리 모델의 키 인사이트는 배우는것 
강력한 자귀회귀 모델을 이용하여 잠재공간에서 미래를 예측하는 표현을 배우는것이다
우리는 확률적 대조 손실을 사용한다
잠재공간에서 미래의 샘플을 예측하기 가장 유용한 정보를 캡쳐하기 위한 잠재공간을 유도하는
또한 모델을 네거티브 샘플링을 사용하기 쉽게 만든다
대부분 앞의 작업은 특별한 양식을 위한 표현을 평가하는데 초점을 맞췄으나
우리는 보였다 우리의 접근이 4가지 도메인에서 강력한 퍼포먼스를 달성한 유용한 표현을 배울 수 있는것을
말하기, 이미지, 텍스트, 그리고 3차원 환경에서의 강화학습 분야에서.




## Negative Sampling