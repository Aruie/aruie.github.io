---
layout: post
title:  "1-2weeks"
categories: "GoogleCloudTraining"
---

# 2 Weeks - Creating a dataset

## Build the model

모델을 생성해봅시다

우린 머신러닝을 이용해 모델을 생성해왔다

tf.estimator는 고수준 api 이다

어떻게 작동하는지 보자면

모델을 셋업
1. 분류인지 회귀인지 확인
2. 라벨이 무엇인지 확인
3. 피쳐가 무엇인지 확인

모델 실행
1. 모델학습
2. 모델평가
3. 모델을 이용하여 예측


## Create a sample dataset
```
datalab create mydatalabvm --zone <존>
```

항상 이걸로 랩을 생성
한번만 만들면 되는듯 하나... 이건 테스트니 계속만든다

텐서플로우를 임포트
사용할 칼럼을 디파인


###범주형 변수의 처리
칼럼의 단어를 알고있을때
```
tf.feature_column.categorical_column_with_vocabulary_list( 'zipcode', vocabulary_list = ['1','2','3','4'])
```
칼럼이 이미 인덱싱 되어있을때 ( 0 ~ N )
```
tf.feature_column.categorical_column_with_identity( 
    '열이름', num_buckets = N)
```
카테고릭칼럼을 DNN에 넣기 위해선 원핫 인코딩을 해야됨
```
tf.feature_column.indicator_column( 칼럼명 )

```

```
CSV_COLUMNS = ['aa','bb','cc']
LABEL_COLUMN = 'amount'
DEFAULTS = [[0.0], ['na'], [0.0]]

def read_dataset(filename, mode, batch_size = 512) :
    def decode_csv(value_column) : 
        columns = tf.decode_csv(value_column,
                                record_defaults=DEFAULTS)
        features = dict(zip(CSV_COLUMNS, columns))
        label = features.pop(LABEL_COLUMN)
        return features, label
    
    dataset = tf.data.TextLineDataset(filename).map(decode_csv)

    return

흠 진짜...

## 넓고 깊은 복합모델을 사용
```
tf.estimator.DNNLinearCombinedClassifier(
    model_dir = ...,
    linear_feature_columns = ...,
    dnn_feature_columns = deep_columns,
    dnn_hidden_units = [100, 50]
)
```

알아둘 함수들
```
tf.decode_csv()
tf.gfile.Glob()
tf.data.TextLineDataset().map()


    