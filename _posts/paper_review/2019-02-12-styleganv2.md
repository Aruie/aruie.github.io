---
layout: post
title:  "Analyzing and Improving the Image Quality of StyleGAN"
tag : "Paper Review"
date : 2020-02-23 10:05:05 +0900
comments: true
---


- GAN에 관심을 갖게된 계기인 StyleGAN의 업그레이드 논문이다
- 안볼수가 없..




# 기존 StyleGAN
- 


# Abstract
- StyleGAN은 현재 Unconditional Image Generator 에서 SOTA를 내고 있으나 결과물에서 이상한 Characteristic Artifacts가 생성되어 이를 분석하고 이를 해결할 방법을 제안
- Latant 에서 이미지로의 맵핑성능 향상을 위해 Generator를 재설계
  - Nomalization, Revisit Progressive Growing, Regularize
  - 이미지 품질 뿐만 아니라 Generator의 Invert를 매우 쉬워지게 해 네트워크의 어느 부분으로 이미지가 생성되는지 신뢰도있는 탐지가 가능해짐
  - 또한 Generator의 출력 해상도와 용량의 관계를 시각화
- 기존의 metrics와 이미지 품질면에서 모두 SOTA를 달성

# 1. Introduction
- StyleGAN과 기존 GAN과의 차별점
  - Latent Z 를 인풋으로 넣는것이 아닌 맵핑네트워크 를 통해 w 로 변환하고 AdaIN을 통해 Generator의 레이어의 스타일을 주입
  - 추가로 Generator 에 Random Noise 를 추가해 확률적 변동을 조정 가능

- Characteristic Artifacts
  - 많은 관찰에서 Blob-like(물방울같은 덩어리)한 Artifact가 생성되는 것을 관찰
    - 원인은 구조 설계상의 결함을 피하기 위해 이것을 생성하는것을발견
    - Normalization를 재설계하여 제거
  - Progressive Growing 에의한 문제
    - 고해상도 이미지 생성에서 큰 성과를 주었던 방법이나 이또한 문제가 발생
    - 네트워크 구조를 변경하지 않고도 같은 목표를 학습하는 구조를 제안
      - 기존과 같이 저해상도 이미지에서 점차적으로 초점을 이동
    - 생성된 이미지의 해상도를 추론 용량증가에 동기를 부여???????????? (4절보고 옵시다)
- Metrics
  - FID : Inception의 특정 공간에서의 분포 차이 측정
  - P&R : 훈련데이터와 유사한 생성이미지의 백분율?????????? (뒤에봅시다)
  - Perceptual Path Length (PPL) : 잠재공간 보간 품질 측정기법???
    - 이것이 형태의 안정성과 일관성에 상관관계가 있음을 관찰함
  - 이것들을 바탕으로 네트워크를 정규화 하여 부드러운 맵핑과 명확한 퀄리티 개선을 달성
    - 계산 비용 감소를 위해 정규화를 적게 실행하는 방법을 제안
- PathLength Regularized Generator 방식의 이점
  - 이방식이 기존 StyleGAN보다 이미지에서 W로의 투영이 매우 잘됨
  - 특정 생성기가 이미지를 생성한것을 신뢰할수있게해줌

- https://github.com/NVlabs/stylegan2
- https://www.youtube.com/watch?v=c-NJtV9Jvp0&feature=youtu.be

![Table1](/assets/post/200223/01.png)
# 2. Removing normalization artifacts
- Blob-like Artifacts
  - 생성된 대부분의 이미지는 물방울같은 모양의 blob 한 Artitacts가 관찰됨
    - 최종 이미지에에는 잘안보이더라도 중간 피쳐맵에서는 항상 존재
    - 64x64 해상도를 중심으로 나타나기 시작하고 점점 강해짐
    - Discriminator가 저것을 판별하게 해야하나 방법이 쉽지않음
  - Generator가 의도적으로 특별 부위에 강한 값을 준것이라 가정
    - 통계적으로 큰 값을 부여 함으로 전체적인 결과를 조정하기 쉬워짐
    - 실제로 정규화 단계를 삭제하자 현상이 완전히 사라짐

![Table1](/assets/post/200223/02.png)
## 2.1 Generator architecture revisited
- Revised normalization
  - 기존의 AdaIN에서 평균을 제거하고 표준편차 만을 사용
    - 결과적으로 표준편차만으로도 충분하다는것을 알게됨
  - bias와 Noise를 Block 외부로 빼서 Style과 Noise와의 영향력을 독립시킴
    - 기존엔 Noise의 영향력이 Style의 크기에 반비례했음
    - 이 방법으로 좀더 Noise에 변화에 의한 결과를 예측가능해짐
  - 이 방법을 사용해 단점없이 안전하게 제거함
    - 품질 Metrics 에선 차이가 없거나 약간의 긍정적인 효과를 줌
## 2.2 Instance normalization revisited
- Instance Normalization의 문제점
  - 너무 강하게 적용될 수 있어 완화할 방법이 필요
  - 하지만 Batch normalization은 고해상도 이미지 합성엔 맞지 않음
  - 단순히 Nomalization을 제거하는것 도 가능하나 이건 좀...
  - 실제 관찰한 결과 Scale별 효과보단 누적이 된상태로 적용되어 제어 가능성을 잃어버림
- 재구성된 구조
  - 피쳐맵의 정보를 기반으로 하나 명시적인 강제력을 적용하자는 아이디어
  - Modulation 
    - $w^{\prime}_{ijk} = s_i\sdot w_{ijk}$
      - $w$ : 기존가중치, $w^\prime$ : Modulation 이후 가중치
      - $i$ : 입력피쳐맵번호, $j$ : 출력 피쳐맵 번호, $k$ : 피쳐맵내 가중치들
  - Demodulation
    - 표준편차가 1인 입력값이 $w^\prime$를 통과한 후의 표준편차
      - $\sigma_j = \sqrt{\underset{i,k}{\sum} {w^\prime_{i,j,k}}^2 }$
      - 결론은 단위 표준편차로 만들기 위해선 가중치를 위 식으로 그냥 나누면 된다
    - $w^{\prime\prime}_{ijk} = w^{\prime}_{ijk} \div \sqrt{\underset{i,k}{\sum} {w^\prime_{i,j,k}}^2 + \epsilon }$
    - 입력값이 아닌 통계적 가정에 기반해 Instance Normalization 보다 강도가 약함
    - 이것은 가중치 정규화와도 비슷하며 GAN에서 가중치 정규화가 효과가 있다는 연구가 있음
- 결과
  - 이방법을 사용해 Artifact를 완전히 제거
  - FID에선 크게 의미가 없으나 P&R에서 큰 변화가 있음
  - 그룹화된 컨볼루션을 통해 효율적으로 구현 가능 (Appendix B 참조)

# 3. Image quality and generator smoothness
- 기존 metrics의 한계
  - FID나 P&R 같은 지표들은 Generator의 향상에 크게 기여했지만 품질에 대해선 허점이 많다
- PPL 
  - 작은 Latent의 변화에 의해 생성된이미지 사이의 평균 LPIPS 거리를 사용
  - 맵핑의 Smoothness와 Quantifying을 정량화하는 지표를 처음으로 도입
  - PPL 낮을경우 전체적으로 이미지 품질이 높은것과 상관관계를 보임
  - 아직 왜 이것이 낮을수록 좋은 결과물이 나오는지는 정확히 밝히지 못함
  - 경험적으로 낮은 PPL을 갖도록 훈련을 하면 이미지 품질이 좋아질것이라 생각해 적용

- 정규화가 비용이 매우커 일반적으로 사용하는것을 설명?
- 
## 3.1 Lazy Regularization
- 일반적으로 정규화와 손실함수는 한 표현식으로 만들고 동시에 최적화됨
- 여기서 정규화텀이 손실함수보다 더 작은 빈도로 계산하여 컴퓨팅 비용과 메모리 사용량을 크게 줄이는것을 제안
- 미니배치 16번당 1번만 사용해도 실제 결과에 아무런 손해도 없는것을 보였고 이것을 적용
    
## 3.2 Path Length Regularization(이해불가)
- 이전 연구에 의하면 Z에서 이미지로 매핑이 잘 되었을 경우 Z 의 각지점에서의 작은 변동은 방향에 관계업이 이미지공간에서 동일한 크기의 변화를 일으킴
- 하나의 입력 W에서 이미지로의 맵핑이 $g(\mathrm{w}) : \mathcal{W} \to \mathcal{Y}$ 일때 야코비안 행렬은 $J_\mathrm{w} = \partial g(\mathrm{w})/\partial \mathrm{w}$
- $\mathbb{E}_{w,y\sim\mathcal{N}(0,I)}(||J_\mathrm{w}^T\mathrm{y}||_2 - a)^2$
  - 방향에 상관없이 결과의 길이를 보존하려는 정규화 항
  - $\mathrm{y}$ : 정규분포의 픽셀을 가진 랜덤 이미지
  - 고차원에서 이 항은 $\mathrm{J}_w$ 가 $\mathrm{w}$ 에서 직교일때 최소화됨
  - $a$의 경우 Global Scale로 학습으로 해결
- 명시적인 야코미안 행렬 계산을 피하기 위해 새로운 함수를 정의
  - $\mathrm{J}_\mathrm{w}^T\mathrm{y} = \triangledown_\mathrm{w}(g(\mathrm{w})\sdot \mathrm{y})$


- 결론
  - 아 도저히 모르겠어요
  - 아무튼 이 정규화를 사용시 더 신뢰가능하고 일관성있는 모델로 만들어짐
  - Generator의 Invert가 더 부드러워진다고 함

# 4. Progressive Growing Revisited
- Progressive Growing
  - 이 기법도 매우 효과적인 성능을 발휘했지만 Characteristic Artifact가 생성됨
  - 여기서 문제는 특정 사항에 대한 강한 위치선호도를 갖게됨(눈, 치아 등)
  - 원인은 모델이 커질때 각 Scale 마다 최종출력으로 작용하므로 순간적으로 최대 주파수 정보를 생성해야 하기에 모델이 커졌을때 중간층의 너무 높아진 주파수때문에 고정되어버리기 때문이라 가정

![Table1](/assets/post/200223/03.png)
# 4.1 Alternative Network Architectures
- Generator와 Discriminator 를 연결하는 Skip-Connection 추가
  - MSGGAN에서 사용된 Generator와 Discriminator의 같은 해상도의 커넥션 방식을 변경
  - Input / output skip : 해상도별로 RGB화 시키고 Upsample을 사용하여 하나의 출력으로 연결
    - 층마다 맵핑 네트워크의 훈련이 추가적으로 필요
  - Upsample을 사용한 잔차 연결방식으로 수정

  ![Table1](/assets/post/200223/04.png)
  - Generator에 