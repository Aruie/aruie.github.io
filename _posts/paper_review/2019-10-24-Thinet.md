---
layout: post
title:  "ThiNet : A Filter Level Pruning Method for Deep Neural network Compression"
tag : "Paper Review"
date : 2019-10-24 10:05:05 +0900
comments: true
---

# ThiNet : A Filter Level Pruning Method for Deep Neural network Compression

원논문 주소 : https://arxiv.org/abs/1707.06342

작은 가중치를 제거해 연산속도를 올리는게 없을까 해서 찾다가 나온 논문
일단 결과는 괜찮아보여서 한번 보게되었다.

# Abstract
- CNN에서 필터를 일부 제거하는 방식으로 연삭속도를 상향 하고 네트워크구조를 변경하지 않으므로 쉽게 적용 가능
- 현재가 아닌 다음 계층에서의 통계정보를 기반으로 앞 계층의 필터를 제거
- 기존모델에 사용시 전체적으로 많은 Flop의 감소에 반해 정확도의 감소는 거의없거나 오히려 증가
![Table1](/assets/post/191025-1.png)

# ThiNet
- 단순히 요약하면 중요성판단, 제거, 미세조정 방식
  - $i+1$ 층의 출력을 추정할 때 $i+1$의 입력의 채널의 부분집합을 사용한다면 사용되지 않은 채널과 연결된 $i$번째 레이어의 필터는 안전하게 제거가 가능
  - 이렇게 제거된 구조는 원래 구조와의 차이가 전혀 없고 필터와 채널만 적게됨(얇아진다 표현, 그래서 ThiNet)
    - 게다가 $i+1$층의 필터수는 변화가 없어 $i+1$의 출력은 변화가 없음
  - 제거되어 떨어진 성능을 회복하기 위한 Fine-tuning 이 진행되는데 한층을 Pruning할때마다 1,2 Epochs 정도의 파인튜닝을 실행(모든 레이어에 적용후엔 많은 Epochs를 진행한다.)

# Data-driven channel selection
- $(\mathcal{I}_i, \mathcal{W}_i, *)$
  - $\mathcal{I}_i \in \mathbb{R}^{C\times H\times W}$ : 입력 텐서
  - $\mathcal{W}_i \in \mathbb{R}^{D\times C\times K\times K}$ : 가중치 (D개의 필터))
- $\hat\mathcal{W} \in \mathbb{R}^{C\times K\times K}$ : $\mathcal{I}_{i+2}$(렐루 전)에서 무작위 추출된 필터
- 