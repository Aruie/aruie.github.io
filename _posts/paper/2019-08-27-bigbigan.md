---
layout: post
title:  "Large Scale Adversarial Representation Learning"
tag : Paper
date : 2019-08-27 10:05:05 +0900
comments: true
---

항상 대형모델 제목엔 소타가...
맨날 소타래

# Abstract

GAN들은 최근 매력적인 이미지 합성 결과를 달성했다.
초기에 비지도표현학습 방식을 사용한 GAN의 성공에도 불구하고, 이후엔 self-supervision(vised) 방식으로 대체되었다.
이 연구에서 우리는 이미지 생성 퀄리티의 향상이 표현학습 퍼포먼스를 상당히 향상 시키는것을 보여준다.
우리의 BigBiGAN은 SOTA 모델인 BigGAN 모델을 기반으로, 인코더를 추가하고 판별기를 수정하여 표현학습 과정을 확장하였다.
우리는 이 BigBiGAN 모델들의 representation learning 과 생성 능력을 광범위하게 평가하고, 이 생성기반모델들이 이미지넷에서의 비지도표현학습과 조건없는 이미지 생성에서 SOTA를 달성한 것을 보여준다.


# 1. Introduction

최근 몇년 동안 비전데이터 생성 모델들은 빠른 진보를 보아왔다. 이 모델들은 적은 모드, 간단한 구조, 그리고 저해상도라는 조건들에 국한되었지만, 모델링과 하드웨어의 발전으로 복잡하고 여러모드를 가지 고해상도 이미지 분포를 리얼하게 생성할 수 있는 능력을 얻게 되었다.
직관적으로, 이 특정 도메인의 데이터 생성 능력은 해당 도메인에 대한 높은 수준의 이해가 필요하다. 이 생각은 데이터가 많고 가격이 낮아져 전형적인 대부분 분류 머신러닝 모델이 예측할 라벨보다 훨씬 더 많은 정보로 구성된 이미지와 함께 오랫동안 인기를 끌고있다.
생성모델이의 발전은 명백했지만, 해결되지않는 질문이 있었다 : 이 모델이 배운 의미는 무엇이며 어떻게 그들이 표현학습에 영향을 주는지를.

원시 데이터만으로 진실을 이해하는 수준이 되려는 생성의 궁극적인 목표는 아직 거의 실현되지 않았다. 대신, 대부분 비지도학습을 위한 대부분의 성공적인접근법은 자기지도학습이라 불리는 지도학습 분야를 적용하는 것이다. 이 접근법들은 일반적으로 데이터의 특정 부분을 변경하거나 고정하고, 누락된 정보 부분을 생성하거나 예측하는 모델을 학습하는것을 포함한다. 예를들면 [37,38] 에서는 비지도방식으로 채색하는것을 제안했고, 모델에게 입력 이미지의 색상채널에 일부분만 주고 빠진 채널을 예측하는 방식이다. 

비지도학습방식의 생성 모델들은 원래의 데이터의 어떤 수정도 필요로하지 않는 전체 데이터 분포를 모델링 하도록 학습되는 자기지도학습을 위한 매력적인 제안을 제공한다.
생성모델의 한 종류는 GAN이다. GAN 프레임웤의 생성기는 랜덤 샘플링된 latent 값으로 부터 feed-foward 맵핑을 통해 데이터를 생성하고, 실제 데이터와 생성된 데이터 샘플 사이의 차이를 학습하는 판별기가 제공하는 신호를 통해 제네레이터가 데이터의 출력이 데이터의 분포를 따르도록 안내한다. ALI(Adversirially learned inference) 혹은 BiGAN 접근법은 기본 GAN의 구조에, 실제 데이터를 latents로 맵핑하는 인코더 모듈과 그리고 생성기로부터 학습된 맵핑의 역구조를 추가해 GAN 프레임웍 확장을 제안한다.

최적의 판별기의 한계는 [4]는 결정론적인 BiGAN은 오토인코더와 같은 방식으로 A의 reconstruction 비용$(l_0)$을 최소화 하는것을 보여준다; 그러나 재구성 오차 표면의 형상은 L2 오류와 같은 단순한 픽셀수준 지표와는 반대로 생성기의 파라미터에 의해 좌우된다. 판별기는 종종 강력한 뉴럴넷이 되지만, 그 희망은 저수준 디테일 보다는 재구성의 의미 오류를을 유도하는 것이다.

[4]에서 BiGAN 또는 ALI 를 통해 학습된 인코더가 이미지넷에서의 다운스트림 작업을 위한 시각적 표현 학습의 방법으로 효과적인 것을 증명했다. 그러나 이것은 DCGAN 스타일의 생성기를 사용하였고, 이 데이터셋에서 고해상도 이미지를 생성하지 못하기 떄문에 인코더가 모델링 할 수 있는 의미가 상당히 제한적이다. 이 연구에서 우리는 
ImageNet에 존재하는 많은 구조와 모드를 캡쳐 가능해 보이는 현대 모델인 BigGAN을 생성기로 사용하는 접근방식을 다시 살펴볼 것이다. 다음 방식을 따른다.

- 우리는 BigBiGAN(BigGAN생성기를 사용한 BiGAN)이 ImageNet의 비지도 표현 학습에서 SOTA인 것을 보여준다.
- BigBiGAN에 생성기를 적용하기 위한 더 안정적인 버전을 제안한다.
- 모델 디자인 선택을 위한 철저한 경험적인 분석과 연구를 수행한다.
- 표현학습이 객관적으로 조건없는 이미지 생성을 도와줄 수 있고, 조건없는 ImageNet 생성에서 SOTA 결과가 나온것을 증명한다.


# 2. BigBiGAN

BiGAN 또는 ALI 접근은 모델 추론 또는 피쳐 표현으로 사용 가능한 인코더의 학습을 GAN 가능하게하는 프레임웍의 확장모델로 제안되었다. 데이터의 분포 $P_x$와 latents $z$ 의 분포 $P_z$(일반적으로 평균이 0인 가우시안 분포) 가 주어지고, 생성기 $\mathcal{G}$ 는 $P_z$ 공간에서 샘플링된 $z$ 가 주어진 데이터 $x$ 의 조건부 분포 $P(x\|z)$ 를 모델링 한다. 마치 일반 GAN 생성기 처럼. 인코더 $\mathcal{E}$는 조건부 분포의 반대인 $P(z\|x)$를 모델링하고, 데이터의 분포 $P_x$에서 샘플링된 $x$가 주어졌을때의 latent $z$ 를 예측한다.

인코더 $\mathcal{E}$ 가 추가된것 외에, BiGAN 프레임웍 안에 있는 GAN에 대한 다른 수정으로는 입력 쌍인 $(x,z)$(일반적인 GAN의 x와는 다르다)를 취하는 판별기 $\mathcal{D}$를 병합하고, (데이터 분포와 인코더) 대 (생성기와 latent 분포) 쌍 사이의 차이를 학습한다. 구체적으로 입력 은 $(x \sim P_x, \hat{z} \sim \mathcal{E}(x))$ 와 $(\hat{x} \sim \mathcal{G}(z), z \sim P_z)$ 이고, $\mathcal{G}$ 와 ${E}$ 의 목적은 구분할 수 없게 샘플링된 쌍 $P_{x\mathcal{E}}$ 와 $P_{\mathcal{G}z}$ 두개의 결합 분포를 만듬으로 판별기를 속이는 것이다. [4,7]에 나온 adversarial minimax objective, GAN의 프레임웍과 유사한 것으로 다음과 같이 정의된다.

$$
\underset{\mathcal{GE}}{min}\underset{\mathcal{D}}{max}

\left\{
    \mathbb{E}_{x\sim P_x, z \sim \mathcal{E}_\Phi(x)} 
    [\log(\sigma(\mathcal{D}(x,z)))] + 

    \mathbb{E}_{z\sim P_z, x \sim \mathcal{G}_\Phi(z)} 
    [\log(1 - \sigma(\mathcal{D}(x,z)))]
\right\}

$$

이 목적함수를 사용한 [4,7]은 최적의 $\mathcal{D,G, E}$ 가 결합분포 $P_{x\mathcal{E}}$ 와 $P_{\mathcal{G}z}$ 사이의 젠슨샤논 발산을 최소화하는 것을 보여주는데다, 전역 최적점에서는 두 결합분포가 같아지고, 일반적은 GAN의 결과와 유사해진다. 
더 나아가, [4]에서는 $\mathcal{E,G}$가 결정함수인 경우(예를들면 조건부분포 $P_{\mathcal{G}(x|z)}$, $P_{\mathcal{E}(z|x)}$ 가 디랙델타 함수일때), 이 두 함수는 최적의 결합 판별기 $x, z$에 재구성 비용에 효과적으로 작용하면서 전역 최적점에서 서로 역함수가 되는 것을 보여준다.




BigBiGAN의 핵심은 BiGAN과 같고 SOTA인 BigGAN의 생성기와 판별기를 적용하였다.
거기에 더하여, 생성기와 타협하지 않고 좋은 표현 학습 결과를 이끄는 향상된 판별기 구조를 찾았다. 즉 [4,7]에서 제안된 데이터와 latent분포를 함꼐 묶는 결합판별기 손실 이외에, 추가적인 짧은 학습 목표를 추가했고 이것은 오직 x나 latent에만 관련된 함수이다. 비록 [4,7]에서 기존 BiGAN의 목적함수로 학습된 결합 분포가 전역 최적점과 일치하는것은 입증 햇지만 $x,z$의 한계분포도 일치함을 암시한다.
이 속성을 명시적으로 적용해 이 짧은 텀이 직관적으로 좋은 방향으로 최적화를 유도한다.
예를들면, 이미지 생성에서 이 $x$에 관한 단항의 손실이 기존 GAN의 목적함수와 매치해주고, latent 입력에 독립적으로 이미지분포가 매칭되도록 생성기를 조종하는 학습신호를 제공한다.

구체적으로, 판별기의 로스 $\mathcal{L_D}$ 과 인코더와 생성기 로스 $\mathcal{L_{EG}}$ 는 다음과 같이 정의된다. 판별기의 점수 함수 $s_\*$ 와 샘플별 로스에 해당하는 $l_*$을 기반으로 한다.

$$
s_x(x)= \theta^T_x F_{\Theta}(x) \\
s_z(z)= \theta^T_z F_{\Theta}(z) \\
s_{xz}(x,z)=\theta^T_{xz} J_\Theta(F_\Theta(x), H_\Theta(z)) \\
\mathcal{l_{EG}}(x,z,y) = y(s_x(x) + s_z(z) +s_{xz}(x,z))) \\
\mathcal{L_{EG}}(P_x, P_z) = 
    \mathbb{E}_{x \sim P_x, \hat{z} \sim \mathcal{E}_\Phi(x)}
    [\mathcal{l_{EG}(x,\hat{z},+1)}] +
    \mathbb{E}_{z \sim P_z, \hat{x} \sim \mathcal{G}_\Phi(z)}
    [\mathcal{l_{EG}(\hat{x},z,-1)}] \\

\mathcal{l_D}(x,z,y) = h(ys_x(x)) + h(ys_z(z)) + h(ys_{xz}(x,z)) \\
\mathcal{L_D}(P_x, P_z) = 
    \mathbb{E}_{x \sim P_x, \hat{z} \sim \mathcal{E}_\Phi(x)}
    [\mathcal{l_D(x,\hat{z},+1)}] +
    \mathbb{E}_{z \sim P_z, \hat{x} \sim \mathcal{G}_\Phi(z)}
    [\mathcal{l_D(\hat{x},z,-1)}] \\
$$

$h(t) = max(0,1-t)$ 는 판별기를 정규화하는데 사용한 힌지손실이고 BigGAN에서도 사용된다. 판별기는 $F, H, J$ 3개의 작은 서브모듈을 포함한다. $F$는 x, $H$는 z 만을 입력을  사용하고, 그들의 출력과 상수 $s_x, s_z$ 가 주어진   $\theta_x, \theta_z$을 투영하여 학습한다. 우리 실험에서 데이터 $x$ 는 이미지, latent $z$ 는 1차원 벡터이다.그리고 $F$ 는 ConvNet, $H$ 는 MLP이다. $x$와$z$를 묶은 결합점수 $s_{xz}$는 $\mathcal{D}$ 서브 모듈과 $\mathcal{J}$, 그리고 $F,H$ 함수의 출력에 주어진다.

$\mathcal{E,G}$의 파라미터 $\Phi$는 $\mathcal{L_{EG}}$를 최소화 하는것으로 최적화되고, $\mathcal{D}$ 의 파라미터 $\Theta$는 $\mathcal{L_D}$를 최소화 하여 최적화된다. 기대값들인 $\mathbb{E}$는 미니배치에서 취한 몬테카를로 샘플링 방식으로 추정된다. 

# 3. Evaluation 

우리실험은 거의 [37]에서 처음 제안된 비지도 학습 기법을 평가하는 일반적인 방식을 따랐다. BigBiGAN을 라벨없는 이미지에서 학습후 고정시키고, 선형 분류기를 결과에 추가하고, 모든 트레이닝셋의 라벨에 대하여 지도학습을 하였다. 또한 이미지 생성 성능을 측정하기 위해, 기존 측정법인 Inception Score 를 기록하고, Frechet Inception Distance(FID)를 사용했다.

## 3.1 Ablation (요약?)

선택된 모델링의 수에 따라 직접적으로 평가하고 광범위한 요약을 시작했다. 테이블 1에 결과가 나와있다. 가능한 각각 다른 시드를 주어 3번이상수행 했고 모든 지표의 평균과 표준편차에 대해 기록했다.

128해상도에선 상대적으로 완전하게 시작되었다. $\mathcal{G}$의 구조와 판별기의 $F$는 BigGAN의 128 구조를 사용했고, 스킵커넥션을 추가하고 [1]에 제안된 임베딩된 노이즈 셰어링을 사용하였다. z 는 120차원이고 6그룹으로 나뉘어 20차원씩 제네레이터의 6레이어에 각각 들어갔다. 남은 컴퓨넌트인 H와 J 는 ResNet 스타일의 스킵커넥션이 들어간 8층의 MLP로 구성 (2 층씩 4개의 residual blocks)하고 2048의 유닛을 가지고 있다. $\mathcal{E}$ 는 ResNet-v2-50 기본을 사용하고 GAP를 추가 후 스킵커넥션이 추가된 4개의 4096유닛의 4층 레이어를 추가하였다 (2층씩 2개의 residual block). 
비조건 BigGAN의 학습은 [24]에서 제안된 설정인 Single Label 에 해당하는 방법을 사용하였고, 한가지의 더미 레이블을 모든 이미지에 사용하였다. (이론적으로는 클래스조건 입력 대신에 편향을 학습하는 것과 같다). 이후 뒤에 상세히 기술한 결과를 가지고 모델의 몇가지 측면을 제거했다. 추가적인 구조나 최적화 디테일은 부록A 에 제공된다.
많은 결과에 대한 전체 학습 곡선 은 부록 D에 포함되어 있다.

### Latent distribution $P_z$ and Stochastic $\mathcal{E}$
ALI에 따르면, 우리 모델의 인코더 $E$ 는 비결정적이고, 분포 $\mathcal{N}(\mu, \sigma)$ 를 수정한다. $\mu, \hat\sigma$는 모델의 출력에 선형 층으로 주어지고, 최종 표준편차인 $\sigma$는 비선형인 비음수함수 softplus로 계산된다 $\sigma = \log(1+\exp(\hat\sigma))$. 최종 $z$는 $z = \mu + \epsilon\sigma, \epsilon \sim \mathcal{N}(0,I)$을 사용한다. deterministic 인코더와 다르게 샘플링 없이 바로 $z$를 예측 가능하고, 비결정적 기반 모델이 상당히 더 높은 분류 성능을 달성했다(생성 부분에서 비용 없이). 또한 BiGAN에서 사용한 선형출력인 $\hat{z}$을 입력으로 $z = \tanh(\hat{z})$ 함수를 사용하는 균등 분포$P_z = \mathcal{U}(-1,1)$ 와도 비교를 해보았지만 이또한 비결정적 기반 모델에 비해 낮은 분류결과가 나타났다.

### Unary loss terms
하나 혹은 양쪽의 unary 로스를제거해가며 성능을 평가 해봤다. 양쪽을 제거하는것은 원래 BiGAN 의 목적함수에 해당한다. $x$텀을 가지는 방법이 생성기 퍼포먼스에 매우 긍정적인 영향을 끼친다. 기본과 x unary만 있는것이 IS 와 FID에서 z unary 만 가진것이나 추가항 이 없는것 보다 훨씬 좋은 결과를 가진다. 이 결과는 일반적인 생성기 로스와 일치하므로 직관적으로 볼 수 있어진다. z unary 텀은 좀더 미미한 변화를 만든다, 아마도 모델이 isotropic 정규분포처럼 간단한 분포여서 모델링시 쉬워서 그런듯 하다.
또한 분류와 생성의 FID 면에서 조금 향상되는 결과를 가져온다 (x unary 없이).
반면에, IS 는 z term 과 함께하면 나빠진다. 아마도 IS 는 생성기의 전체분포 보다 주요한 부분의 분포를 측정하기 때문이다. FID가 잘나오는것보다는 좋은 인코더에 의해 촉진되는것이 좋다. BiGAN에서 반전의 필요는 다양한 latent 공간을 단일한 데이터 분포로 축소하는 대신 생성기를 distinguishable 전체 latent 공간에 걸쳐 구분가능한 출력을 생성하도록 유도하는 것일 수 있다. 

### $\mathcal{G}$ capacity
표현학습에서 생성기의 중요성에 대한 질문을 위해 $\mathcal{G}$의 용량(채널)을 1/3 정도로(small $\mathcal{G}$ = 32) 다르게 했다($\mathcal{E,G}$는 고정). 전반적인 모델은 약간 불안정했고 고용량의 기본모델에 비해 현저히 나쁜 분류결과가 나왔고, 2/3 용량(64)에서는 생성 성능이 상당히 나쁘고 분류성능도 별로 좋지 않았다. 이 결과는  인코더를 통해 좋은 표현을 배우기 위해 필요한것은 고성능의 생성기라는 것을 확인시켰다. 
이 관계가 유지된다면 우리는 더 좋은 생성모델을 쓰느것이 곧 표현학슴을 더 향상시키는 것과 같다는 것을 예상 할 수 있다.

### StandardGAN 
또한 인코더가 없는 조건없는 BigGAN에 판별기에 F ConvNet만 있고 손실엔 $s_x$ 텀만 가지고도 이미지 생성 성능을 비교했다. 기본 GAN은 아주 조금 좋은 IS, 같은 FID, 그리고 새로운 손실항이 추가된 인코더와 판별기의 추가가 성능을 저해하지 않음을 보였다(섹션 2). (이에 반해, $x$텀이 없는 모델은 FID 에서 표준 GAN보다 훨씬 나쁜 생성 성능을 보인다.) IS는 $s_z$텀과 비슷한 이유로 성능이 나빠진다고 생각한다. 다음은 고해상도 입력으로 $\mathcal{E}$가 강화되는 것과, BiGBiGAN의 생성기가 FID에서 표준보다 상당히 향상되는것을 보여줄 것이다.

### High resolution $\mathcal{E}$ with varying resolution $\mathcal{G}$
BiGAN은 $\mathcal{E}$가 $\mathcal{G}$의 출력이나 $\mathcal{D}$의 입력보다 큰해상도의 이미지를 취하는 비대칭 설정을 제안했다. E는 128, G는 64. BigBiGAN에서 이설정을 사용하여 E의 입력을 256으로(ImageNet 기본설정), 그리고 G와 D에 {64, 128, 256}의 다양한 해상도를 사용하였다. 결과는 표1에 있고 G해상도가 E의 해상도에 가깝게 증가할 수록 표현학습의 결과가 좋아지는 것을 보여준다. 그러나 전반적으로 모델의 G를 256으로 할 때 매우 느렸기에 다른 결과들은 전부 128을 사용하였다. 
흥미로운 것은 높은 해상도 E에서 생성은 현저히 증가했다(FID에서). G를 같게 했음에도 불구하고. 이것은 자신과의 적대적 이미지 합성의 향상을 부분에서의 BigBiGAN의 잠재력을 보여준다. 표현학습과 추론 외에도.

### \mathcal{E} architecture
E의 입력을 256으로 고정하고, 다양한 E 구조에 대해 실험을 했다. [21]에서 살펴본 몇몇 ResNet-50 모델을 포함하여. 특히 히든레이어의 용량을 2에서 4로 확대했고, 그리고 residual 블록을 RevNet 구조로 변화시켰다(같은 레이어와 용량에 해당하는 ResNet과 비교). ResNet-50모델 보다 훨씬 월등한 RevNet-50 모델을 찾았다. 그러나 이 네트워크의 너비는 이미 확대되어 있었기에 ResNet-50과 너비를 두배로 확대한 RevNet 50으로 같은 용량으로 시작하였다. 뒤에 심지어 4배확대한 RevNet 모델도 사용할 것인데 이는 마지막 결과인 섹션3.2에 나온다.

### Decoupled $\mathcal{E,G}$ optimization
마지막 향상기법으로 우리는 G로부터 E의 최적화를 분리했고, 간단히 10배 높은 학습율을 E에 적용시키는것으로 극적으로 학습속도의 가속과 포현학습 결과의 상향을 발견할 수 있었다. ResNet-50에서 이 상승효과는 선형 분리기의 정확도를 거의 3% 증가시켰다. 또한 이것을 가장큰 E 구조인 RevNet-50x4에 적용시켜 비슷한 결과를 얻었다.

## 3.2 Comparison with prior methods 

### Representation learning
이제 이미지넷 검증셋에서 가장 높은 정확도를 보여주는 최상의 모델을 취할 것이다. 현재의 비지도 학습 분야의 SOTA와 비교하여. 또한 비교를 위해 작은 ResNet-50을 기반으로 한 최고의 모델의 분류결과를 보여준다. 이 모델은 테이블 1에 아래 두 줄과 일치한다.
결과들은 테이블 2에 나와있다. 몇가지 최근의 자기지도 기법 과 그들의 조합과 비교되어있고, 순수하게 생성모델에 기반을 둔 BigBiGAN 접근은 현재 비지도 학습 결과중 SOTA고, 최근 발표된 결과인 top-1(rotation prediction pre-training 기법을 사용한) 정확도를 55.4에서 60.8로 향상시켜 표현학습 에 매우 적합하다. 
???

또한 BN+CReLU라는 다른 AvePool과 다른 렌더링을 사용하여 학습한 선형 분류기를 실험했는데, 이것은 우리의 최고결과를 top1에서 61.3%까지 상승시켜줬다. GAP를 출력 a에 주고, 우선 h = BatchNorm(a) 을 계산하고 피쳐 마지막에 [ReLU(h), Relu(-h)]를 concat하는 CReLU라는 비선형 함수를 사용하였다. 여기서 배치노말은 파라미터가 없이 사용하였고, 선형학습기의 학습시엔 다른 어떤 추가적인학습도 포함하지 않는다. CReLU는 입력의 모든 정보를 가지고 있고 두배의 차원을 가지며 각 정보는 개선된 결과에 기여한다.

마지막으로 부록C 에 43.3%의 top1 정확도를 달성하는 제로샷 KNN 에 의해 표현을 평가한것이 들어있다. 가장 가까운 이웃의 예는 그림 12에 나와있다.

### Unsupervised image generation
테이블3에서 BigGAN 기반의 비지도와 비교하여 BigBiGAN의 비지도 생성 결과를 보여준다. 그 결과들의 차이는 테이블 1에 비교되어 있고 [24]에 제안된 데이터 증식을 사용하였다. 이 증식방법은 IS와 FID 측면에서 더 좋은 성능을 가져온다. 이러한 개선은 부분적으로 이러한 증식이 평균적으로 이미지의 큰 부분 잘라, 일반적으로 주어진 객체의 거의 대부분을 포함하는 이미지를 생성하는 생성기를 만들기 때문에, 특정 클래스의 더 대표적인 샘플을 더 생산하고 되고 중심부분의 통계와 일치하는 경향이 있기 떄문이다. 이 전처리의 차이 외에는, 테이블 3에서 사용한 방법은 Base 나 테이블1의 High Res E(256) 와 동일하다. 

이 결과들은 BigBiGAN이 IS와 FID 측면에서 같은 Single Label 방식에서 BigGAN 베이스라인의 생성결과를 현저하게 향상시킴을 보여준다. 게다 고해상도 E를 사용함으로 더 욱 향상시키는걸 볼 수 있고, 기존의 비지도 SOTA를 능가한다. [24]에 나온 pseudo-labeling 의 대안으로, 비지도 클러스트링으로 만든 라벨을 사용하고, 이것은 BigBiGAN을 보완하고 두가지를 결합하면 더 많은 개선 효과를 가질 수 있다. 마지막으로  50만번 학습을 진행하는 동안 지속적으로 향상되는 결과를 관찰했고, 백만번 했던 결과도 테이블3에 마지막에 기록되어있다.

## 3.3 Reconstruction
[4,7]에서 보여줫듯 BiGAN의 E 와 G 는 인코더에 의해 예측된 $\hat(z)$ 를 계산하고 이것을 생성기에 전달해 x를 재구성 할 수 있다. 이 재구성은 그림 2에서 보여준다. 이 재구성은 픽셀수준과는 거리가 먼데, 목표에의해 명시적으로 집행되는 재구성 비용이 없는 영향도 있을것이다.(심지어 재구성은 훈련때 계산도 안된다). 그러나 인코더가 모델에게 배우는 특징에 대한 직관이 제공될 것이다. 예를들면, 강아지, 사람, 혹은 음식이 포함된 이미지를 넣었을때 다른 사례에서 비슷한 포즈나 위치, 재질같은 같은 카테고리를 재구성 한다. (같은 방향을 바라보는 개의 비슷한 종). 입력의 저수준 디테일보단 고수준 의미에 의존하고, BigBiGAN 인코더를 후자를 더 모델링 하도록 학습된다. 추가적인 재구성들은 부록 B에 나와있다.

# 4. Related work
자기지도를 기반으로 한 비지도 이미지 표현 학습 방법의 몇가지는 매우 성공적인 것으로 증명되어있다. 자기지도 학습은 일반적으로 지도학습과 매우 비슷하게 디자인된 작업이 포함되어있지만, 수동인부분 없이 자기자신의 데이터로 라벨을 만들어낸다. 예전의 예로는 관련된 위치 예측이 있는데, 모델은 이미지패치를 입력받고 그들의 위치적 관계를 예측한다. Contrastive predictive coding (CPC)가 관련된 접근방법인데, 이미지 패치를 주어주고 모델은 다른 이미지 위치에서 어떤 패치가 발생할지 예측한다.다른 방법은 colorization, 모션 segmentation, 회전예측, 그리고 exemplar matching이 있다.





# Appendix A : Model and Optimization Details
최적화는 BigGAN과 일치한다. Adam을 사용하고 배치사이즈는 2048그리고 같은 학습율과  파라미터를 사용하여 G와 E를 학습했다. G,E가 한번 업데이트할때 D는 두번 업데이트 했고 직교정규화는 사용하지 않았다. SN은 G와 D에 사용되고 E에는 사용되지않앗다. Full cross replica BN이 G와 E에 사용되었다. 모든 평가에서 가중치 0.9999를 갖는지수이동평균을 G와 E에 적용했다(E에대해선 작지만 G에대해선 좋았다). 

BigBiGAN 학습만 아니라 선형분류기의 학습평가때도 ResNet 스타일 데이터증식을 사용한 전처리를 하지만 224가 아닌 128 혹은 256을 사용한다. 

Table1에 나온 선형분류평가 이미지넷으로부터 랜덤하게 고른 10K개의 이미지를 검증셋으로 유지하고 이것의 정확성을 기록한다. Table1의 모든 결과는 500k번 실행되며, 조기정지는 분류정확도에 의존한다. 모든 분류기는 0으로 초기화되고 학습율 0.01과 감쇠 0.9999의 지수이동평균으로 5k번 마다 훈련된다.

BigBiGAN 훈련중 도움이 되는것을 발견했는데, 이 선형분류기 평가를 현재 E의 가중치에 주어지면 각 평가전에 분류기 가중치를 0으로 설정해 도움이된다.

Table 2에서는 BigBiGAN 훈련을 1M으로 연장하고 50k의 결과와 비교한다. 몇가지 학습율을 적용해가며 EMA를 적용한다. 하이퍼파라미터 선택과 조기정지는 학습 분류정확도에 기초한다. FID는 전체 이미지넷 훈련세트에 대한 통계에 대해 기록되고 비교를 위해 FID를 검증셋에 대해 보고하는 Table3처럼  출력 해상도를 재조정하고 중앙크롭을 취해 전처리한다.

모든 모델은 32~512코어의 TPU를 사용하여 병렬훈련 시켰다.

Table4에는 비교를 목적으로 실험에 사용된 모델들의 완전훈련된 결과를 제시한다.


### First later Convolutional filters.
Figure 3에서 RevNet 4의 E의 첫번째 컨볼 레이어의 필터를 시각화 하였다. A와 B의 필터차이를 유사히 보면 B는 높은 E 학습율을 사용했고 필터 모양의 질적 향상이 보인다. BiGAN에서 관찰한것처럼 노이즈가 적고 컬러필터가 더 많다. 이것은 입력층의 필터를 조사하는것이 훈련되지않은 모델에 대한 진단의 역할이 가능한것을 보여준다.