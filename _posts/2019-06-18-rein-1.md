---
layout: post
title:  "강화학습 기초정리"
categories: ReinforcedLearning
comments: true
---


# MDP ( Marcov Decision Process )

- 현재의 정보가 충분히 주어진다면 과거는 미래와 독립이라는 가정을 둔 상태
- 현재 정보만으로 미래의 행동을 결정한다


## 구성요소
### 1. State ( $S$ )
- 관찰 가능한 상태의 집합 (현재 상태)
- $S_t = s$ : 시간 t에서의 상태 s

### 2. Action ( $A$ )
- $S_t$에서 할 수 있는 모든 행동의 집합
- $A_t = a$ : 시간 t에서의 행동 a

### 3. Reward ( $R$ )
- $R^a_s = E[\ R_{t+1}\ \|\ S_t=s, A_t=a\ ]$
- 상태 $s$에서의 행동 $a$를 했을때 받을 보상에 대한 기대값
- 보상의 시점은 행동후 환경이 알려주는 것이기에 $R_{t+1}$을 사용

### 4. Probarblity ( $P$ )
- $P^a_{ss'} = P[\ S_{t+1} = s'\ \|\ S_t=s, A_t=a\ ]$
- 상태 $s$에서 행동 $a$를 취했을때 상태 $s'$에 도달할 확률
- 일반적으로 상태 $s$에서 어떤행동 $a$를 취한다면 에이전트의 상태는 $s'$에 도달하지만 외부 요인에 있어서 실패 할수도 있기에 도달할 확률을 표현

### 5. Discount Factor ( $\gamma$ )
- 보상이 현재가아닌 미래에 이루어진다면 시간만큼의 보상의 가치가 감소함
-  $\gamma \in [0,1]$ : 시간에 따른 감소하는 가치의 비율(0에서 1사이)
- $\gamma^{k-1}R_{t+k}$ : $k$시간 후에 받는 감가된 보상



#최적화

## Policy ($\pi$)
- 모든 상태에서 에이전트가 할 행동 (상태를 입력하면 행동을 출력하는 함수)
- Opitmal Policy(최적정책)을 찾는것이 강화학습의 궁극적인 목표
- $\pi(a\|s) = P[\ A_t=a\ \|\ S_t = s\ ]$ : 상태 $s$에서 행동 $a$를 할 확률

## Value Function ($\mathbf{v}$)
- 현재 상태에서의 받을수 있는 보상의 기대값 (상태의 기대값)
- $G_t = R_{t+1} + R_{t+2} + \cdots$ : 현재 이후 보상의 합
- $\mathrm v(s) = E[\,G_t\,\|\,S_t=s\,]$ : 현재 상태에서 받을 수 있는 보상의 기대값
- $\mathrm v_{\pi}(s) = E[ R_{t+1} + \gamma \mathrm v_{\pi}(S_{t+1})\ \|\ S_t=s\ ]$
- 현재의 보상($R_{t+1}$)과 다음 스텝의 기대값의 합으로 표현가능

## Q-Function ( $\mathrm q$ )

- 현재 상태에서 어떤 행동의 대한 보상의 기대값 (행동의 기대값)
- $\mathbf v_\pi(s) = \sum \pi(a\|s)\ \mathbf{q}_\pi(s, a)$
- $\mathbf q_\pi(s, a) = E_\pi[\ R_{t+1} + \gamma \mathbf q_\pi(S_{t+1},A_{t+1} )\ \|\ S_t=s, A_t=a \ ]$

## 벨만 기대 방정식
- 어떤 정책에서 가장 보상이 좋은 참 가치함수를 구하는 방정식
- $\mathrm v_\pi(s)= E[\ R_{t+1} + \gamma v_{\pi}(S_{t+1})\ \|\ S_t=s\ ]$
  - 위 기대방정식은 기대값을 알아야 계산이 가능

- $\mathbf v_\pi(s) = \sum\pi(a\|s)\ (R_{t+1} + \gamma\sum P^a_{ss'} \mathbf v_\pi(s'))$
  - 기대값을 확률과 보상의 곱의 합으로 풀어서 쓴 방정식으로 계산이 가능
- $\mathbf v_\pi(s) = \sum\pi(a\|s)\ (R_{t+1} + \gamma\mathbf v_\pi(s'))$
  - 확률이 1일때 단순화된 방정식

## 벨만 최적 방정식
-  벨만 기대방정식을 반복하면 현재 정책에서 가장 높은 보상을 받을 수 있는 가치함수를 구할 수 있음
-  최적의 정책은 모든 정책중에서 가장 높은 값을 가진 가치함수를 찾아야함
-  $\mathrm v_*(s) = \underset{\pi}{max}[\mathrm v_\pi(s)]$ : 최적의 가치함수, 계산이 불가능
-  $\mathrm q_*(s,a) = \underset{\pi}{max}[\mathrm q_\pi(s,a)]$ : 최적의 큐함수, 도 계산이 불가능
-  최적의 정책은 모든 행동중 가장 높은 값을 가진 큐함수를 선택하는 정책임

-  $\pi_\*(s\|a) = \begin{cases}  
1, & \mathrm{if}\,\,a = argmax_{a \in A}\,\mathrm q_\*(s,a)  \\\\ 
0, & \mathrm{otherwise}
\end{cases}$


## 여기부터 뭔가 이상하다
-  최적의 큐함수중에 max를 취하는것이 최적의 가치함수
-  $V_*(s) = \underset{a}{\max}[\,q_\*(s,a) \| S_t=s, A_t=a\,]$
-  여기에 큐함수의 정의를 대입 하면
-  $V_*(s) = \underset{a}{\max} E[R_{t+1} + \gamma \mathrm v_\*(S_{t+1}) \| S_t=s, A_t=a]$
-  바로 벨만 최적방정식으로 기대방정식과 차이점이라면 max 하나... 
-  어렵게 돌아왓지만 결론은 최적의 가치함수는 현재의 보상과 감가율을 적용한 다음스텝의 가치함수의 기대값의 합이 가장 크게 만드는 것
-  이상한부분이 $A_t = a$ 부분은 사라져야하는거 아닌가? 한다
-  가치함수는 상태의 보상의 기대값인데 행동을 주어지면 이미 그건 큐함수아닌가?

- 일단 다떠나서 결론은 마지막 수식
- $q_\*(s,a) = E[R_{t+1} + \gamma \underset{a'}{\max} \mathrm q_\*(S_{t+1}, a') \,\|\,S_t=s, A_t=a\, ]$
- 큐함수에 대한 벨만 최적방정식? 이라한다
- 
