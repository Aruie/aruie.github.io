<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-07-31T11:07:18+09:00</updated><id>http://localhost:4000/</id><title type="html">AweSome Blog</title><subtitle>Aru's Daily Page</subtitle><author><name>Aru</name></author><entry><title type="html"></title><link href="http://localhost:4000/2019/07/31/2019-07-05-GNN-google.html" rel="alternate" type="text/html" title="" /><published>2019-07-31T11:07:18+09:00</published><updated>2019-07-31T11:07:18+09:00</updated><id>http://localhost:4000/2019/07/31/2019-07-05-GNN-google</id><content type="html" xml:base="http://localhost:4000/2019/07/31/2019-07-05-GNN-google.html">&lt;h1 id=&quot;relational-inductive-biases-deep-learning-and-graph&quot;&gt;Relational inductive biases, deep learning, and graph&lt;/h1&gt;
&lt;p&gt;arXiv:1806.01261v3 , 17 oct 2018&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;인공지능은 최근 비전, 언어, 조작, 그리고 의사결정 과정에서 흥행하고있다&lt;/p&gt;

&lt;p&gt;데이터와 컴퓨팅자원이 저렴해졌고 딥러닝의 자연스러운 강점 을 가지고있다&lt;/p&gt;

&lt;p&gt;그러나 사람의 지능의 특성은 현재 접근법으로는 접근하지 못하는 경우가 많다&lt;/p&gt;

&lt;p&gt;특히 누군가의 경험을 일반화 하는것
-유아기의 인간지능의 특징-
은 현대 인공지능에서 어려운 도전과제로 남아있다&lt;/p&gt;

&lt;p&gt;다음은 위치, 리뷰, 통합 순이다&lt;/p&gt;

&lt;p&gt;우리는 AI가 인간과 비슷한 능력이 되러면 결합과 일반화가 필요하고 구조화된 표현과 계산이 이 목표를 실현할 열쇠이다&lt;/p&gt;

&lt;p&gt;생물학과 마찬가지로 자연과 양육의 협력을 사용하고, 핸드엔지니어링과 엔드투엔드의 사이에서 잘못된 선택을 하는것을 거부한다
그리고 대신 그것들의 보완적인 강점으로부터의 이득에 접근하는것을 옹호한다&lt;/p&gt;

&lt;p&gt;우리는 딥러닝 아키텍쳐의 엔티티와 관계와 그것의 구성에 대한 학습을 용이하게 할수있는 관계 유도성 편향을 사용하는 방법을 연구한다&lt;/p&gt;

&lt;p&gt;우리는 정했다 새로운 구성 블록을 AI 툴킷을 위한 강한 관계형 편향을 가진
-그래프 네트워크-
일반화하고 다양한 접근법으로 확장하는 
다양한 접근법 그래프를 조작하는&lt;/p&gt;

&lt;p&gt;우리는 어떻게 그래프네트워크가 관계의 설명과 조합의 일반화를 하는지 토론했다&lt;/p&gt;

&lt;p&gt;복잡하고 해석가능하고 추론가능한 추리의 패턴의 토대를 마련했다&lt;/p&gt;

&lt;p&gt;이 논문과 함께 우리는 그래프 네트워크를 구축하기 위한 오픈소스 라이브러리를 발표했고 사용하는 방법을 시연했다&lt;/p&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;인간의 지능의 주요 키는 ‘유한한 방법의 무한한 사용이다 ‘&lt;/p&gt;</content><author><name>Aru</name></author></entry><entry><title type="html">Jekyll 다시 시작</title><link href="http://localhost:4000/jekyll/2019/07/31/jekyll-restart.html" rel="alternate" type="text/html" title="Jekyll 다시 시작" /><published>2019-07-31T10:05:05+09:00</published><updated>2019-07-31T10:05:05+09:00</updated><id>http://localhost:4000/jekyll/2019/07/31/jekyll-restart</id><content type="html" xml:base="http://localhost:4000/jekyll/2019/07/31/jekyll-restart.html">&lt;h1 id=&quot;다시시작&quot;&gt;다시시작&lt;/h1&gt;

&lt;p&gt;전에 시도하다 포기하고 일단 포스트만 올리면서 나중에 좋은스킨 나오면 해야지 하다가
이제 슬슬 포트폴리오겸 만들어야겠다 싶어 다시 도전….&lt;/p&gt;

&lt;p&gt;일단 설정이 문제가 있었을수도있으니&lt;/p&gt;

&lt;p&gt;깔끔하게 우분투를 날려버리고… 다시깔았다…&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get update -y &amp;amp;&amp;amp; sudo apt-get upgrade -y
$ sudo apt-get install -y build-essential ruby-full
$ sudo gem install jekyll bundler
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;아니 이게왠일&lt;/p&gt;

&lt;p&gt;여기까지 아무 에러가 없이 진행되었..&lt;/p&gt;

&lt;p&gt;원하던 스킨을 다운받고&lt;/p&gt;

&lt;p&gt;드디어 대망의&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ jekyll serve
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;는 될리가 없지………ㅜㅜ&lt;/p&gt;

&lt;p&gt;jekyll new 로 새로 생성하면 잘만되는데&lt;/p&gt;

&lt;p&gt;꼭 스킨을 다운받으면 안되는…&lt;/p&gt;

&lt;p&gt;역시 또 하루종일 이거설치 저거설치&lt;/p&gt;

&lt;p&gt;이거삭제 저거삭제&lt;/p&gt;

&lt;p&gt;열심히 해본 결과 알아낸 해결책은 해당폴더에서&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ bundle install
$ bundle exec jekyll serve 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이거였다…&lt;/p&gt;

&lt;p&gt;문제는 또 parsing 오류가 떳는데&lt;/p&gt;

&lt;p&gt;이건 title ‘Aru’s Blog’  여기에서 에러가..&lt;/p&gt;

&lt;p&gt;이래서 특문을 쓸땐 항상조심 ㅠㅠ&lt;/p&gt;

&lt;p&gt;결국&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post/190731-1.png&quot; alt=&quot;성공&quot; /&gt;&lt;/p&gt;

&lt;p&gt;드디어 성공했다 ㅠㅠㅠ&lt;/p&gt;

&lt;p&gt;이제 꾸미기에 들어가 봅시다&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">다시시작 전에 시도하다 포기하고 일단 포스트만 올리면서 나중에 좋은스킨 나오면 해야지 하다가 이제 슬슬 포트폴리오겸 만들어야겠다 싶어 다시 도전…. 일단 설정이 문제가 있었을수도있으니 깔끔하게 우분투를 날려버리고… 다시깔았다… $ sudo apt-get update -y &amp;amp;&amp;amp; sudo apt-get upgrade -y $ sudo apt-get install -y build-essential ruby-full $ sudo gem install jekyll bundler 아니 이게왠일 여기까지 아무 에러가 없이 진행되었.. 원하던 스킨을 다운받고 드디어 대망의 $ jekyll serve 는 될리가 없지………ㅜㅜ jekyll new 로 새로 생성하면 잘만되는데 꼭 스킨을 다운받으면 안되는… 역시 또 하루종일 이거설치 저거설치 이거삭제 저거삭제 열심히 해본 결과 알아낸 해결책은 해당폴더에서 $ bundle install $ bundle exec jekyll serve 이거였다… 문제는 또 parsing 오류가 떳는데 이건 title ‘Aru’s Blog’ 여기에서 에러가.. 이래서 특문을 쓸땐 항상조심 ㅠㅠ 결국 드디어 성공했다 ㅠㅠㅠ 이제 꾸미기에 들어가 봅시다</summary></entry><entry><title type="html">(진행중)Progressive Growing of GANs for Improved Quality Stability and Variation</title><link href="http://localhost:4000/paper/2019/07/20/PGGAN.html" rel="alternate" type="text/html" title="(진행중)Progressive Growing of GANs for Improved Quality Stability and Variation" /><published>2019-07-20T10:05:05+09:00</published><updated>2019-07-20T10:05:05+09:00</updated><id>http://localhost:4000/paper/2019/07/20/PGGAN</id><content type="html" xml:base="http://localhost:4000/paper/2019/07/20/PGGAN.html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;We describe a new training methodology for generative adversarial netwworks. The key idea isto grow both the generator and discriminator progressively : startting from a low resolution, we add new layers that model increasingly fine details as training progressses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at $1024^2$. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR 10. Additionally, we describe several implementation details that are importtant for discouragion unhealthy competition betwnne the generator and discrimination, Finally, we suggest a new metric for evaluation GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher–quality versiong of the CELEBA dataset.&lt;/p&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;Generative mothods that produce novel samples from high-dimensional data distributions, such as images, are finding widespread use, for example in speech synthesis (van den Oord te al., 2016a), image-to-image translation (Zhu et al., 2019; Liu et al., 2019; Wang et al., 2017). and image inpainting (Lizuka et al., 2017). Currently tyhe most prominent approaches are auto regressive models (van den Oord et al., 2016b;c), variarional autoencoders(VAE)(Kingma &amp;amp; Welling, 2014), and generative adversarial networks(GAN) (Goodfellow et al., 2014). Currently they all have significant strengehs and weaknesses. Auto regressive models - such as PixelCNN - produce sharp images but are slow to evaluate and do not have a latent representation as they direvtly model the conditional distribution over pixels, potentially limitiing their applicability. VAEs are easy to train but tend to produce blurry results due to restrictions in the model, although recent work is improveing this (Kingma et al., 2016). GANs produce sharp images, albeit only in fairly small resolutions and with somewhat limited variation, and the training continues to be unstable despite recent progress(Salimans et al., 2016; Gulrajani et al., 2017; Berthelot et al., 2016’ Kodali et al., 2017). Hybrid methods combine various strengths of the three, but so far lag behind GANs in image quality (Makhzani &amp;amp; Frey, 2017; Ulyanov et al., 2017; Dumoulin et al., 2016)&lt;/p&gt;

&lt;p&gt;Typically, a a GAN consists of two networks: generator and discriminator (aka critic). The generator produces a sample, e.g., an image, from a latent code, and the distribution of these images should ideally be indistinguishable from the training distribution. Since it is generally infeasible to engineer a function that tells whether that is the case, a discriminator network is trained to do the assessment, and since networks are differentiable, we also get a gradient we can use to steer both networks to the right direction. Typically, the generator is of main interest - the discriminator is an adaptive loss function that gets discarded once the generator has been trained.&lt;/p&gt;

&lt;p&gt;There are multiple porential problems with this formulation. When we measure the distance between the training disributions do not have substantial overlap, i.e., are too easy to tell apart (Arjovsky * Bottou, 2016). Originally, jensen-Shannon divergence was used as a distance metric (Goodfellow et al., 2014), and recently that formulation has been improved (Hjelm et al., 2016 and a number of more stable alternatives have been proposed, including least squares (Mao et al., 2016b), absolute deviation with margin (Zhao et al., 2016), and Wasserstein distance (Arjovsky et al., 2017; Gulrajani et al., 2017) Our contributions are largely orthogonal to this ongoing discussion, and we primarily use the improbed Wasserstein loss, but alos ecperiment with least-squares loss.&lt;/p&gt;

&lt;p&gt;The generation of high-resolution images is difficult because higher resolution makes it easier to tell the generated images apert from training images (Odena et al., 2017), thus drastically amplifying the gradient problem. Large resolutions also necessitate using smaller minibatches due to memory constraintsm further compromising training stability. Our key insight is that we can grow both the generator and discriminator progressively, stating from easier low-resolution images, and add new layers that introduce higher-resolution details as the training progresses. This greatly speeds up training and improves stability in high resolutions, as we will discuss in Section 2.&lt;/p&gt;

&lt;p&gt;The GAN formulation does not explicitly require the entire training data distribution to be represented by the resulting generative model. The conventional wisdom has been that there is a tradeoff between image quality and variation is currently receiving attention and various methods have been suggested for measuring it,mincludig inception score (Salimans et al., 2016), multi-scale structural similarity(MS-SSIM)(Odena et al., 2017; Wang et al., 2003), birthday paradox (Arora &amp;amp; Zhang, 2017), and explicit tests for the number of discrete modes discovered (Metz et al., 2016). We will describe our method for encouraging variation in Section 3, and propose a new metric for evaluation the quality and variation in Section 5.&lt;/p&gt;

&lt;p&gt;Section 4.1 discusses a subtle modification to the initialization of networks, leading to a more balanced learning speed for different layters. Furthermore, we observe that mode collapses traditionally plaguing GANs tend to happen very quickly, over the course of a dozen minibatches. Commonly they start when the discriminator overshoots, leading to exaggerated gradients, and an unhealthy competition follows where the signal magnitudes escalate in both networks. We propose a mechanism to stop the generator from participation in such escalation, overcoming the issue (Section 4.2)&lt;/p&gt;

&lt;p&gt;We evaluate our contributions using the CELEBA, LSUN, CIFAR10 datasets. We improve the best published iinception score for CIFAR10. Since the datasets commonly used in benchmarking generative methods are limited to a fairly low resolution, we have also created a higher quality version of the CELEBA dataset that allowds experimentation with output resolutions up to $1024 \times 1024$ pixels. This dataset and our full implementation are aavailable at [https://github.com/tkarras/progressive_growing_of_gans], trained networks can be found at [https://drive.google.com/open?id=0B4qLcYyJmiz0NHFULTdYc05lX0U] along with result images, and a supplementary video illustrating the datasets, additional results, and latent space interpolations is at [https://youtu.be/G06dEcZ-QTg].&lt;/p&gt;

&lt;h1 id=&quot;2-progressive-growing-of-gans&quot;&gt;2. Progressive Growing of GANs&lt;/h1&gt;

&lt;p&gt;Our primary contribution is a training methodology for GANs where we start with low-resolution imagesm and then progressively increase the resolution by adding layers to the networks as visualized image distribution and then shift attention to increasingly finer scale detail, instead of having to learn all scales simultaneously.&lt;/p&gt;

&lt;p&gt;We use generator and discriminator networks that are mirror images of each other and always grow in synchrony. All existing layers in both networks remain trainable throughout the training process. When new layers are added to the networks, we fade them in smoothly, as illustrated in Figure 2. This avoids sudden shocks th the already well-trained, smaller-resolution layers. Appendix A describes structure of the generator and discriminator in detail, along with other training parameters.&lt;/p&gt;

&lt;p&gt;We observe that the progressive training has several benefits. Early on, the generation of smaller images is substantially more stable because there is less class information and fewer modes (Odena et al.,2017). By increasing the resolution little by little we are continuously asking a much simpler question compared to the end goal of discovering a mapping from latent vectors to e.g. $1024^2$ images. This approach has conceptual similarity to recent work by Chen &amp;amp; Koltun (2017). in prectice it stabilizes the training sufficiently for us to reliably synthesize megapixel-scale images using WGAN-GP loss (Gulrajani et al.,2017) and even LSGAN loss (Mao et al.,2016b).&lt;/p&gt;

&lt;p&gt;Another benefit is the reduced training time. With progressively growing GANs most of the iterations are done at lower resolutions, and comparable result quality is often obtained up to 2-6 times faster, depending on the final output resolution.&lt;/p&gt;

&lt;p&gt;The ideaof growing GANs progressively is related to the work of Wang et al.(2017), who use multiple discriminators that operate on different spatial resolutions. That work in turn is motivated by Durugkar et al.(2016) who use one generator and multiple discriminators concurrently, and Ghosh et al.(2017) who do the opposite with multiple generators and one discriminator. Hierarchical GANs(Denton et al.,2015; Huand et al.,2016; Zhang et al.,2017) define a generator and discriminator for each level of an image pyramid. These mothods build on the same observation as our work - that the complex mapping from latents to high-resolution images is easier to learn in steps - but the crucial difference is that we have only a single GAN instead of a hierarchy of them. In contrast to early work on adaptively growing networks, e.g., growing neural gas(Fritzke, 1995) and neuro evolution of augmenting topologies (Stanley &amp;amp; Miikkulainen, 2002) that grow networks greedily, we simply defer the introduction of pre-configured layers. in that sense our approach resembles layer-wise training of autoencoders ( Bengio et al.,2007).&lt;/p&gt;

&lt;h1 id=&quot;3-increasing-variation-using-minibatch-standard-deviation&quot;&gt;3. Increasing Variation using minibatch standard deviation&lt;/h1&gt;

&lt;p&gt;GANs have a tendency to capture only a subset of the variation found in training data, and Salimans et al.(2016) suggest “minibatch discrimination” as a solution.&lt;/p&gt;

&lt;p&gt;GAN은 트레이닝 데이터에서 찾은 variation의 일부만 캡쳐하는 경향이 있고, 살리만 이것의 해결법으로 minibatch discrimination 을 제안했다.&lt;/p&gt;

&lt;p&gt;They compute feature statistics not only from individual images but also across the minibatch, thus encouraging the minibatches of generated and training images to show similar statistics.
그들은 각 이미지가 아닌 미니배치에 거쳐 통계량을 계산하고, 따라서 생성되고 학습된 미니배치가 비슷한 통계량을 가지도록 도와준다.&lt;/p&gt;

&lt;p&gt;This is implemented by adding a minibatch layer towards the end of the discriminator, where the layer learns a large tensor that projects the input activation to an array of statistics.&lt;/p&gt;

&lt;p&gt;이것은 discriminator 의 끝쪽에 미니배치레이어를 더하는것으로 구현된다. 
그 레이어는 입력의 통계값의 활성화를 투영한 큰 텐서를 학습한다.&lt;/p&gt;

&lt;p&gt;A separate set of statistics is produced for each example in a minibatch and it is concatenated to the layer’s output, so that the discriminator can use the statistics internally. We simplify this approach drastically while also improving the variation.&lt;/p&gt;

&lt;p&gt;통계량의 일부분은 미니배치의 각 샘플에서 생산되며 레이어의 출력과 concat 된다. 
discrimiantor가 그 통계값을 내부적으로 사용 가능하도록. 이 접근법을 극적으로 단순화시키면서  variation도 향상시켰다.&lt;/p&gt;

&lt;p&gt;Our simplified solution has neither learnable parameters nor new hyperparameters.  &lt;br /&gt;
우리의 간단한 해결책은 학습가능한 파라미터도 아니고 새로운 하이퍼파라미터도 아니다.&lt;/p&gt;

&lt;p&gt;We first compute the standard deviation for each feature in each spatial location over the minibatch. We then average these estimaties over all features and spatial locations to arrive at a single value.  &lt;br /&gt;
첫번째로 각 미니배치를 넘어 각 공간위치안에 피쳐들에 대한 표준편차를 계산한다. 그리고 각 피쳐 및 각 공간위치에 대한 추정값들을 평균내어 단일 값으로 바꾼다&lt;/p&gt;

&lt;p&gt;We replicate the value and concatenate it to all spatial locations and over the minibatch, yielding one additional(constant) feature map. This layer could be inserted anywhere in the discriminator, but we have found it best to insert it towards the end(see Appendix A.1 for details)
우리는 이값을 복제하고 모든 공간위치에 concate 하여 추가적인 하나의 피쳐맵을 만든다. 이 레이어는 discriminator의 어디에든 들어갈 수 있으나 우리가 찾아낸 베스트는 끝쪽에 넣는것이었다&lt;/p&gt;

&lt;p&gt;We experimented with a richer set of statistics, but were not able to improve the variation further. In parellel work, Lin et al.(2017) provide theoretical insights about the benefits of showing multiple images to the discriminator. &lt;br /&gt;
우리는 더 많은 자료로 실험을했지만, variation 을 더 향상시키진 못했다. Lin의 작업에선 여러 이미지를 discriminator 에게 보여줌으로 얻는 이점에 대한 정보를 제공한다.&lt;/p&gt;

&lt;p&gt;Alternative solutions to the variation problem include unrolling the discriminator (Metz et al., 2016) to regularize its updates, and a “repelling regularizer”
(Zhao et al.,2017) that adds a new loss term to the generator, trying to encourage it th orthogonalize the feature vectors in a minibatch. &lt;br /&gt;
다양성 문제에 대책으로는 discriminator 에 unrolling 을 포함하여 업데이트시 정규화하는 방법이 있다. ‘repelling regularizer’ 라는 제네레이터에 새로운 로스 부분을 추가해서 미니배치 안에 피쳐들을 직교화 되도록 하는 방법도 있다.&lt;/p&gt;

&lt;p&gt;The multiple generators of Ghosh et al. (2017) also serve a similar goal. We acknowledge that these solutions may increase the variation even more than our solution - or possibly be orthogonal to it - but leave a detailed comparison to a later time.  &lt;br /&gt;
Ghosh의 다중 generator 도 비슷한 목표를 제공한다. 이 솔루션들은 우리의 해법보다 다양성을 더 높여주거나 가능한 젝교화되도록 하는것을 인정하지만 뒤에있는 디테일 비교가 남아있다.&lt;/p&gt;

&lt;h1 id=&quot;4-normalization-in-generator-and-discriminator&quot;&gt;4. normalization in Generator and discriminator&lt;/h1&gt;

&lt;p&gt;GANs are prone the the escalation of signal magnitudes as a result of unhealthy competition between the two networks. Most if not all earlier solutions discourage this by using a variant of batch normalization (Ioffe &amp;amp; Szegedy,2015; Salimans &amp;amp; Kingma, 2016; Ba et al.,2016) in the generator, and often also in the discriminator. &lt;br /&gt;
GAN들은 두가지 네트워크의 경쟁의 비균형적 결과 에 의해 신호크기가 증가하기가 쉽다. 초기가 아닌 대부분 해결책은 제네레이터에서 배치노말에 변형을 주어 이를 감소시켯고, 종종 그것은 discriminator 에도 들어갔다&lt;/p&gt;

&lt;p&gt;These normalization methods were originally introduced to eliminate covariate shift. However we have not observed that to be an issue in GANs, and thus believe that the actual need in GANs is constraining signal magnitudes and competition. We use a different approach that consists of two ingredients, neither of which include learnable parameters.&lt;br /&gt;
이 정규화 기법은 원래 공변량의 이동을 위해 사용되었다. 그러나 우리는 GAN에서 이 이슈가 관찰되지 않았었고, GAN에서 실제로 필요한것은 신호크기와 경쟁의 제한이라고 믿었다. 우린 어떤 학습가능한 파라미터도 포함하지않는 두가지 로 구성된 다른 접근법을 사용하였다 .&lt;/p&gt;

&lt;h2 id=&quot;41-equlized-learning-rate&quot;&gt;4.1 Equlized learning rate&lt;/h2&gt;

&lt;p&gt;We deviate from the current trend of careful weight initialization, and instead use a trivial $\mathcal{N}(0,1)$ initialization and then explicitly scale the weights at runtime. To be precise, we set $\hat{w_i} = w_i/c$, where $w_i$ are the weights and $c$ is the per-layer normalization constant from He’s initializer(He et al.,2015). The benefit of doing this dynamically instead of during initialization is somewhat subtle, and relates to the scale-invariance in commonly used adaptive stochastic gradient descent methods such as RMSProp (Tieleman &amp;amp; Hinton, 2012) and Adam(Kingma &amp;amp; Ba, 2015). These methods normalize a gradient update by its estimated standard deviation, thus making the update independent of the scale of the parameter. &lt;br /&gt;
우리 가중치 초기화를 조심스러워하는 현재 트렌드에서 벗어났다. 그리고 대신 단순한 표준정규분포 초기화를 사용하고 런타임에 가중치를 명시적으로 스케일했다. 정확히하면 가중치를 허 초기화에서 나온 레이어별 정규화 상수로 나누어줬다. 초기화중 대신 동적으로 하는 이점은 다소 미묘하고 RMSProp이나 Adam 처럼 일반적으로 사용되는 SGD 방법과도 관련이 있다. 이 방법들은 그들의 추정된 표준편차를 기준으로 경사 업데이트를 정규화하고 따라서 매개변수의 스케일과 독립적으로 업데이트된다.&lt;/p&gt;

&lt;p&gt;As a result, if some parameters have a larger dynamic range than others, they will take longer to adjust. This is a scenario modern initializers cause, and thus it is possible that a learning rate is both too large and too small at the same time. Our approach ensures that the dynamic range and thus the learning speed, is the same for all weights. A similar reasoning was independently used by val Laarhoven(2017).  &lt;br /&gt;
결과적으로 만약 몇파라미터가 다른것보다 넓은 유동범위를 가지고 있다면 조정되는데 더 오래걸릴 것이다. 이것은 현대의 초기화 방법이 유발하는 현상이고, 따라서 학습률이 동시에 너무 크거나 혹은 너무 작을 수 있다. 우리의 접근은 유동범위와 학습속도가 모든 가중치에서 같은것을 보증한다. 비슷한 연구로는 val Laarhoven의 독립적 사용이 있다.&lt;/p&gt;

&lt;h2 id=&quot;42-pixelwise-feature-vector-normalization-in-generator&quot;&gt;4.2 Pixelwise feature vector normalization in generator&lt;/h2&gt;

&lt;p&gt;To disallow the scenario where the magnitudes in the generator and discriminator spiral out of control as a result of competition, we normalize the feature vector in each pixel to unit length in the generator after each convolutional layer.&lt;br /&gt;
경쟁의 결과로 생성기와 판별기가 통제 불능의 크기가 되는 현상을 막기 위해. 우리는 생성기의 각 컨볼레이어 뒤에 각 픽셀에서 단위 길이까지의 피쳐벡터를 정규화한다.&lt;/p&gt;

&lt;p&gt;We do this using a variant of “local response normalization”(Krizhevsky et al.,2012), configured as $b_{x,y} = a_{x,y} / \sqrt{\frac{1}{N}\sum^{N-1}&lt;em&gt;{j=0}(a^j&lt;/em&gt;{x,y})^2+\epsilon}$, where $\epsilon = 10^{-8}$, $N$ is the number of feature maps, and $a_{x,y}$ and $b_{x,y}$ are the original and normalized feature vector in pixel $(x,y)$, respectively. We find it surprising that this heavy-handed constraint does not seem to harm the generator in any way, and indeed with most datasets it does not change the results much, but it prevents the escalation of signal magnitudes very effectively when needed.&lt;br /&gt;
이것을 위해 Local response normalization 의 변종을 사용하였다.
위 수식과같은, N은 피쳐맵의 수이고 a,b는 각각 픽셀 x,y 에서의 오리지날, 그리고 정규화된 피쳐이다. 우리는 이 강압적인 규제가 어떤식으로든 제네레이터에 해를 끼치지 않는것을 발견했고, 실제로 대부분의 데이터 셋에서 결과를 많이 바꾸지 않지만 필요할때 매우 효과적으로 신호크기의 증가를 방지하였다&lt;/p&gt;

&lt;h1 id=&quot;5-multi-scale-statistical-similarity-for-assessing-gan-results&quot;&gt;5. Multi-scale statistical similarity for assessing GAN results&lt;/h1&gt;

&lt;p&gt;In order to compare the results of one GAN to another, one needs to investigate a large number of images, which can be tedious, difficult, and subjective. Thus it is desirable to rely on automated methods that compute some indicative metric from large image collections. We noticed that existion methods such as MS-SSIM (Odena et al.,2017) find large-scale mode collapses reliably but fail to react to smaller effects such as loss of variation in colors or textures, and they also do not directly assess image quality in terms of similarity to the training set.&lt;br /&gt;
어떤 간과 다른 간의 결과를 비교하는방법은 이미지의 많은 숫자를 살펴야하고, 지루하고, 어렵고, 주관적이다. 따라서 큰이미지 모음에서 나타나는 몇가지 지표들을 계산하는 자동화된 방법에 의존하는것이 바람직하다. 우리는 MS-SSIM같은 현존하는 방법들이 큰 스케일에서의 mode collapse 문제를 감지했지만, 색상이나 텍스쳐 변화의 손실같은 작은 영향에 반응하지 못했다. 그리고 이미지 퀄리티가 학습셋과 유사한 정도에 직접적으로 접근하지 못한다.&lt;/p&gt;

&lt;p&gt;We build on the intuition that a successful generator will produce samples whose local image structure is similar to the training set over all scales. We propose to study this by considering the multiscale statistical similarity between distributions of local image patches drawn from Laplacian pyramid (Burt &amp;amp; Adelson, 1987) representations of generated and target images, starting at a low-pass resolution of $16\times16$ pixels. As per standard practice, the pyramid progressively doubles until the full resolution is reached, each successive level encoding the difference to an up-sampled version of the previous level.&lt;br /&gt;
우리는 직관을 바탕으로 성공적인 생성기를 모든 스케일에 거쳐 이미지 구조가 학습셋과 유사한 샘플을 생성하게 만들었다. 우리는 이것을 위해 생성된 표현의 라플라시안 피라미드에서 끌어낸 지역 이미지 패치들의 분포와 타겟 이미지 간의 다중스케일 통계적 유사도를 고려하는 것을 제안했고 시작은 낮은 해상도인 16픽셀에서 시작하였다. 매 표준 실행마다, 최대해상도에 도달할때까지 두배씩 점차적으로 커지고 각 연속적인 레벨마다 이전 레벨의 업샘플링 버전과의 차이를 인코딩한다.&lt;/p&gt;

&lt;p&gt;A single Laplacian pyramid level corresponds to a specific spatial frequency band. We randomly sample 1634 images and extract 128 descriptors from each level in the Laplacian pyramid, giving us $2^21$(2.1M) discriptors per level. Each descriptor is a $7\times7$ pixel neightborgoot with 3 color channels, denoted by $x\in\mathbb{R}^{7 \times 7 \times 3}= \mathbb{R}^{147}$. We denote the patches from level $l$ of the training set and generated set as ${x^l_{i} }^{2^{21}}&lt;em&gt;{i=1}$ and ${y^l&lt;/em&gt;{i} }^{2^{21}}&lt;em&gt;{i=1}$, respectively. We first normalize ${x^l_i}$ and ${y^l_i}$ w.r.t. the mean and standard deviation of each color channel, and then estimate the statistical similarity by computing their sliced Wasserstein distance $\mathrm{SWD}({x^l_i}, {y^l_i})$, an efficiently computable randomized approximation to earthmovers distance, using 512 projections ( Rabin et al.,2011)&lt;br /&gt;
하나의 라플라시안 피라미드 레벨은 특정한 주파수 대역과 일치한다. 우리는 랜덤하게 1634개의 이미지를 뽑고 라플라시안 피라미드에의 각 레벨에서 추출한 128개의 설명기를 추출하고 각 레벨당 $2^21$개의 디스크립터를 준다. 각 디스크립터는 $7\times7$ 픽셀 과 3가지 색상 채널이고 $x\in\mathbb{R}^{7 \times 7 \times 3}= \mathbb{R}^{147}$ 로 표기한다. 우리는 학습셋의 $l$번째 레벨에서 나온 패치와 생성된 셋을 각각 ${x^l&lt;/em&gt;{i} }^{2^{21}}&lt;em&gt;{i=1}$ , ${y^l&lt;/em&gt;{i} }^{2^{21}}_{i=1}$ 로 표기한다. 
첫번째로 ${x^l_i}, {y^l_i}$ 에 대해 노말라이즈 하고 각 채널에 대해 평균과 표준편차를 구한다. 그뒤 유사도를 측정하는데 Sliced Wasserstein distance SWD를 사용한다. 효과적으로 계산할 수 있는 무작위화한 지형이동거리로 512차원을 이용했다.&lt;/p&gt;

&lt;p&gt;Intiitively a small Wasserstein distance indicates that the distribution of the patches is similar, meaning that the training images and generator samples appear similar in both appearance and variation at this spatial resolution. In particular, the distance between the patch sets extracted from the lowest resolution $16\times16$images indicate similarity in large-scale image structures, while the finest-level patches encode information about pixel-level attributes such as sharpness of edges and noise.&lt;br /&gt;
직관적인 작은 W거리는 패치들이 얼마나 비슷한지를 표시하고, 이 공간 해상도에서 학습이미지와 생성 샘플의 외형및 변화가 비슷하게 보이는것을 의미한다. 특별히 16의 저해상도 이미지에서 추출된 패치사이의 거리는 큰 이미지 구조에서 유사도를 나타낸다. 가장 좋은 수준의의 패치는 선과 잡음의 날카로움 같은 픽셀레벨 특성에 대한 정보를 인코딩한다&lt;/p&gt;

&lt;h1 id=&quot;6-experiments&quot;&gt;6. Experiments&lt;/h1&gt;

&lt;p&gt;In this section we discuss a set of experiments that we conducted to evaluate the quality of our results. Please refer to Appendix A for detailed description of our network structures and training configurations. We also invite the reader to consult the accompanying video([https://youtu.be/G06dEcz-QTg])for additional result images and latent space interpolations. In this section we will distinguish between the network structure(e.g., convolutional layers, resizing), training configuration (various normalization layers, minibatch-related operations), and training loss (WGAN-GP, LSGAN)&lt;br /&gt;
이 섹션에서 우리는 우리의 결과의 퀄리티를 평가하기 행해진 실험들에 대해 토론한다. 부록 A 참조하면 우리 작업 구조와 학습 환경에 대해 세부적으로 설명되어있다. 우리는 또한 상담을 위해 리더를 초대하여 동반한 비디오를 올렸고 추가적인 결과 이미지와 latent space 에 대해 나와있다. 이 섹션에서 네트워크 구조, 학습환경, 그리고 학습 손실에 대해 다른 네트워크와의 차이를 말할것이다.&lt;/p&gt;

&lt;h2 id=&quot;61-importance-of-individual-contributions-in-terms-of-statistical-similarity&quot;&gt;6.1 Importance of Individual contributions in terms of statistical similarity&lt;/h2&gt;

&lt;p&gt;We will first use the sliced Wasserstein distance (SWD) and multi-scale structural similarty (MS-SSIM)(Odena et al.,2017) to evaluate the importance our individual contributions, and also perceptually validate the metrics themselves. We will do this by building on top of a previous state-of-the-art loss function(WGAN-GP) and training configuration (Gulrajani et al.,2017) in an unsupervised setting using CELEBA (Liu et al.,2015) and LSUN BEDROOM (Yu et al.,2015) datasets in $128^2$ resolution. CELEBA is particularly well suited for such comparison because the training images contain noticeable artifacts(aliasing, compression, blur) that are difficult for the generator to reproduce faithfully. In this test we amplify the differences between training configurations by choosing a relatively low-capacity network structure (Appendix A.2) and termination the training once the discriminator has been shown a total of 10M real images. As such the results are not fully converged.&lt;br /&gt;
첫번째로 SWD를 사용하고 MS-SSIM를 우리의 개별 평가를 위해 사용했다. 그리고 또한 지각적으로 그 평가지표들을 입증하였다. 우리는 그것을 위해 기존 최고성능인 WGAN-GP 로스와 128해상도의 CELEBA 와 LSUN BEDROOM 데이터의 학습환경을 사용하였다. CELEBA 는 이 비교에 특히 잘 적합되었다. 왜냐하면 학습 이미지는 뚜렷한 생성기가 뚜렷하게 복사하기 어려운 아티팩트를 포함하기 때문이다. 이 실험에서 학습환경의 차이를 증폭시키기 위해 상대적으로 적은 용량의 네트워크구조를 선택하고 판별기가 10M의 이미지를 보면 학습을 종료시켰다. 그렇게해서 결과가 완전히 수렴되진 않는다.&lt;/p&gt;

&lt;p&gt;Table 1 lists the numerical values for SWD and MS-SSIM in several training configurations, where our individual contributions are cumulatively enabled one by on top of the baseline (Gulrajani et al.,2017). The MS-SSIM numbers were averaged from 10000 pairs of generated images, and SWD was calculated as described in Sections 5. Generated CELEBA images from these configurations are shown in Figure 3. Due to space constraints, the figure shows only a small number of examples for each row of the table, but a significantly broader set is available in Appendix H. Intuitively, a good evaluation metric should reward plausible images that exhibit plenty of variation in colors, textures, and viewpoints. However, this is not captured by MS-SSIM:we can immediately see that configuration (h) generates significantly better images than configuration (a), but MS-SSIM remains approximately unchanged because it measures only the variation between outputs, not similarity to the training set. SWD, on the other hand, does indicate a clear improvement.&lt;br /&gt;
Table1은 몇개의 환경에서 SWD와 MS-SSIM 의 값 리스트이고, 각각의 환경은 베이스라인에서 하나씩 누적된다. MS-SSIM 값은 10000 쌍의 생성된 이미지의 평균이고, SWD 는 섹션5에서 서술한 방식으로 계산되었다. 이 환경들에서 생성된 CELEBA 이미지는 Figure3에 서 보여진다. 
공간제약 때문에, 테이블의 각행에서 적은 샘플만을 보여주지만, 많은 사진을 Appendix H 에서 볼수 있다.
직관적으로 좋은 평가 측도는 색상, 텍스쳐, 시점에서 충분한 변화를 보여주는 그럴듯한 이미지에 보상을 줘야한다. 하지만 이것은 MS-SSIM은 그런 부분이 감지되지 못하고, 우리는 (a)보다 (h)가 확실히 더 나은 이미지를 생성하는것을 즉시 알수있지만 MSSIM은 출력간의 변화만 측정하기에 거의 변하지 않는다, 학습셋의 유사도와 는 다르게. 반면에 SWD는 분명한 향상을 보여준다&lt;/p&gt;

&lt;p&gt;The first training configuration (a) corresponds to Gulrajani et al.(2017), featuring batch normalization in the generator, layer normalization in the discriminator, and minibatch size of 64. (b) enables progressive growing of the networks, which result in sharper and more believable output images. SWD correctly finds the distribution of generated images to be more similar to the training set.&lt;br /&gt;
첫번째 학습환경인 (a) 는 Gulrajani의 연구에 해당한다. 생성기에서 피쳐 배치노말을 하고, 판별기에선 레이어 노말, 그리고 미니배치사이즈는 64 이다. (b) 는 점점 PG 네트워크를 사용, 더 날카롭고 믿을만한 결과가 나왔다. SWD도 정확히 학습셋에 더 맞는 생성 이미지 분포를 찾았다.&lt;/p&gt;

&lt;p&gt;Our primary goal is to enable high output resolutions, and this requires reducing the size of minibatches in order to stay within the available memory budget. We illustrate the ensuing challenges in (c) where we decrease the minibatch size from 64 to 16. The generated images are unnatural, which is clearly visible in both metrics. In (d), we stabilize the training process by adjusting the hyperparameters as well as by removing batch normalization and layer normalization (Appendix A.2). As an intermediate test (e&lt;em&gt;), we enable minibatch discrimination (Salimans et al.,2016), which somewhat surprisingly fails to improve any of the metrics, including MS-SSIM that measures output variation. In contrast, our minibatch standard deviation (e) improves the average SWD scores and images. We then enable our remaining contributions in (f) and (g), leading to an overall improvement in SWD and subjective visual quality. Finally, in (h) we use a non-crippled network and longer training - we feel the quality of the generated images is at least comparable to the best published results so far.&lt;br /&gt;
우리의 첫번째 목표는 가능한 출력 해상도를 높이는 것이고, 이때 사용가능한 메모리 범위 내에서 미니배치를 줄이는것이 필요하다. (c)를 그릴때 미니배치를 62에서 16으로 감소시켰다. 생성된 이미지들은 
두 지표에서 명확히 보이듯 비정상적이다. (d) 에서는 학습과정에서 하이퍼파라미터를 잘 조정하고 배치노말과 레이어노말을 제거하여 안정되었다. (e&lt;/em&gt;) 에서는 minibatch discrimination 을 사용했고 어떤 측도도 눈에띄게 개선시키지 못했다. 반면 우리의 미니배치 표준편차법을 적용한 (e)는 평균 SWD 및 이미지를 향상시켰다. 우리의 기법들을 추가한 (f)와 (g), SWD와 이미지 퀄리티에서 전체적인 개선을 이끌었고 마지막으로 (h)에서 자르지 않은 네트워크와 긴 학습을 거쳤다. 우리는 생성된 이미지중 지금까지 결과중 가장 좋다고 생각한다.&lt;/p&gt;

&lt;h2 id=&quot;62-convergence-and-training-speed&quot;&gt;6.2 Convergence And Training Speed&lt;/h2&gt;

&lt;p&gt;Figure 4 illustrates the effect of progressive growing in terms of the SWD metric and raw image throughput. The first two plots correspond to the training configuration of Gulrajani et al.,(2017) without and with progressive growing. We observe that the progressive variant offers two main benefits : it converges to a considerable better optimum and also reduces the total training time by about a factor of two. The improved convergence is explained by an implicit form of curriculum learning that is imposed by the gradually increasing network capacity. Without progressive growing, all layers of the generator and discriminator are tasked with simultaneously finding succinct intermediate representations for both the large-scale variation and the small-scale detail. With progressive growing, however, the existing low-resolution layers are likely to have already converged early on, so the networks are only tasked with refining the representations by increasingly smaller-scale effects as new layers are introduced. Indeed, we see in Figure 4(b) that the lagest-scale statistical similarity curve (16) reaches its optimal value very quickly and remains consistent throughout the rest of the training. The smaller-scale curves(32,64,128) level off one by one as the resolution is inreased, but the convergence of each curve is equally consistent. With non-progressive training in Figure4(a), each scale of the SWD metric converges roughly in unison, as could be expected.&lt;br /&gt;
Figure 4는 SWD와 이미지 처리량에 대한 PG의 영향을 그리고 있다. 앞의 두 그림은 PG가 없는구라자니(WGAN-GP)의 학습환경과 일치한다. 단계별 변화가 제공하는 두가지 이점을 관측했다 : 그것은 매우 좋은 최적점으로 수렴하고 전체 학습시간을 2배가량 감소시켰다. 개선된 수렴은 점차적으로 네트워크 용량이 증가함으로 부가된 학습 과정의 암시적인 형태로 설명된다. PG없이 생성기와 판별기의 모든 레이어는 
작은 스케일에서의 디테일과 큰 스케일에서의 변화 양쪽의 가운데 표현을 간결한 중간표현을 동시에 찾는 작업을 수행한다. 그러나 PG와 함께하면 저해상도 레이어들은 미리 수렴되어버리고, 네트워크는 오직 새로운 레이어가 도입됨에 따라 점점 더 작은 효과에 의해 표현을 정제하는 작업을 하게된다. 실제로 Figure 4 에서 가장큰규모의 유사도 곡선은 최적의값에 매우 빨리 도달하고 남은 훈련 내내 유지되는것을 볼수있다. 작은 스케일에서 커브들은&lt;/p&gt;

&lt;p&gt;Teh speedup from progressive growing increases as the output resolution grows. Figure4(c) shows training progress, measured in number of real images shown th the discriminator, as a function of training time when the training progresses all the way to $1024^2$ resolution. We see that progressive growing gains a significant head start because the networks are shallow and quick to evaluate at the begining. Once the full resolution is reached, the image throughput is equal between the tWo methods. The plot shows that the progressive variant reaches approximately 6.4 million images in 96 hours, whereas it can be extrapolated that the non-progressive variant would take about 520 hours to reach the same point. In this case, the progressive growing offers roughly a $5.4\times$ speedup&lt;/p&gt;

&lt;h2 id=&quot;63-high-resolution-image-generation-using-celeba-hq-dataset&quot;&gt;6.3 High-Resolution Image Generation Using CELEBA-HQ Dataset&lt;/h2&gt;

&lt;p&gt;To meaningfully demonstrate our results at high output resolutions, we need a sufficiently varied high-quality dataset. However, virtually all publicly available datasets previously used in GAN literature are limited to relatively low resolutions ranging from $32^2$ to $480^2$. To this end, we created a high-quality version of the CELEBA dataset consisting of 30000 of the images at $1024 \times 1024$resolution. We refer to Appendix C for further details about the generation of this dataset.&lt;/p&gt;

&lt;p&gt;Our contributions allow us to deal with high output resolutions in a robust and efficient fashion. Figure 5 shohws selected $1024\times 1024$ images produced by our network. While megapixel GAN results have been shown before in another dataset(Marchesi, 2017), our results are vastly more varied and of higher perceptual quality. Please refer to Appendix F for a larger set of result images as well as the nearest neighbors found from the training data. The accompanying video shows latent space interpolations and visualizes the progressive training. The interpolation works so that we first randimize a latent code for each frame (512 components sampled individually from $\mathcal{N}(0,1))$, then blur the latents across time with a Gaussian ($\sigma = 45$ frames @ 60 Hz), and finally normalize each vector to lie on a hypersphere.&lt;/p&gt;

&lt;p&gt;We trained the network on 8 Tesla V100 GPUs for 4 days, after which we no longer observed qualitative differences between the result of consecutive training iterations. Our implementation used an adaptive minibatch size depending on the current output resolution so that the available memory budget was optimally utilized.&lt;/p&gt;

&lt;p&gt;In order to demonstrate that our contributions are largely orthogonal to the choice of a loss function, we also trained the same network using LSGAN loss insted of WGAN-GP loss. Figure 1 shows six examples of $1024^2$ images produced using our method using LSGAN. Futher details of this setup are given in Appendix B.&lt;/p&gt;

&lt;h2 id=&quot;64-lsun-results&quot;&gt;6.4 LSUN Results&lt;/h2&gt;

&lt;p&gt;Fugure 6 shows a purely visual comparison between our solution and earlier results in LSUN BEDROOM. Fiture 7 gives selected examples from seven very different LSUN categories at $256^2$. A larger, non-curated set of results from all 30 LSUN categories is available in Appendix G, and the video demonstrates interpolations. We are not aware of earlier results in most of these categories, and while some categories work better than others, we feel that the overall quality is high.&lt;/p&gt;

&lt;h2 id=&quot;65-cifan10-inception-scores&quot;&gt;6.5 CIFAN10 Inception Scores&lt;/h2&gt;

&lt;p&gt;The best inception scores for CIFAR10 (10 categories of $32\times 32$ RGB images) we are aware of are 7.90 for unsupervised and 8.87 for label conditioned setups (Grinblat et al., 2017). The large difference between the two numbers is primarily caused by “ghosts” that necessarily appear between classes in the unsupervised setting while label conditioning can remove many such transitions.&lt;/p&gt;

&lt;p&gt;When all of our contributions are enabled, we get 8.80 in the unsupervised setting. Appendix D shows a representative set of generated images along with a more comprehensive list of results from earlier methods. The network and training setup were the same as for CELEBA, progression limited to $32 \times 32$ of course. The only customization was to the WGAN-GP’s regularization term&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">Abstract</summary></entry><entry><title type="html">파이썬에서 OpenCV 사용</title><link href="http://localhost:4000/opencv/2019/07/12/opencv-python.html" rel="alternate" type="text/html" title="파이썬에서 OpenCV 사용" /><published>2019-07-12T10:05:05+09:00</published><updated>2019-07-12T10:05:05+09:00</updated><id>http://localhost:4000/opencv/2019/07/12/opencv-python</id><content type="html" xml:base="http://localhost:4000/opencv/2019/07/12/opencv-python.html">&lt;h1 id=&quot;opencv&quot;&gt;OpenCV&lt;/h1&gt;

&lt;p&gt;컴퓨터 비전을 위한 오픈소스 라이브러리이다
여러 언어에서도 다 사용가능하고 강력하다하니 배워보자&lt;/p&gt;

&lt;h1 id=&quot;설치&quot;&gt;설치&lt;/h1&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ pip install opencv-python
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;끗&lt;/p&gt;

&lt;h1 id=&quot;불러오기&quot;&gt;불러오기&lt;/h1&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import cv2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;끗&lt;/p&gt;

&lt;p&gt;참고로 따로 창이 생성되어 작업이되는게 많아 jupyter 환경에서는 좀 문제가 많이생기니
VSCode 나 파이참같은 환경을 추천
(물론 피해가는 방법도 있는데 이런것은 추천하지않는다 언제 막힐지 몰라서…)&lt;/p&gt;

&lt;h1 id=&quot;세부-함수&quot;&gt;세부 함수&lt;/h1&gt;

&lt;h2 id=&quot;이미지-함수들&quot;&gt;이미지 함수들&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 이미지 불러오는 함수
image = cv2.imread('이미지 파일')

# 이미지 보여주는 함수
# 참고로 주피터는 사용 불가 plt.imshow 추천
cv2.imshow('Test', image)
cv2.waitKey(0)
cv2.destroyAllWindows()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;영상-함수들&quot;&gt;영상 함수들&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 영상 불러오는 함수
vidcap = cv2.VideoCapture('비디오 파일')

# 영상 한프레임 받아오기
# 자동으로 다음 프레임상태로 넘어감
# 더이상 영상이없으면 ret가 False 출력
# 루프문으로 적당히 돌려주면 된다
# frame 은 ndarray 형태로 바로 사용가능
ret, frame = vidcap.read()

# 메모리 해제
vidcap.release() 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;카메라-함수들&quot;&gt;카메라 함수들&lt;/h1&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 카메라 연결하는 함수
vidcap = cv2.VideoCapture( camera_num )

# 영상 설정
vidcap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
vidcap.set(cv2.CAP_PROP_FRAME_HEIGHT, width)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Aru</name></author><summary type="html">OpenCV</summary></entry><entry><title type="html">Few-Shot Adversarial Learning of Realistic Neural Talking Head Models</title><link href="http://localhost:4000/paper/2019/07/11/Few-Shot.html" rel="alternate" type="text/html" title="Few-Shot Adversarial Learning of Realistic Neural Talking Head Models" /><published>2019-07-11T10:05:05+09:00</published><updated>2019-07-11T10:05:05+09:00</updated><id>http://localhost:4000/paper/2019/07/11/Few-Shot</id><content type="html" xml:base="http://localhost:4000/paper/2019/07/11/Few-Shot.html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural netrworks to generate them.&lt;br /&gt;
몇몇 요새 작업은 컨볼넷을 학습시켜 만든 높은 현실감을 가진 사람의 머리 이미지를 얻었다&lt;/p&gt;

&lt;p&gt;In order to crteate a personalized talking head model, these works require training on a large dataset of images of a single person&lt;br /&gt;
개인이 말하는 머리모델을 만들기 위해서는 한사람의 많은 데이터를 학습할 필요가 있었다&lt;/p&gt;

&lt;p&gt;However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image.&lt;br /&gt;
그러나 많은 상황에서 이 말하는 머리모델은 몇장의 이미지만으로 학습을 해야했고, 심지어 한장의 이미지로도 해야했다.&lt;/p&gt;

&lt;p&gt;Here, we present a system with such few-shot capability. &lt;br /&gt;
우리는 퓨샷 능력을 가진 시스템을 소개한다&lt;/p&gt;

&lt;p&gt;It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators.&lt;br /&gt;
큰 비디오 데이터셋에 대한 장기 학습을 수행하면 이후 큰 규모의 generator와 discriminator 의 적대적 훈련으로 몇장 혹은 한장으로 이전까지 할 수 없던 말하는 머리모델이 가능해진다&lt;/p&gt;

&lt;p&gt;Crucially the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters.&lt;br /&gt;
결정적으로 이 시스템은 generator 와 discriminator 사람 마다 특유의 방법으로 매개변수를 초기화할수 있고 몇장의 이미지를 기반으로 빠른속도로 학습이 가능하다
천만개의 파라미터만 사용하는데도 불구하고&lt;/p&gt;

&lt;p&gt;We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings.&lt;br /&gt;
이 접근법은 높은 현실성과 개인성을 가진 말하는 모델을 학습하는것을 가능하게한다. 새로운 사람이나 심지어 초상화를 가지고&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In this work, we consider the task of creating personalized photorealistic talking head models, i.e. systems that can synthesize plausible video-sequences of speech expressions and mimics of a particular individual.&lt;br /&gt;
이 작업에서 개인적인 사실주의적인 말하는 모델을 만드는것을 고려했다, 예를들면 특정인의 흉내와 연설 비디오를 그럴듯하게 합성해 낼수 있다.&lt;/p&gt;

&lt;p&gt;More specifically, we consider the problem of synthesizing photorealistic personalized head images given a set of face landmarks, which drive the animation of the model.&lt;br /&gt;
더 구체적으로 모델의 애니메이션을 구동하는 얼굴의 랜드마크세트를 주어 사실적인 이미지의 합성 문제를 고려했다.&lt;/p&gt;

&lt;p&gt;Such ability has practical applications for telepresence, including video conferencing and multi-playergames, as well as special effects industry. 이 능력은  telepresence의 실용적인 어플을 위한 능력을 가지고 있다. 화상채팅이나 멀티게임, 특수효과산업까지 포함해서&lt;/p&gt;

&lt;p&gt;Synthesizing realistic talking head sequences is known to be hard for two reasons.&lt;br /&gt;
사실적 말하는 머리 시퀸스를 만들기는 두가지 이유때문에 어려운것으로 알려져있다.&lt;/p&gt;

&lt;p&gt;First, human heads have high photometric, geometric and kinematic complexity.&lt;br /&gt;
첫번째로 휴먼의 머리는 광도, 기하, 운동학적 복합성을 가지고 있다&lt;/p&gt;

&lt;p&gt;This complexty stems not only from modeling faces(for which a large number of modeling approaches exist) but also from modeling mouth cavity, hair, and garments.&lt;br /&gt;
이 복잡도는 얼굴 모델링 뿐만 아니라 입 구멍, 머리 , 그리고 옷까지 해야한다&lt;/p&gt;

&lt;p&gt;The second complicating factor is the acuteness of the human visual system towards even minor mistakes in the appearance modeling of human heads (the so-called uncanny vally effect).&lt;br /&gt;
두번째로 복잡하게 하는 요인은 휴먼헤드모델링에서 생기는 작은 실수에 대한 인간 시각시스템의 날카로움이다(언캐니 밸리 이펙트라 불린다)&lt;/p&gt;

&lt;p&gt;Such low tolerance to modeling mistakes explains the current prevalence of non-photorealistic cartoon-like avatars in man practically-deployed teleconferencing systems.&lt;br /&gt;
이 모델링 실수들에 대한 적은 내성 때문에 
실제 사용되는 원격회의에서 만화같은 비현실적 아바타가 유행하는것이 설명된다&lt;/p&gt;

&lt;p&gt;To overcome the challenges, several works have proposed to synthesize articulated head sequences by warping a single or multiple static frames.&lt;br /&gt;
이 문제를 극복하기 위해, 몇몇 작업은 단일 혹은 복수의 정적 프레임을 휘는 방식으로 관절이 있는 머리에 합성하는것을 제안했다.&lt;/p&gt;

&lt;p&gt;Both classical warping algorithms and warping fields synthesized using machine learning(include deep learning) can be used for such purposes.&lt;br /&gt;
기계학습을 사용하여 합성된 기존의 워핑 알고리즘과 워핑공간 모두 이 목적으로 사용될 수 있다&lt;/p&gt;

&lt;p&gt;While warping-based systems can create talking head sequences from as little as a single image, the amount of motion, head rotation, and disocclusion that they can handle without noticeable artifact is limited.&lt;br /&gt;
워핑 기반 시스템은 단일 이미지에서 말하는 머리 영상을 만들수 있으나, 구체적인 인공조작 없이 처리할수 있는 전체 움직임이나 머리의 회전의 해제에 있어서는 제한적이다&lt;/p&gt;

&lt;p&gt;Direct(warping-free) Synthesis of video frames using adversarially-trained deep convolutional networks(ConvNets) presents the new hope for photorealistic talking heads.&lt;/p&gt;

&lt;p&gt;적대적 컨볼넷을 사용한 비디오 프레임의 직접 합성은 사실적 말하는 머리의 새로운 희망을 제시했다.&lt;/p&gt;

&lt;p&gt;Very recently, some remarkably realistic results have been demonstrated by such system [16,20,37]. However, to succeed, such methods have to train large networks, where both generator and discriminator have tens of millions of parameters for each talking head.&lt;br /&gt;
아주 최근에 매우 현실적인 결과가 위 시스템에 의해 증명되었다. 그러나 성공을 위해서는 위 메소드들은 많은 학습시간을 필요로 했고 생성기와 판별기 는 천만개의 파라미터가 필요하다 각각 머리에 대하여&lt;/p&gt;

&lt;p&gt;These systems, therefore, require a several-minutes-long video [20,37] or a large dataset of photographs [16] as well as hours of GPU training in order to create a new personalized talking head model.&lt;br /&gt;
이 시스템들은 게다가 새로운 사람의 머리모델을 만들러면 몇분의 긴 비디오를필요로 하거나 많은 사진 데이터셋과 시간단위의 GPU 트레이닝이 필요하다&lt;/p&gt;

&lt;p&gt;While this effort is lower than the one required by systems that construct photo-realistic head models using sophisticated physical and optical modeling [1], it is still excessive for most practical telepresence scenarios, where we want to enable users to create their personalized head models with as little effort as possible.&lt;br /&gt;
이 과정은 정교한 물리 및 광학 모델링을 사용해 구축하는거보다 낮음지만 여전히 
사용자가 가능한 적은 노력으로 그들 개인의 머리모델을 만들어 사용하고자 하는 대부분에 실제 화상회의에는 너무 과한 학습이 요구된다&lt;/p&gt;

&lt;p&gt;In this work, we present a system for creating talking head models from a handful of photographs(so-called few shot learning) and with limited training time. In fact, our system can generate a reasonable result based on a single photograph(one-shot learning), while adding a few more photographs increases the fidelity of personalization.&lt;br /&gt;
이 작업에서 제한된 시간에서 몇장의 사진을 가지고 말하는 머리모델을 만드는 시스템을 제시한다
사실 이 시스템은 한장의 사진을 기반으로 해도 합리적인 결과가 나오고 사진의 추가는 개인화를 더 추가해주는 것이다&lt;/p&gt;

&lt;p&gt;Similarly to [16,20,37], the talking heads created by our model are deep ConvNets that synthesize video frames in a direct manner by a sequence of convolutinal operations rather than by warping.&lt;br /&gt;
위 논문들과 비슷하게 말하는 머리는 워핑에 의하기보단 컨볼루션 시퀸스를 직접 합성하는 방식의 컨볼넷으로 만들어 진다.&lt;/p&gt;

&lt;p&gt;The talking heads created by our system can, therefore, handle a large vairety of poses that goes beyond the abilities of warping-based systems.&lt;br /&gt;
말하는 머리는 우리의 시스템에 의해 생성되고, 워핑베이스 시스템의 능력을 뛰어넘는 다양한 포즈를 할수 있다.&lt;/p&gt;

&lt;p&gt;The few-shot learning ability is obtained through extensive pre-training (meta-learning) on a large corpus of talking head videos corresponding to different speakers with diverse appearance.&lt;br /&gt;
퓨샷러닝 능력은 큰 뭉치의 다양한외모의 사람들에 말하는 머리 비디오를 이용해 광범위한 사전 훈련을 통해 얻는다.&lt;/p&gt;

&lt;p&gt;In the course of meta-learning, our system simulates few-shot learning tasks and learns to transform landmark positions into realistically-looking personalized photographs, given a small training set of images with this person.&lt;br /&gt;
이 사전훈련 과정에 우리 시스템은 퓨샷러닝 작업을 시뮬레이트하고 어떤 사람과의 이미지셋을 통한 작은 훈련으로 랜드마크 포지션을 실제처럼 보이는 개인사진으로 바꾸는 방법을 배운다.&lt;/p&gt;

&lt;p&gt;After that, a handful of photographs of a new person sets up a new adversarial learning probelm with high-capacity generator and discriminator pre-trained via meta-laerning.&lt;br /&gt;
이후 적은 수의 새로운 사람의 사진으로 
메타 러닝을 통해 사전학습된 고용량의 생성기와 판별기로 새로운 적대적 학습을 한다&lt;/p&gt;

&lt;p&gt;The new adversarial problem converges to the state that generates realistic and personalized images after a few training steps.&lt;br /&gt;
새로운 적대적문제는 몇 훈련 스텝 이후 현실적이고 개인화된 이미지를 만드는 상태로 수렴한다&lt;/p&gt;

&lt;p&gt;In the experiments, we provide comparisons of talking heads created by our system with alternative neural talking head models [16,40] via quantitative measurements and a user study, where our approach generates images of sufficient realism and personalization fidelity to deceive the study participants.&lt;br /&gt;
이 실험에서 우리의 시스템과 대립되는 뉴럴 말하는 모델의 비교를 정량적 측정과 사용자 연구를 통해 제공하고, 우리의 접근방식은 연구 참가자들을 속이기 위해 사실적이고 개인의 특성이 충분히 들어간 이미지를 생성하였다&lt;/p&gt;

&lt;p&gt;We demonstrate several uses of our talking head models, including video synthesis using landmark tracks extracted from video sequences of the same person, as well as puppeteering (video snthesis of a certain person based on the face landmark tracks of a different person)&lt;br /&gt;
예제 뿐만 아니라 동일인의 비디오에서 추출한 랜드마크 트랙을 사용하여 합성한 비디오를 포함하여 
모델을 몇 차례 사용해보는것으로 증명하였다.
(특정인의 비디오 합성은 다른사람의 랜드마크 트랙을 기반으로 하였다)&lt;/p&gt;

&lt;h1 id=&quot;2-related-work&quot;&gt;2. Related work&lt;/h1&gt;

&lt;p&gt;A huge body of works is devoted to statistical modeling of the apperance of human faces [6], with remarkably good results obtained both with classical techniques [35] and, more recently, with deep learning &lt;a href=&quot;to name just a few&quot;&gt;22,25&lt;/a&gt;.&lt;br /&gt;
거대한 작품들이 인간얼굴의 외관을 통계적으로 모델링하는데 기여하고 있다. 고전적인 기법과 좀더 최근의 딥러인을 이용한 기법 모두 현저히 좋은 결과.&lt;/p&gt;

&lt;p&gt;While modeling faces is a highly related task to talking head modeling, the two tasks are not identical, as the latter also involves modeling non-face pats such as hair, neck, mouth cavity and often shoulders/upper garment.&lt;br /&gt;
얼굴 모델링은 말머리모델과 높은 관계를 가지지만 두 작업은 동일하지 않다, 후자는 머리, 목, 입속이나 종종 어깨나 상의처럼 얼굴이 아닌 부분도 포함한다&lt;/p&gt;

&lt;p&gt;These non-face parts cannot be handled by some trivial extension of the face modeling methods since they are much less amenable for registration and often have higher variability and higher complexity than the face part.&lt;br /&gt;
이 비얼굴 부분은 얼굴 모델링 메소드의 사소한 확장정도로 다뤄지며 안되는데, 그것들은 등록하기 쉽지않고 종종 얼굴파트보다 훨신 더 변화가 많고 높은 복잡성을 가지기 때문이다&lt;/p&gt;

&lt;p&gt;In principle, the results of face modeling [35]or lips modeling [31] can be stitched into an existing head video.&lt;br /&gt;
원칙적으로 얼굴이나 입술모델의 결과는 기존 얼굴헤드 모형에 삽입할수있다.&lt;/p&gt;

&lt;p&gt;Such design, however, does not allow full control over the head rotation in the resulting video and therefore does not result in a fullyfledged talking head system.&lt;br /&gt;
하지만 이 디자인은 결과비디오에서 머리 회전에 대한 완전한 조작을 통제룰 허락하지 못하고 그 결과 완전한 말머리 시스템이 만들어지지 않는다&lt;/p&gt;

&lt;p&gt;The design of our system borrows a lot from the recent progress in generative modeling of images. &lt;br /&gt;
우리의 시스템 디자인은 일반적인 이미지 모델의 최근의 동향을 많이 빌려온다&lt;/p&gt;

&lt;p&gt;Thus, our architecture uses adversarial training [12] and, more specifically, the ideas behind conditional discriminators [23], including projection discriminators [32]. &lt;br /&gt;
구조는 적대적 학습이고 조금 구체적으로는 프로젝션 판별기를 포함한 조건부 판별기 기법이다&lt;/p&gt;

&lt;p&gt;Our meta-learning stage uses the adaptive instance normalization mechanism [14], which was shown to be useful in large-scale conditional generation tasks [2,34].&lt;br /&gt;
메타러닝 부분은 노말라이제이션 대신 adaptive 방식을 사용했고 그것은 큰규모의 조건부 생성작업에서 유용한것을 보여준다.&lt;/p&gt;

&lt;p&gt;The model-agnostic meta-learner(MAML) [10] uses meta-learning to obtain the initial state of an image clasifier, from which it can quickly converge to image classifiers of unseen classes, given few training samples.&lt;br /&gt;
이미지분류기의 초기값을 얻기 위해 메타러닝을 사용한다, 이것은 MAML 는 훈련샘플이 별로 없는 경우 미지의 클래스로의 이미지 분류를 위한 빠른 수렴이 가능하게 한다&lt;/p&gt;

&lt;p&gt;This high-level idea is also utilized by our method, though our implementation of it is rather different. 
이 고수준 아이디어도 우리의 메소드에 의해 활용되었다. 조금 다르긴 하지만.&lt;/p&gt;

&lt;p&gt;Several works have further proposed to combine adversarial training with meta-learning. 
몇몇의 작업은 메타러닝과 적대적학습의 조합으로 제안되었다&lt;/p&gt;

&lt;p&gt;Thus, data-augmentation GAN [3] , Meta-GAN[43], adversarial meta-learning [41] use adversarially-trained networks to generate additinal examples for classes unseen at the meta-learning stage. &lt;br /&gt;
따라서 위 세 연구에서 선행학습 단계에서 미지의 클래스 분류를 위한 추가적인 샘플 생성에서 적대적 학습 네트워크가 사용되었다&lt;/p&gt;

&lt;p&gt;While these methods are focused on boosting the few-shot classification performance, our method deals with the training of image generation models using similar adversarial objectives. &lt;br /&gt;
그 방법들은 몇장을써서 분류하는것의 성능을 높이는것에 집중하지만, 우리방법은 similar adversarial objectives 를 사용하여이미지를 생성 모델을 학습 하는 것을 다루었다.&lt;/p&gt;

&lt;p&gt;To summarize, we bring the adversarial fine-tuning into the meta-learning framework. &lt;br /&gt;
요약하면 우리는 적대적 fine-tuning 을 메타러닝 프레임워크에 집어 넣었다.&lt;/p&gt;

&lt;p&gt;The former is applied after we obtain initial state of the generator and the discriminator networks via the mta-learning stage.&lt;br /&gt;
전자는 메타러닝 스테이지를 통해 생성기와 판별기를 초기상태를 얻은 후에 적용된다.&lt;/p&gt;

&lt;p&gt;Finally, very related to ours are the two recent works on text-to-speech generation [4,18].&lt;br /&gt;
마지막으로 매우 밀접한 것으로는 텍스트로 음성을 생성하는 두가지 작업이 있다.&lt;/p&gt;

&lt;p&gt;Their setting (few-shot learning of generative model) and some of the components (standalone embedder network, generator fine-tunning) are also used in our case.&lt;br /&gt;
그들의 세팅과 몇몇 구성요소는 우리 케이스에도 쓰인다.&lt;/p&gt;

&lt;p&gt;Our work differs in the application domain, the use of adversarial learning, its specific adaptation to the meta-learning process and numerous implementation details.&lt;br /&gt;
우리의 작업은 어플리케이션 영역, 적대적학습의 사용, 메타러닝 과정의 특정 적용방식과 수많은 세부사항에서 다르다.&lt;/p&gt;

&lt;h1 id=&quot;3-methods&quot;&gt;3. Methods&lt;/h1&gt;

&lt;h2 id=&quot;31-architecture-and-notation&quot;&gt;3.1 Architecture and notation&lt;/h2&gt;

&lt;p&gt;The meta-learning stage of our approach assumes the availability of M video sequences, contaning talking heads of different people.&lt;br /&gt;
우리의 접근방식은 메타러닝 단계에서 다른 사람들의 말머리가 들어있는 M 영상의 유용성을 가정한다&lt;/p&gt;

&lt;p&gt;We denote with $\mathrm{x}_i$ the $i$-th video sequence and with $\mathrm{x}_i(t)$ ist $t$-th frame.&lt;br /&gt;
$x_i$는 i번째 비디오를, $\mathrm{x}_i(t)$는 그것의 t 번째 스텝을 나타낸다&lt;/p&gt;

&lt;p&gt;During the learning process, as well as during test time, we assume the availability of the face landmarks’ locations for all frames (we use an off-the-shelf face alignment code [[7] to obtain them).&lt;br /&gt;
테스트타임 뿐만아니라, 학습단계에서 모든 프레임에 대한 페이스 랜드마크 위치의 유용성을 가정했다&lt;/p&gt;

&lt;p&gt;The landmarks are rasterized into three-channel images using a predefined set of colors to connect certain landmarks with line segments. We denote with $\mathrm{y}_i(t)$ the resulting landmark image computed for $\mathrm{x}_i(t)$&lt;/p&gt;

&lt;p&gt;랜드마크는 사전정의된 색상 세트를 사용하여 3차원 이미지로 변환되고 특정 랜드마크와 선 세그먼트가 연결된다. $\mathrm{y}_i(t)$ 는 $\mathrm{x}_i(t)$ 의 이미지 계산 결과임을 나타낸다&lt;/p&gt;

&lt;p&gt;In the meta-learning stage of our approach, the following three networks are trained (Figura 2) :&lt;br /&gt;
우리 방법의 메타러닝 단계에서 아래 3가지 네트워크가 학습된다&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The $embedder$ $E(\mathrm{x}_i(s),\mathrm{y}_i(s); \phi)$ takes a video frame $\mathrm{x}_i(s)$, an associated landmark image $\mathrm{y}_i(s)$ and maps these inputs into an $N$-dimentional vector $\hat{\mathrm{e}}_i(s)$. Here, $\phi$ denotes network parameters that are learned in the meta-learning stage. In general, during meta-learning we aim to learn $\phi$ such that the vector $\hat{\mathrm{e}}_i(s)$ contains video-specific information (such as the person’s identity) that is invariant to the pose and mimics in a particular frame s. We denote embedding vectors computed by the embedder as $\hat{\mathrm{e}}_i$&lt;br /&gt;
임베더 는 비디오 프레임 $\mathrm{x}_i(s)$를 가져오고 연관된 랜드마크 이미지 $\mathrm{y}_i(s)$ 와 N차원 벡터에 맵핑시킨다. 여기서 $\phi$ 는 메타러닝 스테이지에서 학습된 네트워크 파라미터를 나타낸다.
일반적으로 메타러닝동안 벡터 $\hat{\mathrm{e}}_i(s)$ 가 비디오정보(개인의 특성)같은 것을 포함하도록 $\phi$를 배우는것을 목표로 한다. 특정 프레임의 포즈와 표정에 영향을 미치지 않도록. 우리는 임베더에 의해 계산된 임베딩 벡터를 $\hat{\mathrm{e}}_i$ 라고 부른다&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The $generator$ $G(\mathrm{y}_i(t), \hat{\mathrm{e}}_i; \psi, P)$ takes the landmark image $\mathrm{y}_i(s)$ for the video frame not seen by the embedder, the predicted video embedding $\hat{\mathrm{e}}_i$, and outputs a synthesized video frame $\hat{\mathrm{x}}_i(t)$. The generator is trained to maximize the similarity between its output and the ground truth frames. All parameters of the generator are split into two sets: the person-generic parameters $\psi$. During meta-learning only $\psi_i$ are trained directly, while $\psi_i$ are predicted from the embedding vector $\hat{\mathrm{e}}_i$ using a trainable projection matrix $\mathrm{P}:\hat\psi_i=\mathrm{P}\hat\mathrm{e}_i$&lt;br /&gt;
제네레이터 G는 임베더가 보지못한 랜드마크 이미지 y 를 가져오고, 예측된 임베딩 e 와  합성된 비디오프레임 X 를 출력한다. G는 출력과 기존 이미지의 유사도가 최대화 되도록 학습된다. G의 모든 파라미터는 두가지로 나뉜다. 인간의 전체적 파라미터 $\psi$. 메타러닝은 오직 프사이만 직접 학습시킨다, 프사이는 임베딩 벡터 E 를 학습가능한 투영 행렬 P를 사용하여 학습된다&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The $discriminator$ $D(\mathrm{x}_i(t), \mathrm{y}_i(t), i; \theta, \mathrm{W, w}_0, b)$ take a video frame $\mathrm{x}_i(t)$, an associated landmark image $\mathrm{y}_i(t)$ and the index of the training sequence $i$. Here, $\theta, \mathrm{W,w0}$ and $b$ denote the learnable parameters associated with the discriminator. Thr discriminator contains a ConvNet part $V(\mathrm{x}_i(t),\mathrm{y}_i(t);\theta)$ that maps the input frame and the landmark image into an $N$-dimensional vector. The discriminator predicts a single scalar (realism score) $r$, that indicates, whether the input frame $\mathrm{x}_i(t)$ is a real frame of the $i$-th video sequence and whether it matches the input pose $\mathrm{y}_i(t)$, based on the output of its ConvNet part and the parameters $\mathrm{W,w_0}, b$.&lt;/li&gt;
  &lt;li&gt;D 는 비디오프레임 x와 연관된 랜드마크 이미지 y 와 시퀸스인덱스 i 를 입력받는다.
여기서 뒤에 4개는 D 와 관련된 학습가능한 파라미터이다. D 는 컨브넷파트 V를 포함하는데 이것은 입력프레임과 랜드마크이미지를 N차원 벡터에 맵핑한다. D 는 스칼라값 r 을 예측하는데 이것은 실제 입력 프레임x 와 인풋포즈 y 가 얼마나 매치하는지 여부를 나타낸다. V 파트와 D의 파라미터들에 기반하여&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;32-meta-learning-stage&quot;&gt;3.2 Meta-learning stage&lt;/h2&gt;

&lt;p&gt;During the meta-learning stage of our approach, the parameters of all three networks are trained in an adversarial fashion. It is done by simulation episodes of K-shot learning (K=8 in our experiments). In each episode, we randomly draw a training video sequence $i$ and a single frame t from that sequence. In addition to t, we randomly draw additional K frames $s_1,s_2,\cdots,s_K$ from the same sequence.&lt;br /&gt;
우리의 메타러닝 과정에서 3가지 네트워크의 모든 파라미터는 적대적 방식으로 학습된다. 이것은 K샷 학습의 시뮬레이션에피소드로 이루어진다 (K는 8로 실험하였다). 각 에피소드에서 무작위로 i번째 비디오와 그 비디오의 t번째 프레임을 그려낸다
추가로 같은 시퀸스에서 추가로 K개의 프레임을 더 그린다.&lt;/p&gt;

&lt;p&gt;We then compute the estimate $\mathrm{e}$ of the i-th video embedding by simply averaging the embeddings $\hat\mathrm{e}_i(s_k)$ predicted for these additional frames :&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">Abstract</summary></entry><entry><title type="html">EfficientNet : Rethinking Model scaling for Convolutional Neural Networks</title><link href="http://localhost:4000/paper/2019/07/04/EffecientNet.html" rel="alternate" type="text/html" title="EfficientNet : Rethinking Model scaling for Convolutional Neural Networks" /><published>2019-07-04T10:05:05+09:00</published><updated>2019-07-04T10:05:05+09:00</updated><id>http://localhost:4000/paper/2019/07/04/EffecientNet</id><content type="html" xml:base="http://localhost:4000/paper/2019/07/04/EffecientNet.html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Convolutinal Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and the scaled up for better accuracy if more resources are available.&lt;br /&gt;
CNN은 일반적으로 고정된 자원 예산 안에서 개발된다 그리고 더 많은자원이 있을경우 더 높은 정확도를 위해 규모를 키운다&lt;/p&gt;

&lt;p&gt;In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance&lt;br /&gt;
이 논문에서는 체계적으로 모델의 스케일을 학습하고 더 좋은 퍼포먼스를 이끌수있는 깊이와 넓이 그리고 해상도의 밸런스를 알아낸다&lt;/p&gt;

&lt;p&gt;Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/ width/ resolution using a simple yet highly effective compound coefficient.&lt;br /&gt;
이 관측에 기반하여 우리는 간단하고도 매우 효과적인 합성계수를 사용하여 깊이, 넓이, 해상도의 차원을 균일하게 조절할 새로운 스케일 매소드를 제안한다&lt;/p&gt;

&lt;p&gt;We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet&lt;br /&gt;
우리는 이 메소드의 상승 효과를 모바일넷과 레즈넷에서 시연하였다&lt;/p&gt;

&lt;p&gt;To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets.&lt;/p&gt;

&lt;p&gt;더 나아가, 우린 새로운 시작 베이스라인을 디자인하고 그것을 스케일업한 에피션넷이라 불리는모델들을 얻었다 이전의 ConvNet보다 정확도와 효율성이 좋은&lt;/p&gt;

&lt;p&gt;In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on Imagenet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet.&lt;br /&gt;
특히 에피션넷은 이미지넷 에서 탑1에서 84.4%, 탑5에서 97.1%의 최고기록인 정확도를 달성했고 기존의 베스트 ConvNet보다 8.4배 작고 6.1배 빠른 속도로 추론했다&lt;/p&gt;

&lt;p&gt;Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100(91.7%), Flowers(98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.&lt;br /&gt;
에피션트넷의 또다른 최고기록은 CIFAR100에서 91.7%, Flowers에서 98.8%, 그리고 3개의 다른 데이터셋에서도 세웠고 매개변수의 수는 더 적었다.&lt;/p&gt;

&lt;p&gt;Source code is at 
https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.&lt;/p&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;Scaling up ConvNets widely used to achieve better accuracy.  &lt;br /&gt;
콘브넷의 스케일업은 더높은 정확도의달성을 위해 널리 사용된다&lt;/p&gt;

&lt;p&gt;For example, ResNet can be scaled up from ResNet-18 to ResNet-200 by using more layers.&lt;br /&gt;
예를들면 레즈넷은 18부터 200까지 레이어를 늘리는것으로 스케일업이 가능하다&lt;/p&gt;

&lt;p&gt;Recently, GPipe achieved 84.3% ImageNet top-1 accuracy by scaling up a baseline model for time larger&lt;br /&gt;
요즘엔 GPipe는 이미지넷 탑1에서 84.3%를 달성했다 
베이스라인의 4배로 스케일업을해서&lt;/p&gt;

&lt;p&gt;However, the process of scaling up ConvNets has never been well understood and there are currently many ways to do it.&lt;br /&gt;
그러나 콘브넷을 확장하는 과정은 쉽게 이해되지 않고 현재 많은방법이 있다&lt;/p&gt;

&lt;p&gt;The most common way is to scale up ConvNets by their depth or width.
가장 흔한 확장방법은 깊이와 너비를 늘리는것이다&lt;/p&gt;

&lt;p&gt;Another less common, but increasingly popular, method is to scale up models by image resolution.&lt;br /&gt;
다른 아직은 흔하진 않지만 점점 유명해지고 있는 방법은 이미지 해상도를 확장하는것이다&lt;/p&gt;

&lt;p&gt;In previous work, it is common to scale only one of the three dimensions - depth, width, and image size.&lt;/p&gt;

&lt;p&gt;Though it is possible to scale two or three dimensions arbitrarily, arbitrary scaling requires tedious manual tuning and still often yields sub-optimal accuracy and efficiency&lt;br /&gt;
그래도 두개혹은세개의 차원은 임의적으로 확장이 가능하다. 임의확장은 지루한 수동튜닝이 필요하고 여전히 정확도와 효율성에있어 최적의 선택이 아니다&lt;/p&gt;

&lt;p&gt;In this paper, we want to study and rethink the process of scaling up ConvNets
이 논문에서 우리는 콘브넷을 확장하는 과정을 연구하고자 한다&lt;/p&gt;

&lt;p&gt;In particular, we investigate the central question : is there a principled method to scale up ConvNets that can achieve better accuracy and efficiency?&lt;br /&gt;
특히 우리는 이 질문에 대해 연구했다
더 높은정확도와 효율성을달성하기 위한 콘브넷에 스케일업에 규칙적인 방법이 있는가&lt;/p&gt;

&lt;p&gt;Our empirical study shows that it is critical to balance all dimensions of network width/depth/resolution, and surprisingly such balance can be achieved by simply scaling each of them with contant ratio.
우리의경험적인 학습은 보여준다
깊이, 너비, 해상도의 밸런스를 유지하는것이 매우 중요하고 놀랍게도 이러한 밸런스는 단순한 각각의 상수 비율로 나타낼 수 있다&lt;/p&gt;

&lt;p&gt;Based on this observation, we propose a simple yet effective compound scaling method.&lt;br /&gt;
이 관측에 기반하여 우리는 간단하고 효과적인 합성 스케일 방법을 제안한다&lt;/p&gt;

&lt;p&gt;Unlike conventional practice that arbitrary scales these factors, our method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients.&lt;br /&gt;
이런 요인을 임의로 조정하던 기존의 방식과는 달리 우리의 방법은 고정된 조정계수의 집합으로 균등하게 조정한다.&lt;/p&gt;

&lt;p&gt;For example, if we want to use $2^N$ times more computational resources, then we can simply increse the network depth by $\alpha^N$, width by $\beta^N$, and image size by $\gamma^N$, where $\alpha,\beta,\gamma$ are constant coefficients determined by a small grid search on the original small model.
예를들면 만약 $2^N$의 많은 컴퓨팅 자원이 있다면 우리는 네트워크를 단순히 $\alpha^N,\beta^N,\gamma^N$ 만큼 증가시킬수 있다 
위 변수들은 원래의 작은 모델에서의 작은 그리드서치에 의해 결정 되는 상수이다.&lt;/p&gt;

&lt;p&gt;intuitively, the compound scaling method makes sense because if the input image is bigger, then the neywork needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.
직관적으로 이 복합조정방법은 말이된다 왜냐면 이미지가 커지면 네트워크는 증가된 필드를 수용하기위해 많은레이어를 필요로 하고 더 큰 이미지에서 좋은패턴을 얻기 위해 더 많은채널을 필요로 한다&lt;/p&gt;

&lt;p&gt;In fact, precvious theoretical and empirical results both show that there exists certain relationship between network width anddepth, but to our best knowledge, we are the first to empirically quantify the relationship among all three dimensions of network width, depth, and resolution.&lt;br /&gt;
사실상 기존 이론과 실증적인 결과는 둘다 네트워크의 너비와 깊이간의 확실한 관계가 존재하는것을 보여준다
그러나 우리의 최고 지식은 처음으로 실제적으로 세가지 차원에 대한관계를 계량한 것이다.&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">Abstract</summary></entry><entry><title type="html">GAN 시작</title><link href="http://localhost:4000/deeplearning/2019/06/28/gan.html" rel="alternate" type="text/html" title="GAN 시작" /><published>2019-06-28T10:05:05+09:00</published><updated>2019-06-28T10:05:05+09:00</updated><id>http://localhost:4000/deeplearning/2019/06/28/gan</id><content type="html" xml:base="http://localhost:4000/deeplearning/2019/06/28/gan.html">&lt;h1 id=&quot;gan&quot;&gt;GAN?&lt;/h1&gt;

&lt;p&gt;Generative Adversarial Network&lt;/p&gt;

&lt;p&gt;적대적 생성 네트워크?&lt;br /&gt;
한마디로 생성기 Generator 와 판별기 Discriminator 가 서로 배틀을 하며 서로의 능력을키워가는….? 
뭐 이런 네트워크다.
들어보면 이게 뭔가 하겠지만 막상 내부구조를보면 아 이런거구나 하는 느낌이 오는?&lt;/p&gt;

&lt;p&gt;일단 봅시다&lt;/p&gt;

&lt;p&gt;Generator 는 1차원벡터인 Noise(균등분포로 랜덤생성)를 입력받아 해당 이미지를 출력하고 Discriminator는 이미지를 입력받아 그것이 진짜인지 가짜인지 여부를 Sigmoid로 출력한다&lt;/p&gt;

&lt;p&gt;간단히하자면 Generator에 노이즈를 입력해서 만든 Fake Image 를 Discriminator에 넣고 이것의 결과가 0이되도록 , 그리고 실제 이미지를 Discriminator 에 넣고 이것의 결과는 1이 되도록 학습시키는것이다.&lt;/p&gt;

&lt;p&gt;이 학습이 반복되면 Generator는 점점 실제에 가까운 이미지를 생성해내게 되고 Discriminator는 두 그림의 차이를 점점더 세밀하게 찾아내게 된다. 하지만 점점 가짜 이미지는 실제 이미지와 유사해지고 결국은 판별을 못하게 되는것이다.&lt;/p&gt;

&lt;p&gt;궁극적 목표는 바로 이 Discriminator 가 가짜 이미지와 진짜 이미지를 판별할 능력이 0.5 (찍는것과 동일)로 수렴하게 만드는것!&lt;/p&gt;

&lt;p&gt;실제 이 방식이 위와같은 수렴을 하게 만든다는것도 증명을 해놨는데 수식이 어렵다. 나중에 시간나면 봐야지…
생각보다 구조는 간단하다
문제는 엄청난수의 하이퍼파라미터들에 정확한 값을 넣지않으면 학습이 잘 되지않는다……&lt;/p&gt;

&lt;h1 id=&quot;dcgan&quot;&gt;DCGAN&lt;/h1&gt;
&lt;p&gt;Deep Convolution GAN &lt;br /&gt;
이름만 봐선 뭔지…&lt;/p&gt;

&lt;p&gt;위에 GAN의경우 불안정함을 많이 보였다고 한다.
실제로 해봐도 진짜 학습하기 어렵…
이것의 안정화를 위해 나온 방식이라 보면되는데
현재 대부분 모델은 대부분 다 이것을 기반으로 한다고 한다.&lt;/p&gt;

&lt;p&gt;자 그럼 살펴보자.&lt;/p&gt;

&lt;p&gt;일단 특징?&lt;br /&gt;
어떤 상황에서도 정말 안정적으로 학습이됨.&lt;br /&gt;
생성된 벡터가 산술연산이 가능
필터 시각화 가능
이미지 분류 성능이 매우 높음&lt;/p&gt;

&lt;p&gt;외워서 만든 이미지가 아님을 보여줘야함
latent space 에서 움직일때 변화가 부드러워야함&lt;/p&gt;

&lt;p&gt;아키텍쳐 가이드라인&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Discriminator에서 모든 풀링 레이어는 스트라이드된 컨볼루션으로 교체&lt;/li&gt;
  &lt;li&gt;Generator에선 Fractinal 스트라이드 컨볼루션으로 교체&lt;/li&gt;
  &lt;li&gt;양쪽에 전부 배치노말 사용, Generator의 output과 discriminator의 인풋은 제외&lt;/li&gt;
  &lt;li&gt;FCL 제거&lt;/li&gt;
  &lt;li&gt;Generator에선 Relu 사용&lt;/li&gt;
  &lt;li&gt;Discriminator에 모든 레이어에 LeakyRelu 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Maxpooling 을 제거하여 모두 미분가능하게 변화&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[http://jaejunyoo.blogspot.com/]&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">GAN?</summary></entry><entry><title type="html">VScode에서 C++ 적용 With Ubuntu</title><link href="http://localhost:4000/environment/2019/06/28/vscode-cpp.html" rel="alternate" type="text/html" title="VScode에서 C++ 적용 With Ubuntu" /><published>2019-06-28T10:05:05+09:00</published><updated>2019-06-28T10:05:05+09:00</updated><id>http://localhost:4000/environment/2019/06/28/vscode-cpp</id><content type="html" xml:base="http://localhost:4000/environment/2019/06/28/vscode-cpp.html">&lt;h1 id=&quot;c-in-vscode&quot;&gt;C++ in Vscode&lt;/h1&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install build-essential
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://webnautes.tistory.com/1158&quot;&gt;https://webnautes.tistory.com/1158&lt;/a&gt;&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">C++ in Vscode</summary></entry><entry><title type="html">새 커널 처음부터 해보기</title><link href="http://localhost:4000/kaggle/2019/06/28/kaggle-new-kernel.html" rel="alternate" type="text/html" title="새 커널 처음부터 해보기" /><published>2019-06-28T10:05:05+09:00</published><updated>2019-06-28T10:05:05+09:00</updated><id>http://localhost:4000/kaggle/2019/06/28/kaggle-new-kernel</id><content type="html" xml:base="http://localhost:4000/kaggle/2019/06/28/kaggle-new-kernel.html">&lt;p&gt;#&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">#</summary></entry><entry><title type="html">제네레이터를 써보자</title><link href="http://localhost:4000/keras/2019/06/28/keras-generator.html" rel="alternate" type="text/html" title="제네레이터를 써보자" /><published>2019-06-28T10:05:05+09:00</published><updated>2019-06-28T10:05:05+09:00</updated><id>http://localhost:4000/keras/2019/06/28/keras-generator</id><content type="html" xml:base="http://localhost:4000/keras/2019/06/28/keras-generator.html">&lt;h1 id=&quot;제네레이터가-뭘까&quot;&gt;제네레이터가 뭘까&lt;/h1&gt;

&lt;p&gt;나중에해야지&lt;br /&gt;
이게더 어려워&lt;/p&gt;

&lt;h2 id=&quot;keraspreprocessingimageimagedatagenerator&quot;&gt;keras.preprocessing.image.ImageDataGenerator&lt;/h2&gt;

&lt;p&gt;케라스에서 제공하는 이미지 제네레이터이다&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;길다……………………….&lt;br /&gt;
하나씩 다 파보는건 레퍼런스가서 보시고&lt;/p&gt;

&lt;p&gt;rotation_range : 회전 제한 각도  (0~180) 
width_shift_range, height_shift_range : 상하좌우 이동가능비율&lt;br /&gt;
horizontal_flip : 좌우반전여부 &lt;br /&gt;
vertical_flip = 상하반전여부 
zoom_range : 확대축소비율 (0~1)&lt;/p&gt;

&lt;p&gt;이정도면 되겠네요 적당한 숫자를 넣어 줍시다&lt;/p&gt;

&lt;h2 id=&quot;주로-같이-사용하는-함수&quot;&gt;주로 같이 사용하는 함수&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plt.imshow()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;이미지 프린트&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PIL.Image.open
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;이미지를 연다. jpg도 가능 &lt;br /&gt;
불러오는 파일에 따라 다른타입으로 저장되는데 그냥 img로 퉁침&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;img.crop((x1,y1,x2,y2))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;대망의 자르기&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;img.resize(size=(x,y), box =(x1, y, x2, y2) )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;대망의 사이즈 바꾸기인데 box 에 위에 crop에 해당하는 좌표를 넣어주면 두작업을 한번에 할 수 있다&lt;br /&gt;
한마디로 크롭 무쓸모? 속도차이가 나려나&lt;/p&gt;

&lt;h2 id=&quot;참고&quot;&gt;참고&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;os.path.join(path, image_name)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;이미지 이름과 패스를 합치는 함수&lt;br /&gt;
그냥 약간의 편의성 증진&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">제네레이터가 뭘까</summary></entry></feed>