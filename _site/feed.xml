<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-08-02T09:10:32+09:00</updated><id>http://localhost:4000/</id><title type="html">AweSome Blog</title><subtitle>Aru's Daily Page</subtitle><author><name>Aru</name></author><entry><title type="html"></title><link href="http://localhost:4000/2019/08/02/2019-07-05-GNN-google.html" rel="alternate" type="text/html" title="" /><published>2019-08-02T09:10:32+09:00</published><updated>2019-08-02T09:10:32+09:00</updated><id>http://localhost:4000/2019/08/02/2019-07-05-GNN-google</id><content type="html" xml:base="http://localhost:4000/2019/08/02/2019-07-05-GNN-google.html">&lt;h1 id=&quot;relational-inductive-biases-deep-learning-and-graph&quot;&gt;Relational inductive biases, deep learning, and graph&lt;/h1&gt;
&lt;p&gt;arXiv:1806.01261v3 , 17 oct 2018&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;인공지능은 최근 비전, 언어, 조작, 그리고 의사결정 과정에서 흥행하고있다&lt;/p&gt;

&lt;p&gt;데이터와 컴퓨팅자원이 저렴해졌고 딥러닝의 자연스러운 강점 을 가지고있다&lt;/p&gt;

&lt;p&gt;그러나 사람의 지능의 특성은 현재 접근법으로는 접근하지 못하는 경우가 많다&lt;/p&gt;

&lt;p&gt;특히 누군가의 경험을 일반화 하는것
-유아기의 인간지능의 특징-
은 현대 인공지능에서 어려운 도전과제로 남아있다&lt;/p&gt;

&lt;p&gt;다음은 위치, 리뷰, 통합 순이다&lt;/p&gt;

&lt;p&gt;우리는 AI가 인간과 비슷한 능력이 되러면 결합과 일반화가 필요하고 구조화된 표현과 계산이 이 목표를 실현할 열쇠이다&lt;/p&gt;

&lt;p&gt;생물학과 마찬가지로 자연과 양육의 협력을 사용하고, 핸드엔지니어링과 엔드투엔드의 사이에서 잘못된 선택을 하는것을 거부한다
그리고 대신 그것들의 보완적인 강점으로부터의 이득에 접근하는것을 옹호한다&lt;/p&gt;

&lt;p&gt;우리는 딥러닝 아키텍쳐의 엔티티와 관계와 그것의 구성에 대한 학습을 용이하게 할수있는 관계 유도성 편향을 사용하는 방법을 연구한다&lt;/p&gt;

&lt;p&gt;우리는 정했다 새로운 구성 블록을 AI 툴킷을 위한 강한 관계형 편향을 가진
-그래프 네트워크-
일반화하고 다양한 접근법으로 확장하는 
다양한 접근법 그래프를 조작하는&lt;/p&gt;

&lt;p&gt;우리는 어떻게 그래프네트워크가 관계의 설명과 조합의 일반화를 하는지 토론했다&lt;/p&gt;

&lt;p&gt;복잡하고 해석가능하고 추론가능한 추리의 패턴의 토대를 마련했다&lt;/p&gt;

&lt;p&gt;이 논문과 함께 우리는 그래프 네트워크를 구축하기 위한 오픈소스 라이브러리를 발표했고 사용하는 방법을 시연했다&lt;/p&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;인간의 지능의 주요 키는 ‘유한한 방법의 무한한 사용이다 ‘&lt;/p&gt;</content><author><name>Aru</name></author></entry><entry><title type="html">Research and Improvement of Single Image Super Resolution Based on Generative Adversarial network</title><link href="http://localhost:4000/paper/2019/08/01/research_srgan.html" rel="alternate" type="text/html" title="Research and Improvement of Single Image Super Resolution Based on Generative Adversarial network" /><published>2019-08-01T10:05:05+09:00</published><updated>2019-08-01T10:05:05+09:00</updated><id>http://localhost:4000/paper/2019/08/01/research_srgan</id><content type="html" xml:base="http://localhost:4000/paper/2019/08/01/research_srgan.html">&lt;p&gt;SRGAN 의 점수 감소로 인해… ㅠㅠ 여러가지 테스트하긴 시간이 부족하고
점수를 올려 줄 수있는 방법을 찾던중 새로운 논문 발견!
일단 내용은 SRGAN의 향상에 대한 연구로 봐야겠다.
결과를보면 PSNR 점수가 비약적으로 상승한걸로 봐서 이거다 싶어서 보기시작.
시간상 일단 중요한 부분만 보자 ㅠㅠ&lt;/p&gt;

&lt;h2 id=&quot;21-improved-model-structure&quot;&gt;2.1 Improved model structure&lt;/h2&gt;

&lt;p&gt;네트워크의 깊이와 너비를 증가시키는것은 네트워크의 성능을 향상시킬수 있다. 많은 실험이 같은 상황에서 깊고 얕은 네트워크를 가지고 실험을 했는데 깊은네트워크의 퍼포먼스는 는 일반적으로 좋지 않았다. 얕은 네트워크의 파라미터는 는 뒤에 1층만 남긴채 깊은 네트워크의 앞의 몇몇층이 옮겨올 수 있다. 이런 맵핑은 얕은 네트워크의 학습 효과를 볼수 있다. 예를들면 흔한 VGG 구조는 AlexNet 네트워크의 깊이를 증가시키는 것으로 성능을 비약적으로 향상시켰다. 그러나 단순하게 네트워크의 깊이를 증가시키는 것은 기울기 소실 혹은 발산을 야기한다. 이 문제에서 네트워크의 레이어중 몇개는 정규화와 배치노말을 통해 학습될 수 있다.
그러나, 새로운 성능저하 문제가 발생하는데, 레이어가 증가하면 training 셋의 정확도는 포화하거나 심지어 감소한다. 이것은 오버피팅으로는 이해 할 수 없는데 왜냐면 오버피팅은 학습셋에서는 더 좋은 성능을 가져오기 때문이다.&lt;/p&gt;

&lt;p&gt;He에 의해 잔차 네트워크와 뛰어넘는 연결이 제안되면서, 네트워크구조가 더 깊을때 쉽게 학습되도록 만들어지고, 인식 정확도는 레이어가 증가함에 따라 향상 될 수 있었다. residual block 기존 구조는 Figure2에 나와있다. 잔차네트워크는 기본 컨볼루션 레이어에 숏컷을 추가하고 skip connection 을 연결하여 기본 residual block을 만든다. 기존의 H(x) 는 H(x) = F(x) + x 로 표현한다. 잔차구조는 
H(x) 학습을 F(x)학습으로 전환하고 F(x) 를 학습하는것은 H(x)를 학습하는것보다 쉽다.  잔차구조는 깊은 네트워크에서의 degradation 문제를 효과적으로 완화시키고 residusl block 을 레이어처럼 축척하는것을 통해 네트워크의 성능을 향상시킨다.&lt;/p&gt;

&lt;p&gt;원래의 SRGAN 생성 모델은 몇개의 residual block 을 포함하고 있다. 다수의 배치노말 레이어가 residual 구조에 사용되고있다. 배치노말 레이어는 학습시 배치단위의 평균과 분산을 사용하여 피쳐를 normalize 하고 테스트시엔 전체 학습 데이터의 추정된 평균과 분산을 사용한다. 학습과 테스트의 통계적 지표가 매우 다를경우 배치노말은 모델의 일반화성능을 제한하는 불편한 물건이 된다. 실험들은 배치노말을 제거하는것이 복잡도와 메모리사용, 그리고 모델의 일반화 성능을 향상 시키는것을 보여준다.&lt;/p&gt;

&lt;p&gt;네트워크 깊이 측면에서, literature[4-6] 은 깊은 네트워크가 높은 복잡도의 맵핑을 구축하고 엄청나게 네트워크 정확도를 향상시킬 수 있는것을 보여준다. 이 논문에서 원래 모델의 생성기의 residual block을 32개로 증가시켰다. 실험에서는 fitting 현상은 발생하지 않앗고 모델은 어느정도 향상되었다. 개선된 구조는 Figure 3에서 보여준다.&lt;/p&gt;

&lt;h2 id=&quot;22-improved-loss-function&quot;&gt;2.2 Improved loss function&lt;/h2&gt;

&lt;p&gt;원래 SRGAN의 손실함수는 두부분으로 나뉜다. VGG기반의 constent loss와 적대적 모델 기반의 countermeasure loss. $D_{\theta D}$ 는 실제 고해상도 이미지에 해당하고, $G_{\theta G}(I^{LR})$ 재구성된 고해상도 이미지이다. 이미지의 텍스쳐 디테일은 향상되었지만 PSNR 과 SSIM 은 좋지 않았다.&lt;/p&gt;

&lt;p&gt;This improves the texture details of the image, but the PSNR and SSIM 지표는 좋지 못했다. 이 논문에서 MSE 기반의 손실함수를 추가하면서 손실함수를 향상시켰다. 그 이유는 VGG의 재구성 효과는 다른 레이어마다 다르고, 여기에 또 가중치 상수를 더했다 VGG 함수에. 
 향상된 손실함수는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l^{SR} = l^{SR}_{VGG} + 10^{-3}l^{SR}_{Gen} \\

 l^{SR}_{VGG(i,j)} = \frac{1}{W_{i,j}H_{i,j}} \sum\sum(\Phi_{i,j}(I^{HR})_{x,y}-\Phi_{i,j}(G_{\theta G}(I^{LR}))_{x,y})^2 \\

 l^{SR}_{Gen} = \sum -\log D_{\theta D}(G_{\theta G}(I^{LR})) \\

 l^{SR}_{MSE} = \frac{1}{r^2WH}\sum\sum((I^{HR})_{x,y}-(G_{\theta G}(I^{LR}))_{x,y}) \\

 l^{SR} = i^{SR}_{VGG}+\alpha i^{SR}_{MSE} + \beta i^{SR}_{Gen}&lt;/script&gt;

&lt;h1 id=&quot;3-experiment&quot;&gt;3. Experiment&lt;/h1&gt;</content><author><name>Aru</name></author><summary type="html">SRGAN 의 점수 감소로 인해… ㅠㅠ 여러가지 테스트하긴 시간이 부족하고 점수를 올려 줄 수있는 방법을 찾던중 새로운 논문 발견! 일단 내용은 SRGAN의 향상에 대한 연구로 봐야겠다. 결과를보면 PSNR 점수가 비약적으로 상승한걸로 봐서 이거다 싶어서 보기시작. 시간상 일단 중요한 부분만 보자 ㅠㅠ</summary></entry><entry><title type="html">Jekyll 다시 시작</title><link href="http://localhost:4000/jekyll/2019/07/31/jekyll-restart.html" rel="alternate" type="text/html" title="Jekyll 다시 시작" /><published>2019-07-31T10:05:05+09:00</published><updated>2019-07-31T10:05:05+09:00</updated><id>http://localhost:4000/jekyll/2019/07/31/jekyll-restart</id><content type="html" xml:base="http://localhost:4000/jekyll/2019/07/31/jekyll-restart.html">&lt;h1 id=&quot;다시시작&quot;&gt;다시시작&lt;/h1&gt;

&lt;p&gt;전에 시도하다 포기하고 일단 포스트만 올리면서 나중에 좋은스킨 나오면 해야지 하다가
이제 슬슬 포트폴리오겸 만들어야겠다 싶어 다시 도전….&lt;/p&gt;

&lt;p&gt;일단 설정이 문제가 있었을수도있으니&lt;/p&gt;

&lt;p&gt;깔끔하게 우분투를 날려버리고… 다시깔았다…&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get update -y &amp;amp;&amp;amp; sudo apt-get upgrade -y
$ sudo apt-get install -y build-essential ruby-full
$ sudo gem install jekyll bundler
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;아니 이게왠일&lt;/p&gt;

&lt;p&gt;여기까지 아무 에러가 없이 진행되었..&lt;/p&gt;

&lt;p&gt;원하던 스킨을 다운받고&lt;/p&gt;

&lt;p&gt;드디어 대망의&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ jekyll serve
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;는 될리가 없지………ㅜㅜ&lt;/p&gt;

&lt;p&gt;jekyll new 로 새로 생성하면 잘만되는데&lt;/p&gt;

&lt;p&gt;꼭 스킨을 다운받으면 안되는…&lt;/p&gt;

&lt;p&gt;역시 또 하루종일 이거설치 저거설치&lt;/p&gt;

&lt;p&gt;이거삭제 저거삭제&lt;/p&gt;

&lt;p&gt;열심히 해본 결과 알아낸 해결책은 해당폴더에서&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ bundle install
$ bundle exec jekyll serve 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이거였다…&lt;/p&gt;

&lt;p&gt;문제는 또 parsing 오류가 떳는데&lt;/p&gt;

&lt;p&gt;이건 title ‘Aru’s Blog’  여기에서 에러가..&lt;/p&gt;

&lt;p&gt;이래서 특문을 쓸땐 항상조심 ㅠㅠ&lt;/p&gt;

&lt;p&gt;결국&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post/190731-1.png&quot; alt=&quot;성공&quot; /&gt;&lt;/p&gt;

&lt;p&gt;드디어 성공했다 ㅠㅠㅠ&lt;/p&gt;

&lt;p&gt;이제 꾸미기에 들어가 봅시다&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">다시시작 전에 시도하다 포기하고 일단 포스트만 올리면서 나중에 좋은스킨 나오면 해야지 하다가 이제 슬슬 포트폴리오겸 만들어야겠다 싶어 다시 도전…. 일단 설정이 문제가 있었을수도있으니 깔끔하게 우분투를 날려버리고… 다시깔았다… $ sudo apt-get update -y &amp;amp;&amp;amp; sudo apt-get upgrade -y $ sudo apt-get install -y build-essential ruby-full $ sudo gem install jekyll bundler 아니 이게왠일 여기까지 아무 에러가 없이 진행되었.. 원하던 스킨을 다운받고 드디어 대망의 $ jekyll serve 는 될리가 없지………ㅜㅜ jekyll new 로 새로 생성하면 잘만되는데 꼭 스킨을 다운받으면 안되는… 역시 또 하루종일 이거설치 저거설치 이거삭제 저거삭제 열심히 해본 결과 알아낸 해결책은 해당폴더에서 $ bundle install $ bundle exec jekyll serve 이거였다… 문제는 또 parsing 오류가 떳는데 이건 title ‘Aru’s Blog’ 여기에서 에러가.. 이래서 특문을 쓸땐 항상조심 ㅠㅠ 결국 드디어 성공했다 ㅠㅠㅠ 이제 꾸미기에 들어가 봅시다</summary></entry><entry><title type="html">Photo-Realistic Single image Super-Resolution Using Generative Adversarial Network</title><link href="http://localhost:4000/paper/review/2019/07/30/srgan.html" rel="alternate" type="text/html" title="Photo-Realistic Single image Super-Resolution Using Generative Adversarial Network" /><published>2019-07-30T10:05:05+09:00</published><updated>2019-07-30T10:05:05+09:00</updated><id>http://localhost:4000/paper/review/2019/07/30/srgan</id><content type="html" xml:base="http://localhost:4000/paper/review/2019/07/30/srgan.html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;깊고 빠른 컨볼루션 뉴럴넷을 이용하여 싱글이미지의 super-resolution의 정확도와 속도가 발전했음에도 불구하고 하나의 큰문제는여전히 남아있다 : 
큰 배수의 고해상도작업에서 어떻게 좋은 디테일을 복구할것인가? 
최적화 기반의 super-resolution 방법들은 주로 목적함수를 고르는것에 좌우된다.
현재의 작업은 MSRE 를 최소화 하는데 큰 초점을 맞추고 있다.
결과인 추정치들은 높은 PSNR 을 가지고 있으나, 그들은 높은 확률로 
더 높은 해상도에서는 사람의 지각적으로 기대할만한 결과를 만드는데는 실패했다
이 논문에서는 SRGAN 이라는 super-resolution 을 위한 GAN 모델을 보여준다.
우리가 아는 바로는 이것은 사진을 리얼하게 4배로 확대할수 있는 최초의 프레임웤이다
이것을 하기위해, 지각적인 손실 함수를 제안한다. adversarial loss과 content loss 로 이루어진.
adversarial loss 는 확대된 이미지와 오리지날 이미지를 구분하도록 학습된 판별기를 사용하여 우리의 해결책을 제시한다. 추가로 픽셀공간의 유사도 대신 지각적 유사도에 반응하는 content loss를 사용했다. 우리의 deep residual network 는 공공 이미지의 매우 축소된 이미지에서 사실적인 텍스쳐를 복구 할수 있다. 연장된 MOS 검사는 SRGAN을 사용한 지각적 퀄리티의 매우 중요한 이점을 보여준다. SRGAN 과 MOS 점수는 다른 어떤 최신방법보다 더 실제 이미지에 가깝게 해준다.&lt;/p&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;저해상도 이미지로부터 고해상도 이미지를 예측하는 도전적인 작업은 super-resolution 이라 한다.
SR 은 CV 연구에서 많은 관심을 받았으며 수 많은 어플을 보유하고 있다.&lt;/p&gt;

&lt;p&gt;#####SR문제의 나쁜 본질은 특히 높은 배율에서 확연히 나타나는데, 
#####재구성된 SR 이미지가 일반적으로 없다?&lt;/p&gt;

&lt;p&gt;SR 지도 알고리즘의 최적화 대상은 실제이미지와 생성된 이미지의 MSE 가 흔하다.
MSE를 최소화 하는것은 또한 PSNR을 최대화 하는것이기도 한데, SR알고리즘에 평가 및 비교에 사용되는 일반적인 척도이다. 
그러나 MSE와 PSNR의 고급 텍스쳐 디테일에서 지각적인 차이를 잡아내는 능력은 매우 제한적이다&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">Abstract</summary></entry><entry><title type="html">(진행중)Progressive Growing of GANs for Improved Quality Stability and Variation</title><link href="http://localhost:4000/paper/2019/07/20/PGGAN.html" rel="alternate" type="text/html" title="(진행중)Progressive Growing of GANs for Improved Quality Stability and Variation" /><published>2019-07-20T10:05:05+09:00</published><updated>2019-07-20T10:05:05+09:00</updated><id>http://localhost:4000/paper/2019/07/20/PGGAN</id><content type="html" xml:base="http://localhost:4000/paper/2019/07/20/PGGAN.html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;We describe a new training methodology for generative adversarial netwworks. The key idea isto grow both the generator and discriminator progressively : startting from a low resolution, we add new layers that model increasingly fine details as training progressses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at $1024^2$. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR 10. Additionally, we describe several implementation details that are importtant for discouragion unhealthy competition betwnne the generator and discrimination, Finally, we suggest a new metric for evaluation GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher–quality versiong of the CELEBA dataset.&lt;/p&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;Generative mothods that produce novel samples from high-dimensional data distributions, such as images, are finding widespread use, for example in speech synthesis (van den Oord te al., 2016a), image-to-image translation (Zhu et al., 2019; Liu et al., 2019; Wang et al., 2017). and image inpainting (Lizuka et al., 2017). Currently tyhe most prominent approaches are auto regressive models (van den Oord et al., 2016b;c), variarional autoencoders(VAE)(Kingma &amp;amp; Welling, 2014), and generative adversarial networks(GAN) (Goodfellow et al., 2014). Currently they all have significant strengehs and weaknesses. Auto regressive models - such as PixelCNN - produce sharp images but are slow to evaluate and do not have a latent representation as they direvtly model the conditional distribution over pixels, potentially limitiing their applicability. VAEs are easy to train but tend to produce blurry results due to restrictions in the model, although recent work is improveing this (Kingma et al., 2016). GANs produce sharp images, albeit only in fairly small resolutions and with somewhat limited variation, and the training continues to be unstable despite recent progress(Salimans et al., 2016; Gulrajani et al., 2017; Berthelot et al., 2016’ Kodali et al., 2017). Hybrid methods combine various strengths of the three, but so far lag behind GANs in image quality (Makhzani &amp;amp; Frey, 2017; Ulyanov et al., 2017; Dumoulin et al., 2016)&lt;/p&gt;

&lt;p&gt;Typically, a a GAN consists of two networks: generator and discriminator (aka critic). The generator produces a sample, e.g., an image, from a latent code, and the distribution of these images should ideally be indistinguishable from the training distribution. Since it is generally infeasible to engineer a function that tells whether that is the case, a discriminator network is trained to do the assessment, and since networks are differentiable, we also get a gradient we can use to steer both networks to the right direction. Typically, the generator is of main interest - the discriminator is an adaptive loss function that gets discarded once the generator has been trained.&lt;/p&gt;

&lt;p&gt;There are multiple porential problems with this formulation. When we measure the distance between the training disributions do not have substantial overlap, i.e., are too easy to tell apart (Arjovsky * Bottou, 2016). Originally, jensen-Shannon divergence was used as a distance metric (Goodfellow et al., 2014), and recently that formulation has been improved (Hjelm et al., 2016 and a number of more stable alternatives have been proposed, including least squares (Mao et al., 2016b), absolute deviation with margin (Zhao et al., 2016), and Wasserstein distance (Arjovsky et al., 2017; Gulrajani et al., 2017) Our contributions are largely orthogonal to this ongoing discussion, and we primarily use the improbed Wasserstein loss, but alos ecperiment with least-squares loss.&lt;/p&gt;

&lt;p&gt;The generation of high-resolution images is difficult because higher resolution makes it easier to tell the generated images apert from training images (Odena et al., 2017), thus drastically amplifying the gradient problem. Large resolutions also necessitate using smaller minibatches due to memory constraintsm further compromising training stability. Our key insight is that we can grow both the generator and discriminator progressively, stating from easier low-resolution images, and add new layers that introduce higher-resolution details as the training progresses. This greatly speeds up training and improves stability in high resolutions, as we will discuss in Section 2.&lt;/p&gt;

&lt;p&gt;The GAN formulation does not explicitly require the entire training data distribution to be represented by the resulting generative model. The conventional wisdom has been that there is a tradeoff between image quality and variation is currently receiving attention and various methods have been suggested for measuring it,mincludig inception score (Salimans et al., 2016), multi-scale structural similarity(MS-SSIM)(Odena et al., 2017; Wang et al., 2003), birthday paradox (Arora &amp;amp; Zhang, 2017), and explicit tests for the number of discrete modes discovered (Metz et al., 2016). We will describe our method for encouraging variation in Section 3, and propose a new metric for evaluation the quality and variation in Section 5.&lt;/p&gt;

&lt;p&gt;Section 4.1 discusses a subtle modification to the initialization of networks, leading to a more balanced learning speed for different layters. Furthermore, we observe that mode collapses traditionally plaguing GANs tend to happen very quickly, over the course of a dozen minibatches. Commonly they start when the discriminator overshoots, leading to exaggerated gradients, and an unhealthy competition follows where the signal magnitudes escalate in both networks. We propose a mechanism to stop the generator from participation in such escalation, overcoming the issue (Section 4.2)&lt;/p&gt;

&lt;p&gt;We evaluate our contributions using the CELEBA, LSUN, CIFAR10 datasets. We improve the best published iinception score for CIFAR10. Since the datasets commonly used in benchmarking generative methods are limited to a fairly low resolution, we have also created a higher quality version of the CELEBA dataset that allowds experimentation with output resolutions up to $1024 \times 1024$ pixels. This dataset and our full implementation are aavailable at [https://github.com/tkarras/progressive_growing_of_gans], trained networks can be found at [https://drive.google.com/open?id=0B4qLcYyJmiz0NHFULTdYc05lX0U] along with result images, and a supplementary video illustrating the datasets, additional results, and latent space interpolations is at [https://youtu.be/G06dEcZ-QTg].&lt;/p&gt;

&lt;h1 id=&quot;2-progressive-growing-of-gans&quot;&gt;2. Progressive Growing of GANs&lt;/h1&gt;

&lt;p&gt;Our primary contribution is a training methodology for GANs where we start with low-resolution imagesm and then progressively increase the resolution by adding layers to the networks as visualized image distribution and then shift attention to increasingly finer scale detail, instead of having to learn all scales simultaneously.&lt;/p&gt;

&lt;p&gt;We use generator and discriminator networks that are mirror images of each other and always grow in synchrony. All existing layers in both networks remain trainable throughout the training process. When new layers are added to the networks, we fade them in smoothly, as illustrated in Figure 2. This avoids sudden shocks th the already well-trained, smaller-resolution layers. Appendix A describes structure of the generator and discriminator in detail, along with other training parameters.&lt;/p&gt;

&lt;p&gt;We observe that the progressive training has several benefits. Early on, the generation of smaller images is substantially more stable because there is less class information and fewer modes (Odena et al.,2017). By increasing the resolution little by little we are continuously asking a much simpler question compared to the end goal of discovering a mapping from latent vectors to e.g. $1024^2$ images. This approach has conceptual similarity to recent work by Chen &amp;amp; Koltun (2017). in prectice it stabilizes the training sufficiently for us to reliably synthesize megapixel-scale images using WGAN-GP loss (Gulrajani et al.,2017) and even LSGAN loss (Mao et al.,2016b).&lt;/p&gt;

&lt;p&gt;Another benefit is the reduced training time. With progressively growing GANs most of the iterations are done at lower resolutions, and comparable result quality is often obtained up to 2-6 times faster, depending on the final output resolution.&lt;/p&gt;

&lt;p&gt;The ideaof growing GANs progressively is related to the work of Wang et al.(2017), who use multiple discriminators that operate on different spatial resolutions. That work in turn is motivated by Durugkar et al.(2016) who use one generator and multiple discriminators concurrently, and Ghosh et al.(2017) who do the opposite with multiple generators and one discriminator. Hierarchical GANs(Denton et al.,2015; Huand et al.,2016; Zhang et al.,2017) define a generator and discriminator for each level of an image pyramid. These mothods build on the same observation as our work - that the complex mapping from latents to high-resolution images is easier to learn in steps - but the crucial difference is that we have only a single GAN instead of a hierarchy of them. In contrast to early work on adaptively growing networks, e.g., growing neural gas(Fritzke, 1995) and neuro evolution of augmenting topologies (Stanley &amp;amp; Miikkulainen, 2002) that grow networks greedily, we simply defer the introduction of pre-configured layers. in that sense our approach resembles layer-wise training of autoencoders ( Bengio et al.,2007).&lt;/p&gt;

&lt;h1 id=&quot;3-increasing-variation-using-minibatch-standard-deviation&quot;&gt;3. Increasing Variation using minibatch standard deviation&lt;/h1&gt;

&lt;p&gt;GANs have a tendency to capture only a subset of the variation found in training data, and Salimans et al.(2016) suggest “minibatch discrimination” as a solution.&lt;/p&gt;

&lt;p&gt;GAN은 트레이닝 데이터에서 찾은 variation의 일부만 캡쳐하는 경향이 있고, 살리만 이것의 해결법으로 minibatch discrimination 을 제안했다.&lt;/p&gt;

&lt;p&gt;They compute feature statistics not only from individual images but also across the minibatch, thus encouraging the minibatches of generated and training images to show similar statistics.
그들은 각 이미지가 아닌 미니배치에 거쳐 통계량을 계산하고, 따라서 생성되고 학습된 미니배치가 비슷한 통계량을 가지도록 도와준다.&lt;/p&gt;

&lt;p&gt;This is implemented by adding a minibatch layer towards the end of the discriminator, where the layer learns a large tensor that projects the input activation to an array of statistics.&lt;/p&gt;

&lt;p&gt;이것은 discriminator 의 끝쪽에 미니배치레이어를 더하는것으로 구현된다. 
그 레이어는 입력의 통계값의 활성화를 투영한 큰 텐서를 학습한다.&lt;/p&gt;

&lt;p&gt;A separate set of statistics is produced for each example in a minibatch and it is concatenated to the layer’s output, so that the discriminator can use the statistics internally. We simplify this approach drastically while also improving the variation.&lt;/p&gt;

&lt;p&gt;통계량의 일부분은 미니배치의 각 샘플에서 생산되며 레이어의 출력과 concat 된다. 
discrimiantor가 그 통계값을 내부적으로 사용 가능하도록. 이 접근법을 극적으로 단순화시키면서  variation도 향상시켰다.&lt;/p&gt;

&lt;p&gt;Our simplified solution has neither learnable parameters nor new hyperparameters.  &lt;br /&gt;
우리의 간단한 해결책은 학습가능한 파라미터도 아니고 새로운 하이퍼파라미터도 아니다.&lt;/p&gt;

&lt;p&gt;We first compute the standard deviation for each feature in each spatial location over the minibatch. We then average these estimaties over all features and spatial locations to arrive at a single value.  &lt;br /&gt;
첫번째로 각 미니배치를 넘어 각 공간위치안에 피쳐들에 대한 표준편차를 계산한다. 그리고 각 피쳐 및 각 공간위치에 대한 추정값들을 평균내어 단일 값으로 바꾼다&lt;/p&gt;

&lt;p&gt;We replicate the value and concatenate it to all spatial locations and over the minibatch, yielding one additional(constant) feature map. This layer could be inserted anywhere in the discriminator, but we have found it best to insert it towards the end(see Appendix A.1 for details)
우리는 이값을 복제하고 모든 공간위치에 concate 하여 추가적인 하나의 피쳐맵을 만든다. 이 레이어는 discriminator의 어디에든 들어갈 수 있으나 우리가 찾아낸 베스트는 끝쪽에 넣는것이었다&lt;/p&gt;

&lt;p&gt;We experimented with a richer set of statistics, but were not able to improve the variation further. In parellel work, Lin et al.(2017) provide theoretical insights about the benefits of showing multiple images to the discriminator. &lt;br /&gt;
우리는 더 많은 자료로 실험을했지만, variation 을 더 향상시키진 못했다. Lin의 작업에선 여러 이미지를 discriminator 에게 보여줌으로 얻는 이점에 대한 정보를 제공한다.&lt;/p&gt;

&lt;p&gt;Alternative solutions to the variation problem include unrolling the discriminator (Metz et al., 2016) to regularize its updates, and a “repelling regularizer”
(Zhao et al.,2017) that adds a new loss term to the generator, trying to encourage it th orthogonalize the feature vectors in a minibatch. &lt;br /&gt;
다양성 문제에 대책으로는 discriminator 에 unrolling 을 포함하여 업데이트시 정규화하는 방법이 있다. ‘repelling regularizer’ 라는 제네레이터에 새로운 로스 부분을 추가해서 미니배치 안에 피쳐들을 직교화 되도록 하는 방법도 있다.&lt;/p&gt;

&lt;p&gt;The multiple generators of Ghosh et al. (2017) also serve a similar goal. We acknowledge that these solutions may increase the variation even more than our solution - or possibly be orthogonal to it - but leave a detailed comparison to a later time.  &lt;br /&gt;
Ghosh의 다중 generator 도 비슷한 목표를 제공한다. 이 솔루션들은 우리의 해법보다 다양성을 더 높여주거나 가능한 젝교화되도록 하는것을 인정하지만 뒤에있는 디테일 비교가 남아있다.&lt;/p&gt;

&lt;h1 id=&quot;4-normalization-in-generator-and-discriminator&quot;&gt;4. normalization in Generator and discriminator&lt;/h1&gt;

&lt;p&gt;GANs are prone the the escalation of signal magnitudes as a result of unhealthy competition between the two networks. Most if not all earlier solutions discourage this by using a variant of batch normalization (Ioffe &amp;amp; Szegedy,2015; Salimans &amp;amp; Kingma, 2016; Ba et al.,2016) in the generator, and often also in the discriminator. &lt;br /&gt;
GAN들은 두가지 네트워크의 경쟁의 비균형적 결과 에 의해 신호크기가 증가하기가 쉽다. 초기가 아닌 대부분 해결책은 제네레이터에서 배치노말에 변형을 주어 이를 감소시켯고, 종종 그것은 discriminator 에도 들어갔다&lt;/p&gt;

&lt;p&gt;These normalization methods were originally introduced to eliminate covariate shift. However we have not observed that to be an issue in GANs, and thus believe that the actual need in GANs is constraining signal magnitudes and competition. We use a different approach that consists of two ingredients, neither of which include learnable parameters.&lt;br /&gt;
이 정규화 기법은 원래 공변량의 이동을 위해 사용되었다. 그러나 우리는 GAN에서 이 이슈가 관찰되지 않았었고, GAN에서 실제로 필요한것은 신호크기와 경쟁의 제한이라고 믿었다. 우린 어떤 학습가능한 파라미터도 포함하지않는 두가지 로 구성된 다른 접근법을 사용하였다 .&lt;/p&gt;

&lt;h2 id=&quot;41-equlized-learning-rate&quot;&gt;4.1 Equlized learning rate&lt;/h2&gt;

&lt;p&gt;We deviate from the current trend of careful weight initialization, and instead use a trivial $\mathcal{N}(0,1)$ initialization and then explicitly scale the weights at runtime. To be precise, we set $\hat{w_i} = w_i/c$, where $w_i$ are the weights and $c$ is the per-layer normalization constant from He’s initializer(He et al.,2015). The benefit of doing this dynamically instead of during initialization is somewhat subtle, and relates to the scale-invariance in commonly used adaptive stochastic gradient descent methods such as RMSProp (Tieleman &amp;amp; Hinton, 2012) and Adam(Kingma &amp;amp; Ba, 2015). These methods normalize a gradient update by its estimated standard deviation, thus making the update independent of the scale of the parameter. &lt;br /&gt;
우리 가중치 초기화를 조심스러워하는 현재 트렌드에서 벗어났다. 그리고 대신 단순한 표준정규분포 초기화를 사용하고 런타임에 가중치를 명시적으로 스케일했다. 정확히하면 가중치를 허 초기화에서 나온 레이어별 정규화 상수로 나누어줬다. 초기화중 대신 동적으로 하는 이점은 다소 미묘하고 RMSProp이나 Adam 처럼 일반적으로 사용되는 SGD 방법과도 관련이 있다. 이 방법들은 그들의 추정된 표준편차를 기준으로 경사 업데이트를 정규화하고 따라서 매개변수의 스케일과 독립적으로 업데이트된다.&lt;/p&gt;

&lt;p&gt;As a result, if some parameters have a larger dynamic range than others, they will take longer to adjust. This is a scenario modern initializers cause, and thus it is possible that a learning rate is both too large and too small at the same time. Our approach ensures that the dynamic range and thus the learning speed, is the same for all weights. A similar reasoning was independently used by val Laarhoven(2017).  &lt;br /&gt;
결과적으로 만약 몇파라미터가 다른것보다 넓은 유동범위를 가지고 있다면 조정되는데 더 오래걸릴 것이다. 이것은 현대의 초기화 방법이 유발하는 현상이고, 따라서 학습률이 동시에 너무 크거나 혹은 너무 작을 수 있다. 우리의 접근은 유동범위와 학습속도가 모든 가중치에서 같은것을 보증한다. 비슷한 연구로는 val Laarhoven의 독립적 사용이 있다.&lt;/p&gt;

&lt;h2 id=&quot;42-pixelwise-feature-vector-normalization-in-generator&quot;&gt;4.2 Pixelwise feature vector normalization in generator&lt;/h2&gt;

&lt;p&gt;To disallow the scenario where the magnitudes in the generator and discriminator spiral out of control as a result of competition, we normalize the feature vector in each pixel to unit length in the generator after each convolutional layer.&lt;br /&gt;
경쟁의 결과로 생성기와 판별기가 통제 불능의 크기가 되는 현상을 막기 위해. 우리는 생성기의 각 컨볼레이어 뒤에 각 픽셀에서 단위 길이까지의 피쳐벡터를 정규화한다.&lt;/p&gt;

&lt;p&gt;We do this using a variant of “local response normalization”(Krizhevsky et al.,2012), configured as $b_{x,y} = a_{x,y} / \sqrt{\frac{1}{N}\sum^{N-1}&lt;em&gt;{j=0}(a^j&lt;/em&gt;{x,y})^2+\epsilon}$, where $\epsilon = 10^{-8}$, $N$ is the number of feature maps, and $a_{x,y}$ and $b_{x,y}$ are the original and normalized feature vector in pixel $(x,y)$, respectively. We find it surprising that this heavy-handed constraint does not seem to harm the generator in any way, and indeed with most datasets it does not change the results much, but it prevents the escalation of signal magnitudes very effectively when needed.&lt;br /&gt;
이것을 위해 Local response normalization 의 변종을 사용하였다.
위 수식과같은, N은 피쳐맵의 수이고 a,b는 각각 픽셀 x,y 에서의 오리지날, 그리고 정규화된 피쳐이다. 우리는 이 강압적인 규제가 어떤식으로든 제네레이터에 해를 끼치지 않는것을 발견했고, 실제로 대부분의 데이터 셋에서 결과를 많이 바꾸지 않지만 필요할때 매우 효과적으로 신호크기의 증가를 방지하였다&lt;/p&gt;

&lt;h1 id=&quot;5-multi-scale-statistical-similarity-for-assessing-gan-results&quot;&gt;5. Multi-scale statistical similarity for assessing GAN results&lt;/h1&gt;

&lt;p&gt;In order to compare the results of one GAN to another, one needs to investigate a large number of images, which can be tedious, difficult, and subjective. Thus it is desirable to rely on automated methods that compute some indicative metric from large image collections. We noticed that existion methods such as MS-SSIM (Odena et al.,2017) find large-scale mode collapses reliably but fail to react to smaller effects such as loss of variation in colors or textures, and they also do not directly assess image quality in terms of similarity to the training set.&lt;br /&gt;
어떤 간과 다른 간의 결과를 비교하는방법은 이미지의 많은 숫자를 살펴야하고, 지루하고, 어렵고, 주관적이다. 따라서 큰이미지 모음에서 나타나는 몇가지 지표들을 계산하는 자동화된 방법에 의존하는것이 바람직하다. 우리는 MS-SSIM같은 현존하는 방법들이 큰 스케일에서의 mode collapse 문제를 감지했지만, 색상이나 텍스쳐 변화의 손실같은 작은 영향에 반응하지 못했다. 그리고 이미지 퀄리티가 학습셋과 유사한 정도에 직접적으로 접근하지 못한다.&lt;/p&gt;

&lt;p&gt;We build on the intuition that a successful generator will produce samples whose local image structure is similar to the training set over all scales. We propose to study this by considering the multiscale statistical similarity between distributions of local image patches drawn from Laplacian pyramid (Burt &amp;amp; Adelson, 1987) representations of generated and target images, starting at a low-pass resolution of $16\times16$ pixels. As per standard practice, the pyramid progressively doubles until the full resolution is reached, each successive level encoding the difference to an up-sampled version of the previous level.&lt;br /&gt;
우리는 직관을 바탕으로 성공적인 생성기를 모든 스케일에 거쳐 이미지 구조가 학습셋과 유사한 샘플을 생성하게 만들었다. 우리는 이것을 위해 생성된 표현의 라플라시안 피라미드에서 끌어낸 지역 이미지 패치들의 분포와 타겟 이미지 간의 다중스케일 통계적 유사도를 고려하는 것을 제안했고 시작은 낮은 해상도인 16픽셀에서 시작하였다. 매 표준 실행마다, 최대해상도에 도달할때까지 두배씩 점차적으로 커지고 각 연속적인 레벨마다 이전 레벨의 업샘플링 버전과의 차이를 인코딩한다.&lt;/p&gt;

&lt;p&gt;A single Laplacian pyramid level corresponds to a specific spatial frequency band. We randomly sample 1634 images and extract 128 descriptors from each level in the Laplacian pyramid, giving us $2^21$(2.1M) discriptors per level. Each descriptor is a $7\times7$ pixel neightborgoot with 3 color channels, denoted by $x\in\mathbb{R}^{7 \times 7 \times 3}= \mathbb{R}^{147}$. We denote the patches from level $l$ of the training set and generated set as ${x^l_{i} }^{2^{21}}&lt;em&gt;{i=1}$ and ${y^l&lt;/em&gt;{i} }^{2^{21}}&lt;em&gt;{i=1}$, respectively. We first normalize ${x^l_i}$ and ${y^l_i}$ w.r.t. the mean and standard deviation of each color channel, and then estimate the statistical similarity by computing their sliced Wasserstein distance $\mathrm{SWD}({x^l_i}, {y^l_i})$, an efficiently computable randomized approximation to earthmovers distance, using 512 projections ( Rabin et al.,2011)&lt;br /&gt;
하나의 라플라시안 피라미드 레벨은 특정한 주파수 대역과 일치한다. 우리는 랜덤하게 1634개의 이미지를 뽑고 라플라시안 피라미드에의 각 레벨에서 추출한 128개의 설명기를 추출하고 각 레벨당 $2^21$개의 디스크립터를 준다. 각 디스크립터는 $7\times7$ 픽셀 과 3가지 색상 채널이고 $x\in\mathbb{R}^{7 \times 7 \times 3}= \mathbb{R}^{147}$ 로 표기한다. 우리는 학습셋의 $l$번째 레벨에서 나온 패치와 생성된 셋을 각각 ${x^l&lt;/em&gt;{i} }^{2^{21}}&lt;em&gt;{i=1}$ , ${y^l&lt;/em&gt;{i} }^{2^{21}}_{i=1}$ 로 표기한다. 
첫번째로 ${x^l_i}, {y^l_i}$ 에 대해 노말라이즈 하고 각 채널에 대해 평균과 표준편차를 구한다. 그뒤 유사도를 측정하는데 Sliced Wasserstein distance SWD를 사용한다. 효과적으로 계산할 수 있는 무작위화한 지형이동거리로 512차원을 이용했다.&lt;/p&gt;

&lt;p&gt;Intiitively a small Wasserstein distance indicates that the distribution of the patches is similar, meaning that the training images and generator samples appear similar in both appearance and variation at this spatial resolution. In particular, the distance between the patch sets extracted from the lowest resolution $16\times16$images indicate similarity in large-scale image structures, while the finest-level patches encode information about pixel-level attributes such as sharpness of edges and noise.&lt;br /&gt;
직관적인 작은 W거리는 패치들이 얼마나 비슷한지를 표시하고, 이 공간 해상도에서 학습이미지와 생성 샘플의 외형및 변화가 비슷하게 보이는것을 의미한다. 특별히 16의 저해상도 이미지에서 추출된 패치사이의 거리는 큰 이미지 구조에서 유사도를 나타낸다. 가장 좋은 수준의의 패치는 선과 잡음의 날카로움 같은 픽셀레벨 특성에 대한 정보를 인코딩한다&lt;/p&gt;

&lt;h1 id=&quot;6-experiments&quot;&gt;6. Experiments&lt;/h1&gt;

&lt;p&gt;In this section we discuss a set of experiments that we conducted to evaluate the quality of our results. Please refer to Appendix A for detailed description of our network structures and training configurations. We also invite the reader to consult the accompanying video([https://youtu.be/G06dEcz-QTg])for additional result images and latent space interpolations. In this section we will distinguish between the network structure(e.g., convolutional layers, resizing), training configuration (various normalization layers, minibatch-related operations), and training loss (WGAN-GP, LSGAN)&lt;br /&gt;
이 섹션에서 우리는 우리의 결과의 퀄리티를 평가하기 행해진 실험들에 대해 토론한다. 부록 A 참조하면 우리 작업 구조와 학습 환경에 대해 세부적으로 설명되어있다. 우리는 또한 상담을 위해 리더를 초대하여 동반한 비디오를 올렸고 추가적인 결과 이미지와 latent space 에 대해 나와있다. 이 섹션에서 네트워크 구조, 학습환경, 그리고 학습 손실에 대해 다른 네트워크와의 차이를 말할것이다.&lt;/p&gt;

&lt;h2 id=&quot;61-importance-of-individual-contributions-in-terms-of-statistical-similarity&quot;&gt;6.1 Importance of Individual contributions in terms of statistical similarity&lt;/h2&gt;

&lt;p&gt;We will first use the sliced Wasserstein distance (SWD) and multi-scale structural similarty (MS-SSIM)(Odena et al.,2017) to evaluate the importance our individual contributions, and also perceptually validate the metrics themselves. We will do this by building on top of a previous state-of-the-art loss function(WGAN-GP) and training configuration (Gulrajani et al.,2017) in an unsupervised setting using CELEBA (Liu et al.,2015) and LSUN BEDROOM (Yu et al.,2015) datasets in $128^2$ resolution. CELEBA is particularly well suited for such comparison because the training images contain noticeable artifacts(aliasing, compression, blur) that are difficult for the generator to reproduce faithfully. In this test we amplify the differences between training configurations by choosing a relatively low-capacity network structure (Appendix A.2) and termination the training once the discriminator has been shown a total of 10M real images. As such the results are not fully converged.&lt;br /&gt;
첫번째로 SWD를 사용하고 MS-SSIM를 우리의 개별 평가를 위해 사용했다. 그리고 또한 지각적으로 그 평가지표들을 입증하였다. 우리는 그것을 위해 기존 최고성능인 WGAN-GP 로스와 128해상도의 CELEBA 와 LSUN BEDROOM 데이터의 학습환경을 사용하였다. CELEBA 는 이 비교에 특히 잘 적합되었다. 왜냐하면 학습 이미지는 뚜렷한 생성기가 뚜렷하게 복사하기 어려운 아티팩트를 포함하기 때문이다. 이 실험에서 학습환경의 차이를 증폭시키기 위해 상대적으로 적은 용량의 네트워크구조를 선택하고 판별기가 10M의 이미지를 보면 학습을 종료시켰다. 그렇게해서 결과가 완전히 수렴되진 않는다.&lt;/p&gt;

&lt;p&gt;Table 1 lists the numerical values for SWD and MS-SSIM in several training configurations, where our individual contributions are cumulatively enabled one by on top of the baseline (Gulrajani et al.,2017). The MS-SSIM numbers were averaged from 10000 pairs of generated images, and SWD was calculated as described in Sections 5. Generated CELEBA images from these configurations are shown in Figure 3. Due to space constraints, the figure shows only a small number of examples for each row of the table, but a significantly broader set is available in Appendix H. Intuitively, a good evaluation metric should reward plausible images that exhibit plenty of variation in colors, textures, and viewpoints. However, this is not captured by MS-SSIM:we can immediately see that configuration (h) generates significantly better images than configuration (a), but MS-SSIM remains approximately unchanged because it measures only the variation between outputs, not similarity to the training set. SWD, on the other hand, does indicate a clear improvement.&lt;br /&gt;
Table1은 몇개의 환경에서 SWD와 MS-SSIM 의 값 리스트이고, 각각의 환경은 베이스라인에서 하나씩 누적된다. MS-SSIM 값은 10000 쌍의 생성된 이미지의 평균이고, SWD 는 섹션5에서 서술한 방식으로 계산되었다. 이 환경들에서 생성된 CELEBA 이미지는 Figure3에 서 보여진다. 
공간제약 때문에, 테이블의 각행에서 적은 샘플만을 보여주지만, 많은 사진을 Appendix H 에서 볼수 있다.
직관적으로 좋은 평가 측도는 색상, 텍스쳐, 시점에서 충분한 변화를 보여주는 그럴듯한 이미지에 보상을 줘야한다. 하지만 이것은 MS-SSIM은 그런 부분이 감지되지 못하고, 우리는 (a)보다 (h)가 확실히 더 나은 이미지를 생성하는것을 즉시 알수있지만 MSSIM은 출력간의 변화만 측정하기에 거의 변하지 않는다, 학습셋의 유사도와 는 다르게. 반면에 SWD는 분명한 향상을 보여준다&lt;/p&gt;

&lt;p&gt;The first training configuration (a) corresponds to Gulrajani et al.(2017), featuring batch normalization in the generator, layer normalization in the discriminator, and minibatch size of 64. (b) enables progressive growing of the networks, which result in sharper and more believable output images. SWD correctly finds the distribution of generated images to be more similar to the training set.&lt;br /&gt;
첫번째 학습환경인 (a) 는 Gulrajani의 연구에 해당한다. 생성기에서 피쳐 배치노말을 하고, 판별기에선 레이어 노말, 그리고 미니배치사이즈는 64 이다. (b) 는 점점 PG 네트워크를 사용, 더 날카롭고 믿을만한 결과가 나왔다. SWD도 정확히 학습셋에 더 맞는 생성 이미지 분포를 찾았다.&lt;/p&gt;

&lt;p&gt;Our primary goal is to enable high output resolutions, and this requires reducing the size of minibatches in order to stay within the available memory budget. We illustrate the ensuing challenges in (c) where we decrease the minibatch size from 64 to 16. The generated images are unnatural, which is clearly visible in both metrics. In (d), we stabilize the training process by adjusting the hyperparameters as well as by removing batch normalization and layer normalization (Appendix A.2). As an intermediate test (e&lt;em&gt;), we enable minibatch discrimination (Salimans et al.,2016), which somewhat surprisingly fails to improve any of the metrics, including MS-SSIM that measures output variation. In contrast, our minibatch standard deviation (e) improves the average SWD scores and images. We then enable our remaining contributions in (f) and (g), leading to an overall improvement in SWD and subjective visual quality. Finally, in (h) we use a non-crippled network and longer training - we feel the quality of the generated images is at least comparable to the best published results so far.&lt;br /&gt;
우리의 첫번째 목표는 가능한 출력 해상도를 높이는 것이고, 이때 사용가능한 메모리 범위 내에서 미니배치를 줄이는것이 필요하다. (c)를 그릴때 미니배치를 62에서 16으로 감소시켰다. 생성된 이미지들은 
두 지표에서 명확히 보이듯 비정상적이다. (d) 에서는 학습과정에서 하이퍼파라미터를 잘 조정하고 배치노말과 레이어노말을 제거하여 안정되었다. (e&lt;/em&gt;) 에서는 minibatch discrimination 을 사용했고 어떤 측도도 눈에띄게 개선시키지 못했다. 반면 우리의 미니배치 표준편차법을 적용한 (e)는 평균 SWD 및 이미지를 향상시켰다. 우리의 기법들을 추가한 (f)와 (g), SWD와 이미지 퀄리티에서 전체적인 개선을 이끌었고 마지막으로 (h)에서 자르지 않은 네트워크와 긴 학습을 거쳤다. 우리는 생성된 이미지중 지금까지 결과중 가장 좋다고 생각한다.&lt;/p&gt;

&lt;h2 id=&quot;62-convergence-and-training-speed&quot;&gt;6.2 Convergence And Training Speed&lt;/h2&gt;

&lt;p&gt;Figure 4 illustrates the effect of progressive growing in terms of the SWD metric and raw image throughput. The first two plots correspond to the training configuration of Gulrajani et al.,(2017) without and with progressive growing. We observe that the progressive variant offers two main benefits : it converges to a considerable better optimum and also reduces the total training time by about a factor of two. The improved convergence is explained by an implicit form of curriculum learning that is imposed by the gradually increasing network capacity. Without progressive growing, all layers of the generator and discriminator are tasked with simultaneously finding succinct intermediate representations for both the large-scale variation and the small-scale detail. With progressive growing, however, the existing low-resolution layers are likely to have already converged early on, so the networks are only tasked with refining the representations by increasingly smaller-scale effects as new layers are introduced. Indeed, we see in Figure 4(b) that the lagest-scale statistical similarity curve (16) reaches its optimal value very quickly and remains consistent throughout the rest of the training. The smaller-scale curves(32,64,128) level off one by one as the resolution is inreased, but the convergence of each curve is equally consistent. With non-progressive training in Figure4(a), each scale of the SWD metric converges roughly in unison, as could be expected.&lt;br /&gt;
Figure 4는 SWD와 이미지 처리량에 대한 PG의 영향을 그리고 있다. 앞의 두 그림은 PG가 없는구라자니(WGAN-GP)의 학습환경과 일치한다. 단계별 변화가 제공하는 두가지 이점을 관측했다 : 그것은 매우 좋은 최적점으로 수렴하고 전체 학습시간을 2배가량 감소시켰다. 개선된 수렴은 점차적으로 네트워크 용량이 증가함으로 부가된 학습 과정의 암시적인 형태로 설명된다. PG없이 생성기와 판별기의 모든 레이어는 
작은 스케일에서의 디테일과 큰 스케일에서의 변화 양쪽의 가운데 표현을 간결한 중간표현을 동시에 찾는 작업을 수행한다. 그러나 PG와 함께하면 저해상도 레이어들은 미리 수렴되어버리고, 네트워크는 오직 새로운 레이어가 도입됨에 따라 점점 더 작은 효과에 의해 표현을 정제하는 작업을 하게된다. 실제로 Figure 4 에서 가장큰규모의 유사도 곡선은 최적의값에 매우 빨리 도달하고 남은 훈련 내내 유지되는것을 볼수있다. 작은 스케일에서 커브들은&lt;/p&gt;

&lt;p&gt;Teh speedup from progressive growing increases as the output resolution grows. Figure4(c) shows training progress, measured in number of real images shown th the discriminator, as a function of training time when the training progresses all the way to $1024^2$ resolution. We see that progressive growing gains a significant head start because the networks are shallow and quick to evaluate at the begining. Once the full resolution is reached, the image throughput is equal between the tWo methods. The plot shows that the progressive variant reaches approximately 6.4 million images in 96 hours, whereas it can be extrapolated that the non-progressive variant would take about 520 hours to reach the same point. In this case, the progressive growing offers roughly a $5.4\times$ speedup&lt;/p&gt;

&lt;h2 id=&quot;63-high-resolution-image-generation-using-celeba-hq-dataset&quot;&gt;6.3 High-Resolution Image Generation Using CELEBA-HQ Dataset&lt;/h2&gt;

&lt;p&gt;To meaningfully demonstrate our results at high output resolutions, we need a sufficiently varied high-quality dataset. However, virtually all publicly available datasets previously used in GAN literature are limited to relatively low resolutions ranging from $32^2$ to $480^2$. To this end, we created a high-quality version of the CELEBA dataset consisting of 30000 of the images at $1024 \times 1024$resolution. We refer to Appendix C for further details about the generation of this dataset.&lt;/p&gt;

&lt;p&gt;Our contributions allow us to deal with high output resolutions in a robust and efficient fashion. Figure 5 shohws selected $1024\times 1024$ images produced by our network. While megapixel GAN results have been shown before in another dataset(Marchesi, 2017), our results are vastly more varied and of higher perceptual quality. Please refer to Appendix F for a larger set of result images as well as the nearest neighbors found from the training data. The accompanying video shows latent space interpolations and visualizes the progressive training. The interpolation works so that we first randimize a latent code for each frame (512 components sampled individually from $\mathcal{N}(0,1))$, then blur the latents across time with a Gaussian ($\sigma = 45$ frames @ 60 Hz), and finally normalize each vector to lie on a hypersphere.&lt;/p&gt;

&lt;p&gt;We trained the network on 8 Tesla V100 GPUs for 4 days, after which we no longer observed qualitative differences between the result of consecutive training iterations. Our implementation used an adaptive minibatch size depending on the current output resolution so that the available memory budget was optimally utilized.&lt;/p&gt;

&lt;p&gt;In order to demonstrate that our contributions are largely orthogonal to the choice of a loss function, we also trained the same network using LSGAN loss insted of WGAN-GP loss. Figure 1 shows six examples of $1024^2$ images produced using our method using LSGAN. Futher details of this setup are given in Appendix B.&lt;/p&gt;

&lt;h2 id=&quot;64-lsun-results&quot;&gt;6.4 LSUN Results&lt;/h2&gt;

&lt;p&gt;Fugure 6 shows a purely visual comparison between our solution and earlier results in LSUN BEDROOM. Fiture 7 gives selected examples from seven very different LSUN categories at $256^2$. A larger, non-curated set of results from all 30 LSUN categories is available in Appendix G, and the video demonstrates interpolations. We are not aware of earlier results in most of these categories, and while some categories work better than others, we feel that the overall quality is high.&lt;/p&gt;

&lt;h2 id=&quot;65-cifan10-inception-scores&quot;&gt;6.5 CIFAN10 Inception Scores&lt;/h2&gt;

&lt;p&gt;The best inception scores for CIFAR10 (10 categories of $32\times 32$ RGB images) we are aware of are 7.90 for unsupervised and 8.87 for label conditioned setups (Grinblat et al., 2017). The large difference between the two numbers is primarily caused by “ghosts” that necessarily appear between classes in the unsupervised setting while label conditioning can remove many such transitions.&lt;/p&gt;

&lt;p&gt;When all of our contributions are enabled, we get 8.80 in the unsupervised setting. Appendix D shows a representative set of generated images along with a more comprehensive list of results from earlier methods. The network and training setup were the same as for CELEBA, progression limited to $32 \times 32$ of course. The only customization was to the WGAN-GP’s regularization term&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">Abstract</summary></entry><entry><title type="html">파이썬에서 OpenCV 사용</title><link href="http://localhost:4000/opencv/2019/07/12/opencv-python.html" rel="alternate" type="text/html" title="파이썬에서 OpenCV 사용" /><published>2019-07-12T10:05:05+09:00</published><updated>2019-07-12T10:05:05+09:00</updated><id>http://localhost:4000/opencv/2019/07/12/opencv-python</id><content type="html" xml:base="http://localhost:4000/opencv/2019/07/12/opencv-python.html">&lt;h1 id=&quot;opencv&quot;&gt;OpenCV&lt;/h1&gt;

&lt;p&gt;컴퓨터 비전을 위한 오픈소스 라이브러리이다
여러 언어에서도 다 사용가능하고 강력하다하니 배워보자&lt;/p&gt;

&lt;h1 id=&quot;설치&quot;&gt;설치&lt;/h1&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ pip install opencv-python
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;끗&lt;/p&gt;

&lt;h1 id=&quot;불러오기&quot;&gt;불러오기&lt;/h1&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import cv2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;끗&lt;/p&gt;

&lt;p&gt;참고로 따로 창이 생성되어 작업이되는게 많아 jupyter 환경에서는 좀 문제가 많이생기니
VSCode 나 파이참같은 환경을 추천
(물론 피해가는 방법도 있는데 이런것은 추천하지않는다 언제 막힐지 몰라서…)&lt;/p&gt;

&lt;h1 id=&quot;세부-함수&quot;&gt;세부 함수&lt;/h1&gt;

&lt;h2 id=&quot;이미지-함수들&quot;&gt;이미지 함수들&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 이미지 불러오는 함수
image = cv2.imread('이미지 파일')

# 이미지 보여주는 함수
# 참고로 주피터는 사용 불가 plt.imshow 추천
cv2.imshow('Test', image)
cv2.waitKey(0)
cv2.destroyAllWindows()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;영상-함수들&quot;&gt;영상 함수들&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 영상 불러오는 함수
vidcap = cv2.VideoCapture('비디오 파일')

# 영상 한프레임 받아오기
# 자동으로 다음 프레임상태로 넘어감
# 더이상 영상이없으면 ret가 False 출력
# 루프문으로 적당히 돌려주면 된다
# frame 은 ndarray 형태로 바로 사용가능
ret, frame = vidcap.read()

# 메모리 해제
vidcap.release() 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;카메라-함수들&quot;&gt;카메라 함수들&lt;/h1&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 카메라 연결하는 함수
vidcap = cv2.VideoCapture( camera_num )

# 영상 설정
vidcap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
vidcap.set(cv2.CAP_PROP_FRAME_HEIGHT, width)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Aru</name></author><summary type="html">OpenCV</summary></entry><entry><title type="html">Few-Shot Adversarial Learning of Realistic Neural Talking Head Models</title><link href="http://localhost:4000/paper/2019/07/11/Few-Shot.html" rel="alternate" type="text/html" title="Few-Shot Adversarial Learning of Realistic Neural Talking Head Models" /><published>2019-07-11T10:05:05+09:00</published><updated>2019-07-11T10:05:05+09:00</updated><id>http://localhost:4000/paper/2019/07/11/Few-Shot</id><content type="html" xml:base="http://localhost:4000/paper/2019/07/11/Few-Shot.html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural netrworks to generate them.&lt;br /&gt;
몇몇 요새 작업은 컨볼넷을 학습시켜 만든 높은 현실감을 가진 사람의 머리 이미지를 얻었다&lt;/p&gt;

&lt;p&gt;In order to crteate a personalized talking head model, these works require training on a large dataset of images of a single person&lt;br /&gt;
개인이 말하는 머리모델을 만들기 위해서는 한사람의 많은 데이터를 학습할 필요가 있었다&lt;/p&gt;

&lt;p&gt;However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image.&lt;br /&gt;
그러나 많은 상황에서 이 말하는 머리모델은 몇장의 이미지만으로 학습을 해야했고, 심지어 한장의 이미지로도 해야했다.&lt;/p&gt;

&lt;p&gt;Here, we present a system with such few-shot capability. &lt;br /&gt;
우리는 퓨샷 능력을 가진 시스템을 소개한다&lt;/p&gt;

&lt;p&gt;It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators.&lt;br /&gt;
큰 비디오 데이터셋에 대한 장기 학습을 수행하면 이후 큰 규모의 generator와 discriminator 의 적대적 훈련으로 몇장 혹은 한장으로 이전까지 할 수 없던 말하는 머리모델이 가능해진다&lt;/p&gt;

&lt;p&gt;Crucially the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters.&lt;br /&gt;
결정적으로 이 시스템은 generator 와 discriminator 사람 마다 특유의 방법으로 매개변수를 초기화할수 있고 몇장의 이미지를 기반으로 빠른속도로 학습이 가능하다
천만개의 파라미터만 사용하는데도 불구하고&lt;/p&gt;

&lt;p&gt;We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings.&lt;br /&gt;
이 접근법은 높은 현실성과 개인성을 가진 말하는 모델을 학습하는것을 가능하게한다. 새로운 사람이나 심지어 초상화를 가지고&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In this work, we consider the task of creating personalized photorealistic talking head models, i.e. systems that can synthesize plausible video-sequences of speech expressions and mimics of a particular individual.&lt;br /&gt;
이 작업에서 개인적인 사실주의적인 말하는 모델을 만드는것을 고려했다, 예를들면 특정인의 흉내와 연설 비디오를 그럴듯하게 합성해 낼수 있다.&lt;/p&gt;

&lt;p&gt;More specifically, we consider the problem of synthesizing photorealistic personalized head images given a set of face landmarks, which drive the animation of the model.&lt;br /&gt;
더 구체적으로 모델의 애니메이션을 구동하는 얼굴의 랜드마크세트를 주어 사실적인 이미지의 합성 문제를 고려했다.&lt;/p&gt;

&lt;p&gt;Such ability has practical applications for telepresence, including video conferencing and multi-playergames, as well as special effects industry. 이 능력은  telepresence의 실용적인 어플을 위한 능력을 가지고 있다. 화상채팅이나 멀티게임, 특수효과산업까지 포함해서&lt;/p&gt;

&lt;p&gt;Synthesizing realistic talking head sequences is known to be hard for two reasons.&lt;br /&gt;
사실적 말하는 머리 시퀸스를 만들기는 두가지 이유때문에 어려운것으로 알려져있다.&lt;/p&gt;

&lt;p&gt;First, human heads have high photometric, geometric and kinematic complexity.&lt;br /&gt;
첫번째로 휴먼의 머리는 광도, 기하, 운동학적 복합성을 가지고 있다&lt;/p&gt;

&lt;p&gt;This complexty stems not only from modeling faces(for which a large number of modeling approaches exist) but also from modeling mouth cavity, hair, and garments.&lt;br /&gt;
이 복잡도는 얼굴 모델링 뿐만 아니라 입 구멍, 머리 , 그리고 옷까지 해야한다&lt;/p&gt;

&lt;p&gt;The second complicating factor is the acuteness of the human visual system towards even minor mistakes in the appearance modeling of human heads (the so-called uncanny vally effect).&lt;br /&gt;
두번째로 복잡하게 하는 요인은 휴먼헤드모델링에서 생기는 작은 실수에 대한 인간 시각시스템의 날카로움이다(언캐니 밸리 이펙트라 불린다)&lt;/p&gt;

&lt;p&gt;Such low tolerance to modeling mistakes explains the current prevalence of non-photorealistic cartoon-like avatars in man practically-deployed teleconferencing systems.&lt;br /&gt;
이 모델링 실수들에 대한 적은 내성 때문에 
실제 사용되는 원격회의에서 만화같은 비현실적 아바타가 유행하는것이 설명된다&lt;/p&gt;

&lt;p&gt;To overcome the challenges, several works have proposed to synthesize articulated head sequences by warping a single or multiple static frames.&lt;br /&gt;
이 문제를 극복하기 위해, 몇몇 작업은 단일 혹은 복수의 정적 프레임을 휘는 방식으로 관절이 있는 머리에 합성하는것을 제안했다.&lt;/p&gt;

&lt;p&gt;Both classical warping algorithms and warping fields synthesized using machine learning(include deep learning) can be used for such purposes.&lt;br /&gt;
기계학습을 사용하여 합성된 기존의 워핑 알고리즘과 워핑공간 모두 이 목적으로 사용될 수 있다&lt;/p&gt;

&lt;p&gt;While warping-based systems can create talking head sequences from as little as a single image, the amount of motion, head rotation, and disocclusion that they can handle without noticeable artifact is limited.&lt;br /&gt;
워핑 기반 시스템은 단일 이미지에서 말하는 머리 영상을 만들수 있으나, 구체적인 인공조작 없이 처리할수 있는 전체 움직임이나 머리의 회전의 해제에 있어서는 제한적이다&lt;/p&gt;

&lt;p&gt;Direct(warping-free) Synthesis of video frames using adversarially-trained deep convolutional networks(ConvNets) presents the new hope for photorealistic talking heads.&lt;/p&gt;

&lt;p&gt;적대적 컨볼넷을 사용한 비디오 프레임의 직접 합성은 사실적 말하는 머리의 새로운 희망을 제시했다.&lt;/p&gt;

&lt;p&gt;Very recently, some remarkably realistic results have been demonstrated by such system [16,20,37]. However, to succeed, such methods have to train large networks, where both generator and discriminator have tens of millions of parameters for each talking head.&lt;br /&gt;
아주 최근에 매우 현실적인 결과가 위 시스템에 의해 증명되었다. 그러나 성공을 위해서는 위 메소드들은 많은 학습시간을 필요로 했고 생성기와 판별기 는 천만개의 파라미터가 필요하다 각각 머리에 대하여&lt;/p&gt;

&lt;p&gt;These systems, therefore, require a several-minutes-long video [20,37] or a large dataset of photographs [16] as well as hours of GPU training in order to create a new personalized talking head model.&lt;br /&gt;
이 시스템들은 게다가 새로운 사람의 머리모델을 만들러면 몇분의 긴 비디오를필요로 하거나 많은 사진 데이터셋과 시간단위의 GPU 트레이닝이 필요하다&lt;/p&gt;

&lt;p&gt;While this effort is lower than the one required by systems that construct photo-realistic head models using sophisticated physical and optical modeling [1], it is still excessive for most practical telepresence scenarios, where we want to enable users to create their personalized head models with as little effort as possible.&lt;br /&gt;
이 과정은 정교한 물리 및 광학 모델링을 사용해 구축하는거보다 낮음지만 여전히 
사용자가 가능한 적은 노력으로 그들 개인의 머리모델을 만들어 사용하고자 하는 대부분에 실제 화상회의에는 너무 과한 학습이 요구된다&lt;/p&gt;

&lt;p&gt;In this work, we present a system for creating talking head models from a handful of photographs(so-called few shot learning) and with limited training time. In fact, our system can generate a reasonable result based on a single photograph(one-shot learning), while adding a few more photographs increases the fidelity of personalization.&lt;br /&gt;
이 작업에서 제한된 시간에서 몇장의 사진을 가지고 말하는 머리모델을 만드는 시스템을 제시한다
사실 이 시스템은 한장의 사진을 기반으로 해도 합리적인 결과가 나오고 사진의 추가는 개인화를 더 추가해주는 것이다&lt;/p&gt;

&lt;p&gt;Similarly to [16,20,37], the talking heads created by our model are deep ConvNets that synthesize video frames in a direct manner by a sequence of convolutinal operations rather than by warping.&lt;br /&gt;
위 논문들과 비슷하게 말하는 머리는 워핑에 의하기보단 컨볼루션 시퀸스를 직접 합성하는 방식의 컨볼넷으로 만들어 진다.&lt;/p&gt;

&lt;p&gt;The talking heads created by our system can, therefore, handle a large vairety of poses that goes beyond the abilities of warping-based systems.&lt;br /&gt;
말하는 머리는 우리의 시스템에 의해 생성되고, 워핑베이스 시스템의 능력을 뛰어넘는 다양한 포즈를 할수 있다.&lt;/p&gt;

&lt;p&gt;The few-shot learning ability is obtained through extensive pre-training (meta-learning) on a large corpus of talking head videos corresponding to different speakers with diverse appearance.&lt;br /&gt;
퓨샷러닝 능력은 큰 뭉치의 다양한외모의 사람들에 말하는 머리 비디오를 이용해 광범위한 사전 훈련을 통해 얻는다.&lt;/p&gt;

&lt;p&gt;In the course of meta-learning, our system simulates few-shot learning tasks and learns to transform landmark positions into realistically-looking personalized photographs, given a small training set of images with this person.&lt;br /&gt;
이 사전훈련 과정에 우리 시스템은 퓨샷러닝 작업을 시뮬레이트하고 어떤 사람과의 이미지셋을 통한 작은 훈련으로 랜드마크 포지션을 실제처럼 보이는 개인사진으로 바꾸는 방법을 배운다.&lt;/p&gt;

&lt;p&gt;After that, a handful of photographs of a new person sets up a new adversarial learning probelm with high-capacity generator and discriminator pre-trained via meta-laerning.&lt;br /&gt;
이후 적은 수의 새로운 사람의 사진으로 
메타 러닝을 통해 사전학습된 고용량의 생성기와 판별기로 새로운 적대적 학습을 한다&lt;/p&gt;

&lt;p&gt;The new adversarial problem converges to the state that generates realistic and personalized images after a few training steps.&lt;br /&gt;
새로운 적대적문제는 몇 훈련 스텝 이후 현실적이고 개인화된 이미지를 만드는 상태로 수렴한다&lt;/p&gt;

&lt;p&gt;In the experiments, we provide comparisons of talking heads created by our system with alternative neural talking head models [16,40] via quantitative measurements and a user study, where our approach generates images of sufficient realism and personalization fidelity to deceive the study participants.&lt;br /&gt;
이 실험에서 우리의 시스템과 대립되는 뉴럴 말하는 모델의 비교를 정량적 측정과 사용자 연구를 통해 제공하고, 우리의 접근방식은 연구 참가자들을 속이기 위해 사실적이고 개인의 특성이 충분히 들어간 이미지를 생성하였다&lt;/p&gt;

&lt;p&gt;We demonstrate several uses of our talking head models, including video synthesis using landmark tracks extracted from video sequences of the same person, as well as puppeteering (video snthesis of a certain person based on the face landmark tracks of a different person)&lt;br /&gt;
예제 뿐만 아니라 동일인의 비디오에서 추출한 랜드마크 트랙을 사용하여 합성한 비디오를 포함하여 
모델을 몇 차례 사용해보는것으로 증명하였다.
(특정인의 비디오 합성은 다른사람의 랜드마크 트랙을 기반으로 하였다)&lt;/p&gt;

&lt;h1 id=&quot;2-related-work&quot;&gt;2. Related work&lt;/h1&gt;

&lt;p&gt;A huge body of works is devoted to statistical modeling of the apperance of human faces [6], with remarkably good results obtained both with classical techniques [35] and, more recently, with deep learning &lt;a href=&quot;to name just a few&quot;&gt;22,25&lt;/a&gt;.&lt;br /&gt;
거대한 작품들이 인간얼굴의 외관을 통계적으로 모델링하는데 기여하고 있다. 고전적인 기법과 좀더 최근의 딥러인을 이용한 기법 모두 현저히 좋은 결과.&lt;/p&gt;

&lt;p&gt;While modeling faces is a highly related task to talking head modeling, the two tasks are not identical, as the latter also involves modeling non-face pats such as hair, neck, mouth cavity and often shoulders/upper garment.&lt;br /&gt;
얼굴 모델링은 말머리모델과 높은 관계를 가지지만 두 작업은 동일하지 않다, 후자는 머리, 목, 입속이나 종종 어깨나 상의처럼 얼굴이 아닌 부분도 포함한다&lt;/p&gt;

&lt;p&gt;These non-face parts cannot be handled by some trivial extension of the face modeling methods since they are much less amenable for registration and often have higher variability and higher complexity than the face part.&lt;br /&gt;
이 비얼굴 부분은 얼굴 모델링 메소드의 사소한 확장정도로 다뤄지며 안되는데, 그것들은 등록하기 쉽지않고 종종 얼굴파트보다 훨신 더 변화가 많고 높은 복잡성을 가지기 때문이다&lt;/p&gt;

&lt;p&gt;In principle, the results of face modeling [35]or lips modeling [31] can be stitched into an existing head video.&lt;br /&gt;
원칙적으로 얼굴이나 입술모델의 결과는 기존 얼굴헤드 모형에 삽입할수있다.&lt;/p&gt;

&lt;p&gt;Such design, however, does not allow full control over the head rotation in the resulting video and therefore does not result in a fullyfledged talking head system.&lt;br /&gt;
하지만 이 디자인은 결과비디오에서 머리 회전에 대한 완전한 조작을 통제룰 허락하지 못하고 그 결과 완전한 말머리 시스템이 만들어지지 않는다&lt;/p&gt;

&lt;p&gt;The design of our system borrows a lot from the recent progress in generative modeling of images. &lt;br /&gt;
우리의 시스템 디자인은 일반적인 이미지 모델의 최근의 동향을 많이 빌려온다&lt;/p&gt;

&lt;p&gt;Thus, our architecture uses adversarial training [12] and, more specifically, the ideas behind conditional discriminators [23], including projection discriminators [32]. &lt;br /&gt;
구조는 적대적 학습이고 조금 구체적으로는 프로젝션 판별기를 포함한 조건부 판별기 기법이다&lt;/p&gt;

&lt;p&gt;Our meta-learning stage uses the adaptive instance normalization mechanism [14], which was shown to be useful in large-scale conditional generation tasks [2,34].&lt;br /&gt;
메타러닝 부분은 노말라이제이션 대신 adaptive 방식을 사용했고 그것은 큰규모의 조건부 생성작업에서 유용한것을 보여준다.&lt;/p&gt;

&lt;p&gt;The model-agnostic meta-learner(MAML) [10] uses meta-learning to obtain the initial state of an image clasifier, from which it can quickly converge to image classifiers of unseen classes, given few training samples.&lt;br /&gt;
이미지분류기의 초기값을 얻기 위해 메타러닝을 사용한다, 이것은 MAML 는 훈련샘플이 별로 없는 경우 미지의 클래스로의 이미지 분류를 위한 빠른 수렴이 가능하게 한다&lt;/p&gt;

&lt;p&gt;This high-level idea is also utilized by our method, though our implementation of it is rather different. 
이 고수준 아이디어도 우리의 메소드에 의해 활용되었다. 조금 다르긴 하지만.&lt;/p&gt;

&lt;p&gt;Several works have further proposed to combine adversarial training with meta-learning. 
몇몇의 작업은 메타러닝과 적대적학습의 조합으로 제안되었다&lt;/p&gt;

&lt;p&gt;Thus, data-augmentation GAN [3] , Meta-GAN[43], adversarial meta-learning [41] use adversarially-trained networks to generate additinal examples for classes unseen at the meta-learning stage. &lt;br /&gt;
따라서 위 세 연구에서 선행학습 단계에서 미지의 클래스 분류를 위한 추가적인 샘플 생성에서 적대적 학습 네트워크가 사용되었다&lt;/p&gt;

&lt;p&gt;While these methods are focused on boosting the few-shot classification performance, our method deals with the training of image generation models using similar adversarial objectives. &lt;br /&gt;
그 방법들은 몇장을써서 분류하는것의 성능을 높이는것에 집중하지만, 우리방법은 similar adversarial objectives 를 사용하여이미지를 생성 모델을 학습 하는 것을 다루었다.&lt;/p&gt;

&lt;p&gt;To summarize, we bring the adversarial fine-tuning into the meta-learning framework. &lt;br /&gt;
요약하면 우리는 적대적 fine-tuning 을 메타러닝 프레임워크에 집어 넣었다.&lt;/p&gt;

&lt;p&gt;The former is applied after we obtain initial state of the generator and the discriminator networks via the mta-learning stage.&lt;br /&gt;
전자는 메타러닝 스테이지를 통해 생성기와 판별기를 초기상태를 얻은 후에 적용된다.&lt;/p&gt;

&lt;p&gt;Finally, very related to ours are the two recent works on text-to-speech generation [4,18].&lt;br /&gt;
마지막으로 매우 밀접한 것으로는 텍스트로 음성을 생성하는 두가지 작업이 있다.&lt;/p&gt;

&lt;p&gt;Their setting (few-shot learning of generative model) and some of the components (standalone embedder network, generator fine-tunning) are also used in our case.&lt;br /&gt;
그들의 세팅과 몇몇 구성요소는 우리 케이스에도 쓰인다.&lt;/p&gt;

&lt;p&gt;Our work differs in the application domain, the use of adversarial learning, its specific adaptation to the meta-learning process and numerous implementation details.&lt;br /&gt;
우리의 작업은 어플리케이션 영역, 적대적학습의 사용, 메타러닝 과정의 특정 적용방식과 수많은 세부사항에서 다르다.&lt;/p&gt;

&lt;h1 id=&quot;3-methods&quot;&gt;3. Methods&lt;/h1&gt;

&lt;h2 id=&quot;31-architecture-and-notation&quot;&gt;3.1 Architecture and notation&lt;/h2&gt;

&lt;p&gt;The meta-learning stage of our approach assumes the availability of M video sequences, contaning talking heads of different people.&lt;br /&gt;
우리의 접근방식은 메타러닝 단계에서 다른 사람들의 말머리가 들어있는 M 영상의 유용성을 가정한다&lt;/p&gt;

&lt;p&gt;We denote with $\mathrm{x}_i$ the $i$-th video sequence and with $\mathrm{x}_i(t)$ ist $t$-th frame.&lt;br /&gt;
$x_i$는 i번째 비디오를, $\mathrm{x}_i(t)$는 그것의 t 번째 스텝을 나타낸다&lt;/p&gt;

&lt;p&gt;During the learning process, as well as during test time, we assume the availability of the face landmarks’ locations for all frames (we use an off-the-shelf face alignment code [[7] to obtain them).&lt;br /&gt;
테스트타임 뿐만아니라, 학습단계에서 모든 프레임에 대한 페이스 랜드마크 위치의 유용성을 가정했다&lt;/p&gt;

&lt;p&gt;The landmarks are rasterized into three-channel images using a predefined set of colors to connect certain landmarks with line segments. We denote with $\mathrm{y}_i(t)$ the resulting landmark image computed for $\mathrm{x}_i(t)$&lt;/p&gt;

&lt;p&gt;랜드마크는 사전정의된 색상 세트를 사용하여 3차원 이미지로 변환되고 특정 랜드마크와 선 세그먼트가 연결된다. $\mathrm{y}_i(t)$ 는 $\mathrm{x}_i(t)$ 의 이미지 계산 결과임을 나타낸다&lt;/p&gt;

&lt;p&gt;In the meta-learning stage of our approach, the following three networks are trained (Figura 2) :&lt;br /&gt;
우리 방법의 메타러닝 단계에서 아래 3가지 네트워크가 학습된다&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The $embedder$ $E(\mathrm{x}_i(s),\mathrm{y}_i(s); \phi)$ takes a video frame $\mathrm{x}_i(s)$, an associated landmark image $\mathrm{y}_i(s)$ and maps these inputs into an $N$-dimentional vector $\hat{\mathrm{e}}_i(s)$. Here, $\phi$ denotes network parameters that are learned in the meta-learning stage. In general, during meta-learning we aim to learn $\phi$ such that the vector $\hat{\mathrm{e}}_i(s)$ contains video-specific information (such as the person’s identity) that is invariant to the pose and mimics in a particular frame s. We denote embedding vectors computed by the embedder as $\hat{\mathrm{e}}_i$&lt;br /&gt;
임베더 는 비디오 프레임 $\mathrm{x}_i(s)$를 가져오고 연관된 랜드마크 이미지 $\mathrm{y}_i(s)$ 와 N차원 벡터에 맵핑시킨다. 여기서 $\phi$ 는 메타러닝 스테이지에서 학습된 네트워크 파라미터를 나타낸다.
일반적으로 메타러닝동안 벡터 $\hat{\mathrm{e}}_i(s)$ 가 비디오정보(개인의 특성)같은 것을 포함하도록 $\phi$를 배우는것을 목표로 한다. 특정 프레임의 포즈와 표정에 영향을 미치지 않도록. 우리는 임베더에 의해 계산된 임베딩 벡터를 $\hat{\mathrm{e}}_i$ 라고 부른다&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The $generator$ $G(\mathrm{y}_i(t), \hat{\mathrm{e}}_i; \psi, P)$ takes the landmark image $\mathrm{y}_i(s)$ for the video frame not seen by the embedder, the predicted video embedding $\hat{\mathrm{e}}_i$, and outputs a synthesized video frame $\hat{\mathrm{x}}_i(t)$. The generator is trained to maximize the similarity between its output and the ground truth frames. All parameters of the generator are split into two sets: the person-generic parameters $\psi$. During meta-learning only $\psi_i$ are trained directly, while $\psi_i$ are predicted from the embedding vector $\hat{\mathrm{e}}_i$ using a trainable projection matrix $\mathrm{P}:\hat\psi_i=\mathrm{P}\hat\mathrm{e}_i$&lt;br /&gt;
제네레이터 G는 임베더가 보지못한 랜드마크 이미지 y 를 가져오고, 예측된 임베딩 e 와  합성된 비디오프레임 X 를 출력한다. G는 출력과 기존 이미지의 유사도가 최대화 되도록 학습된다. G의 모든 파라미터는 두가지로 나뉜다. 인간의 전체적 파라미터 $\psi$. 메타러닝은 오직 프사이만 직접 학습시킨다, 프사이는 임베딩 벡터 E 를 학습가능한 투영 행렬 P를 사용하여 학습된다&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The $discriminator$ $D(\mathrm{x}_i(t), \mathrm{y}_i(t), i; \theta, \mathrm{W, w}_0, b)$ take a video frame $\mathrm{x}_i(t)$, an associated landmark image $\mathrm{y}_i(t)$ and the index of the training sequence $i$. Here, $\theta, \mathrm{W,w0}$ and $b$ denote the learnable parameters associated with the discriminator. Thr discriminator contains a ConvNet part $V(\mathrm{x}_i(t),\mathrm{y}_i(t);\theta)$ that maps the input frame and the landmark image into an $N$-dimensional vector. The discriminator predicts a single scalar (realism score) $r$, that indicates, whether the input frame $\mathrm{x}_i(t)$ is a real frame of the $i$-th video sequence and whether it matches the input pose $\mathrm{y}_i(t)$, based on the output of its ConvNet part and the parameters $\mathrm{W,w_0}, b$.&lt;/li&gt;
  &lt;li&gt;D 는 비디오프레임 x와 연관된 랜드마크 이미지 y 와 시퀸스인덱스 i 를 입력받는다.
여기서 뒤에 4개는 D 와 관련된 학습가능한 파라미터이다. D 는 컨브넷파트 V를 포함하는데 이것은 입력프레임과 랜드마크이미지를 N차원 벡터에 맵핑한다. D 는 스칼라값 r 을 예측하는데 이것은 실제 입력 프레임x 와 인풋포즈 y 가 얼마나 매치하는지 여부를 나타낸다. V 파트와 D의 파라미터들에 기반하여&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;32-meta-learning-stage&quot;&gt;3.2 Meta-learning stage&lt;/h2&gt;

&lt;p&gt;During the meta-learning stage of our approach, the parameters of all three networks are trained in an adversarial fashion. It is done by simulation episodes of K-shot learning (K=8 in our experiments). In each episode, we randomly draw a training video sequence $i$ and a single frame t from that sequence. In addition to t, we randomly draw additional K frames $s_1,s_2,\cdots,s_K$ from the same sequence.&lt;br /&gt;
우리의 메타러닝 과정에서 3가지 네트워크의 모든 파라미터는 적대적 방식으로 학습된다. 이것은 K샷 학습의 시뮬레이션에피소드로 이루어진다 (K는 8로 실험하였다). 각 에피소드에서 무작위로 i번째 비디오와 그 비디오의 t번째 프레임을 그려낸다
추가로 같은 시퀸스에서 추가로 K개의 프레임을 더 그린다.&lt;/p&gt;

&lt;p&gt;We then compute the estimate $\mathrm{e}$ of the i-th video embedding by simply averaging the embeddings $\hat\mathrm{e}_i(s_k)$ predicted for these additional frames :&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">Abstract</summary></entry><entry><title type="html">EfficientNet : Rethinking Model scaling for Convolutional Neural Networks</title><link href="http://localhost:4000/paper/2019/07/04/EffecientNet.html" rel="alternate" type="text/html" title="EfficientNet : Rethinking Model scaling for Convolutional Neural Networks" /><published>2019-07-04T10:05:05+09:00</published><updated>2019-07-04T10:05:05+09:00</updated><id>http://localhost:4000/paper/2019/07/04/EffecientNet</id><content type="html" xml:base="http://localhost:4000/paper/2019/07/04/EffecientNet.html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Convolutinal Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and the scaled up for better accuracy if more resources are available.&lt;br /&gt;
CNN은 일반적으로 고정된 자원 예산 안에서 개발된다 그리고 더 많은자원이 있을경우 더 높은 정확도를 위해 규모를 키운다&lt;/p&gt;

&lt;p&gt;In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance&lt;br /&gt;
이 논문에서는 체계적으로 모델의 스케일을 학습하고 더 좋은 퍼포먼스를 이끌수있는 깊이와 넓이 그리고 해상도의 밸런스를 알아낸다&lt;/p&gt;

&lt;p&gt;Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/ width/ resolution using a simple yet highly effective compound coefficient.&lt;br /&gt;
이 관측에 기반하여 우리는 간단하고도 매우 효과적인 합성계수를 사용하여 깊이, 넓이, 해상도의 차원을 균일하게 조절할 새로운 스케일 매소드를 제안한다&lt;/p&gt;

&lt;p&gt;We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet&lt;br /&gt;
우리는 이 메소드의 상승 효과를 모바일넷과 레즈넷에서 시연하였다&lt;/p&gt;

&lt;p&gt;To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets.&lt;/p&gt;

&lt;p&gt;더 나아가, 우린 새로운 시작 베이스라인을 디자인하고 그것을 스케일업한 에피션넷이라 불리는모델들을 얻었다 이전의 ConvNet보다 정확도와 효율성이 좋은&lt;/p&gt;

&lt;p&gt;In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on Imagenet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet.&lt;br /&gt;
특히 에피션넷은 이미지넷 에서 탑1에서 84.4%, 탑5에서 97.1%의 최고기록인 정확도를 달성했고 기존의 베스트 ConvNet보다 8.4배 작고 6.1배 빠른 속도로 추론했다&lt;/p&gt;

&lt;p&gt;Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100(91.7%), Flowers(98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.&lt;br /&gt;
에피션트넷의 또다른 최고기록은 CIFAR100에서 91.7%, Flowers에서 98.8%, 그리고 3개의 다른 데이터셋에서도 세웠고 매개변수의 수는 더 적었다.&lt;/p&gt;

&lt;p&gt;Source code is at 
https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.&lt;/p&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;Scaling up ConvNets widely used to achieve better accuracy.  &lt;br /&gt;
콘브넷의 스케일업은 더높은 정확도의달성을 위해 널리 사용된다&lt;/p&gt;

&lt;p&gt;For example, ResNet can be scaled up from ResNet-18 to ResNet-200 by using more layers.&lt;br /&gt;
예를들면 레즈넷은 18부터 200까지 레이어를 늘리는것으로 스케일업이 가능하다&lt;/p&gt;

&lt;p&gt;Recently, GPipe achieved 84.3% ImageNet top-1 accuracy by scaling up a baseline model for time larger&lt;br /&gt;
요즘엔 GPipe는 이미지넷 탑1에서 84.3%를 달성했다 
베이스라인의 4배로 스케일업을해서&lt;/p&gt;

&lt;p&gt;However, the process of scaling up ConvNets has never been well understood and there are currently many ways to do it.&lt;br /&gt;
그러나 콘브넷을 확장하는 과정은 쉽게 이해되지 않고 현재 많은방법이 있다&lt;/p&gt;

&lt;p&gt;The most common way is to scale up ConvNets by their depth or width.
가장 흔한 확장방법은 깊이와 너비를 늘리는것이다&lt;/p&gt;

&lt;p&gt;Another less common, but increasingly popular, method is to scale up models by image resolution.&lt;br /&gt;
다른 아직은 흔하진 않지만 점점 유명해지고 있는 방법은 이미지 해상도를 확장하는것이다&lt;/p&gt;

&lt;p&gt;In previous work, it is common to scale only one of the three dimensions - depth, width, and image size.&lt;/p&gt;

&lt;p&gt;Though it is possible to scale two or three dimensions arbitrarily, arbitrary scaling requires tedious manual tuning and still often yields sub-optimal accuracy and efficiency&lt;br /&gt;
그래도 두개혹은세개의 차원은 임의적으로 확장이 가능하다. 임의확장은 지루한 수동튜닝이 필요하고 여전히 정확도와 효율성에있어 최적의 선택이 아니다&lt;/p&gt;

&lt;p&gt;In this paper, we want to study and rethink the process of scaling up ConvNets
이 논문에서 우리는 콘브넷을 확장하는 과정을 연구하고자 한다&lt;/p&gt;

&lt;p&gt;In particular, we investigate the central question : is there a principled method to scale up ConvNets that can achieve better accuracy and efficiency?&lt;br /&gt;
특히 우리는 이 질문에 대해 연구했다
더 높은정확도와 효율성을달성하기 위한 콘브넷에 스케일업에 규칙적인 방법이 있는가&lt;/p&gt;

&lt;p&gt;Our empirical study shows that it is critical to balance all dimensions of network width/depth/resolution, and surprisingly such balance can be achieved by simply scaling each of them with contant ratio.
우리의경험적인 학습은 보여준다
깊이, 너비, 해상도의 밸런스를 유지하는것이 매우 중요하고 놀랍게도 이러한 밸런스는 단순한 각각의 상수 비율로 나타낼 수 있다&lt;/p&gt;

&lt;p&gt;Based on this observation, we propose a simple yet effective compound scaling method.&lt;br /&gt;
이 관측에 기반하여 우리는 간단하고 효과적인 합성 스케일 방법을 제안한다&lt;/p&gt;

&lt;p&gt;Unlike conventional practice that arbitrary scales these factors, our method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients.&lt;br /&gt;
이런 요인을 임의로 조정하던 기존의 방식과는 달리 우리의 방법은 고정된 조정계수의 집합으로 균등하게 조정한다.&lt;/p&gt;

&lt;p&gt;For example, if we want to use $2^N$ times more computational resources, then we can simply increse the network depth by $\alpha^N$, width by $\beta^N$, and image size by $\gamma^N$, where $\alpha,\beta,\gamma$ are constant coefficients determined by a small grid search on the original small model.
예를들면 만약 $2^N$의 많은 컴퓨팅 자원이 있다면 우리는 네트워크를 단순히 $\alpha^N,\beta^N,\gamma^N$ 만큼 증가시킬수 있다 
위 변수들은 원래의 작은 모델에서의 작은 그리드서치에 의해 결정 되는 상수이다.&lt;/p&gt;

&lt;p&gt;intuitively, the compound scaling method makes sense because if the input image is bigger, then the neywork needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.
직관적으로 이 복합조정방법은 말이된다 왜냐면 이미지가 커지면 네트워크는 증가된 필드를 수용하기위해 많은레이어를 필요로 하고 더 큰 이미지에서 좋은패턴을 얻기 위해 더 많은채널을 필요로 한다&lt;/p&gt;

&lt;p&gt;In fact, previous theoretical and empirical results both show that there exists certain relationship between network width and depth, but to our best knowledge, we are the first to empirically quantify the relationship among all three dimensions of network width, depth, and resolution.&lt;br /&gt;
사실 앞의 이론적이고 경험적인 결과 둘다 네트워크의 깊이와 너비에 어떤 관계가 존재하는걸 보여주지만 우리의 최고의 성과는 이 세가지 차원의 관계를 처음으로 정량화한 것이다.&lt;/p&gt;

&lt;p&gt;we demonstrate that our scaling method work well on existing MobileNets and ResNet.&lt;br /&gt;
우리는 우리의 스케일링 방식이 레즈넷과 모바일넷에서 잘 작동하는것을 시연한다&lt;/p&gt;

&lt;p&gt;Notably, the effectiveness of model scaling heavily depends on the baseline network; to go even further, we use neural architecture search to develop a new baseline network, and scale it up to obtain a family of models, called EfficientNets.&lt;br /&gt;
특히 모델의 확장 효과는 베이스라인에 의존한다. 더 나아가 신경망 아키텍쳐를 더 좋은 새로운 베이스모델을 만들기 위해 사용하고 그것으르 스케일하여 만든 모델의 집합을 에피션넷츠라 부른다&lt;/p&gt;

&lt;p&gt;Figure 1 summarizes the ImageNet performance, where our EfficientNets significantly outperform other ConvNets.&lt;br /&gt;
그림1은 이미지넷 퍼포먼스의 요약이고 에피션트넷이 다른 컨브넷을 훨씬 능가하는것을 보여준다&lt;/p&gt;

&lt;p&gt;In particular, out EfficientNet-B7 surpasses the best existing GPipe accuracy, but using 8.4x fewer parameters and running 6.1x faster on inference.&lt;br /&gt;
특히 B7 은 현존하는 최고의 정확도를 넘어섰지만 8.4배 적은 파라미터를 가지고 추론시 6.1배 빠른 속도가 나온다&lt;/p&gt;

&lt;p&gt;Compared to the widely used ResNet-50, our EfficientNet-B4 improves the top-1 accuracy from 76.3 to 82.6 with similar FLOPS&lt;br /&gt;
널리 사용되는 레즈넷 50과 비교하면 B4는 비슷한 연산량에서 탑1 정확도를 8프로나 올렸다&lt;/p&gt;

&lt;p&gt;Besides ImageNet, Efficient Nets also transfer well and achieve state-of-the-art accuracy on 5 out of 8 widely used datasets, while reducing parameters by up to 21x than existing ConvNets&lt;br /&gt;
이미지넷 외에 에피션넷은 또한 널리 사용되는 8개의 데이터세트중 5개에서 최고기록을 달성하면서도 파라미터는 최대 21배까지도 감소시켰다&lt;/p&gt;

&lt;h1 id=&quot;2-related-work&quot;&gt;2. Related Work&lt;/h1&gt;

&lt;h2 id=&quot;convnet-accuracy&quot;&gt;ConvNet Accuracy&lt;/h2&gt;
&lt;p&gt;Since AlexNet won the 2012 ImageNet competition, ConvNets have become incresingly more accurate by going bigger: while the 2014 ImageNet winner GoogleNet achieves 74.8% top-1 accuracy with about 6.8M parameters, the 2017 ImageNet winner SENet achieves 82.7 top-1 accuracy with 145 parameters.
알렉스넷이 2012에 우승한 이후 컨브넷은 커짐으로써 점점 높은 정확도를 달성했다. 2014이미지넷에서 우승한 구글넷은 680만개, 207에서 우승한 SENet은 1억4500백만개의 파라미터를 가졌다&lt;/p&gt;

&lt;p&gt;Recently, GPipe further pushes the state-of-the-art ImageNet top-1 validation accuracy to 84.3% using 557M parameters : it is so big that it can only be trained with a specialized pipeline parallelism library by partitioning the network and spreading each part to a different accelerator&lt;br /&gt;
현재 GPipe는 최고기록 84.3%를 위해 5억5700만개의 파라미터를 사용했다
이것은 매우 커서 다른 가속기로 분산하는 특별한 파이프 병렬 파이프라인 라이브러리를 사용해야만 한다&lt;/p&gt;

&lt;p&gt;While these models are mainly designed for ImageNet, recent studies have shown better ImageNet models also perform better across a variety of transfer learning datasets and other computer vision tasks such as object detection.&lt;br /&gt;
이 모델들이 이미지넷에서 디자인되는 동안, 최근 연구들은 이미지넷모델이 또한 많은 종류의 전이학습이나 오브젝트 디텍션 같은 다른 컴퓨터비전 학습에서도 좋은 성능을 제공하는것을 보여주었다&lt;/p&gt;

&lt;p&gt;Although higher accuracy is critical for many applications, we have already hit the hardware memory limit, and thus further accuracy gain needs better efficiency&lt;br /&gt;
높은 정확도가 많은 어플리케이션에서 중요할지라도 우리는 이미 하드웨어 메모리 제한에 도달했고 그래서 정확성의 증가는 더 좋은 효율이 필요했다.&lt;/p&gt;

&lt;h2 id=&quot;convnet-efficiency&quot;&gt;ConvNet Efficiency&lt;/h2&gt;
&lt;p&gt;Deep ConvNets are often overparameterized. &lt;br /&gt;
심층컨볼넷은 종종 오버파라미터된다.&lt;/p&gt;

&lt;p&gt;Model compression is a common way to reduce model size by trading accuracy for efficiency.&lt;br /&gt;
모델 압축은 효율을 위해 정확도를 낮춰 모델의 크기를 줄이는 흔한 방법이다&lt;/p&gt;

&lt;p&gt;As mobile phones become ubiquitous, it is also common to handcraft efficient mobile-size ConvNets, such as SqueezeNets, MobileNets, and ShuffleNets.&lt;br /&gt;
모바일 폰은 어디에나있다. 그것은 수작업된 효율좋은 모바일사이즈 컨볼넷 또한 흔하다 스퀴즈넷, 모바일넷, 셔플넷같은.&lt;/p&gt;

&lt;p&gt;Recently, neural archiecture search becomes increasingly popular in designing efficient mobile-size ConvNets, and achieves even better efficiency than hand-crafted mobile ConvNets by extensively tuning the network width, depth, convolution kernel types and sizes.&lt;br /&gt;
최근 신경망 아키텍쳐는 효율적인 모바일크기 컨볼넷의 디자인이 인기가 많아지고있고 수작업으로 만들어진 모바일 컨브넷보다 광범위하게 튜닝 가능한 것이 훨씬 더 좋은 성취를 한다.&lt;/p&gt;

&lt;p&gt;However, it is unclear how to apply these techniques for larger models that have much larger design space and much more expensive tuning cost.&lt;br /&gt;
그러나 모델을 크게만드는 테크닉을 적용하는 방법은 아직 모른다.
그래서 큰 디자인을 설계하는데는 매우 많은 튜닝 비용이 필요하다&lt;/p&gt;

&lt;p&gt;In this paper, we aim to study model efficiency for super large ConvNets that surpass state-of-the-art accuracy. To achieve this goal, we resort to model scaling.&lt;br /&gt;
이 논문에서 최고의 정확도를 능가하는 매우 큰 컨볼넷의 효율을 연구하는것에 집중했고 이 목적을 달성하러면 우린 모델 확장에 의존해야 한다&lt;/p&gt;

&lt;h2 id=&quot;model-scaling&quot;&gt;Model Scaling&lt;/h2&gt;
&lt;p&gt;There are many ways to scale a ConvNet for different resource constraints : ResNet can be scaled down or up by adjusting network depth, while WideResNet and MobileNets can be scaled by network width&lt;br /&gt;
다양한 자원의 제약을 위해 컨볼넷을 확장하는 방법은 많이 있다
ResNet은 깊이를 조절하여 스케일 업다운이 가능하고 모바일넷과 와이드레즈넷은 네트워크의 너비를 조정 가능하다&lt;/p&gt;

&lt;p&gt;It is also well-recognized that bigger input image size will help accuracy with the overhead of more FLOPS&lt;br /&gt;
큰 입력 이미지 사이즈가 정확도 를 돕지만 연산량이 늘어난다는것 또한 잘 알려져있다.&lt;/p&gt;

&lt;p&gt;Although prior studies have shown that network deep and width are both important for ConvNets’ expressive power, it still remains an open question of how to effectively scale a ConvNet to achieve better efficiency and accuracy.&lt;br /&gt;
비록 이전의 연구가 컨볼넷의 표현력을 위해 네트워크의 깊이와 넓이 가 모두 중요한 것임을 보여줬지만 여전히 어떻게 높은 효율과 정확도를위해 효과적으로 컨볼넷을 스케일하하는지는 의문으로 남아있다.&lt;/p&gt;

&lt;p&gt;Our work systematically and empirically studies ConvNet scaling for all three dimensions of network width, depth, and resolutions&lt;br /&gt;
우리의 작업은 3가지 기준을 이용한 컨볼넷의 확장을 체계적이고 경험적으로 연구하는것이다&lt;/p&gt;

&lt;h1 id=&quot;3-compound-model-scaling&quot;&gt;3. Compound Model Scaling&lt;/h1&gt;
&lt;p&gt;In this section, we will formulate the scaling problem, study different approaches, and propose our new scaling method&lt;br /&gt;
이 부분에서 스케일 문제를 공식화하고, 다른접근법을 연구하고, 새로운 스케일 메소드를 제안하겠다&lt;/p&gt;

&lt;h2 id=&quot;31-problem-fomulation&quot;&gt;3.1. Problem Fomulation&lt;/h2&gt;
&lt;p&gt;A ConvNet Layer $i$ can be defined as a function : $Y_i = \mathcal{F}_i(X_i)$, where $\mathcal{F}_i$ is the operator, $Y_i$ is output tensor, $X_i$ is input tensor, with tensor shape $\left\langle H_i, W_i, C_i  \right\rangle^1$, where $H_i$ and $W_i$ are spatial dimension and $C_i$ is the channel dimension.&lt;/p&gt;

&lt;p&gt;컨볼넷 레이어 i 는  $Y_i = \mathcal{F}_i(X_i)$ 로 정의된다 F는 오퍼레이터 Y는 아웃풋텐서 X는 인풋텐서로 쉐이프는  $\left\langle H_i, W_i, C_i  \right\rangle^1$ 이고 높이와 너비 , 채널을 의미한다&lt;/p&gt;

&lt;p&gt;A ConvNet $\mathcal{N}$ can be represented by a list of composed layers : $\mathcal{N} = \mathcal{F}&lt;em&gt;k 
\bigodot \cdots \bigodot \mathcal{F}_2 \bigodot \mathcal{F}_1(X_1) = \bigodot&lt;/em&gt;{j=1\cdots k}\mathcal{F}_j(X_1)$&lt;br /&gt;
컨볼넷 N은 위와같이 레이어로 구성된 리스트로 표현가능하다&lt;/p&gt;

&lt;p&gt;In pratice, ConvNet layers are often partitioned into multiple stages and all layers in each stage share the same architecture : for example, ResNet has five stages, and all layers in each stage has the same convolutional type except the first layer performs down-sampling.&lt;br /&gt;
실제로 컨볼레이언느 종종 몇개의 스테이지로 구분되어있고 각 스테이지의 모든 레이어는 같은 아키텍쳐를 공유한다.
예를들면 레즈넷은 5개의 스테이지를 가지고 각 스테이지의 모든 레이어가 다운샘플링을 위한 첫레이어를 제외하고는 같은 컨볼루션 타입을 가진다&lt;/p&gt;

&lt;p&gt;Therefore, we can define a ConvNet as :&lt;br /&gt;
(1)&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\mathcal{N} = \underset{i=1\cdots s}{\bigodot}\mathcal{F}^{L_i}_i(X_{\left\langle H_i,W_i,C_i \right\rangle})&lt;/script&gt; 
Where $\mathcal{F}^{L_i}_i$ denotes layer $\mathcal{F}_i$ is repeated $L_i$ times in stage $i$, $\left\langle H_i,W_i,C_i \right\rangle$ denotes the shape of input tensor $X$ of layer $i$.&lt;br /&gt;
컨볼넷을 위처럼 정의할수도 있는데 여기서 F는 스테이지 i 에서 L만큼 반복된다는 것이고 X는 인풋텐서이다)&lt;/p&gt;

&lt;p&gt;Figure 2(a) illustrate a representative ConvNet, where the spatial dimension is gradually shrunk but the channel dimension is expanded over layers, for example, from initial input shape &amp;lt;224,224,3&amp;gt;, to final output shape&amp;lt;7,7,512&amp;gt;.&lt;br /&gt;
그림 2 는 콘브넷의 표현을 그린것인데 
차원 공간이 점차적으로 눌어들지만 채널은 레이어가갈수록 확장된다 예를들면 input은 224 224 3 이지만 아웃풋은 7 7 512 이다&lt;/p&gt;

&lt;p&gt;Unlike regular ConvNet designs that mostly focus on finding the best layer architecture $\mathcal{F}_i$, model scaling tries to expand the network length $(L_i)$, width $(C_i)$, and/or resulution $(H_i, W_i)$ without changing $\mathcal{F}_i$ predefined in the baseline network. &lt;br /&gt;
최고의 레이어 아키텍쳐를 찾는데 가장 집중된  일반적인 컨볼루션 디자인과는 다르게, 모델 스케일링은 네트워크의 길이,너비, 그리고 해상도를 미리정의된 베이스라인네트워크를 바꾸지 않고모델을 확장하려고 노력한다&lt;/p&gt;

&lt;p&gt;By fixing $\mathcal{F}_i$, model scaling simplifies the design problem for new resource constraints, but it still remains a large design space to explore different $L_i,C_i,H_i,W_i$ for each layer.&lt;br /&gt;
F를 고정하여 모델 확장의 새로운 자원 제약 디자인 문제를 간단하게 한다, 그러나 이것은 여전히 각각 레이어의 다른 변수들의 거대한 탐험 공간이 남아있다&lt;/p&gt;

&lt;p&gt;In order to further reduce the design space, we restrict that all layers must be scaled uniformly with constant ratio.&lt;br /&gt;
디자인 공간을 더 줄이는 방법으로 모든 레이어는 균일한 상수 비율로 스케일 되어야 하는 제한을 걸었다&lt;/p&gt;

&lt;p&gt;Our target is to maximize the model accuracy for any given resource constraints, which can be formulated as an optimization problem
우리의 목표는 최적화 문제를 공식화할 수있는 어떤 자원제약을 주어 모델의 정확도를 최대화 하는 것이다&lt;/p&gt;

&lt;p&gt;(2)
&lt;script type=&quot;math/tex&quot;&gt;\underset{d,w,r}{max}\ \  Accuracy(\mathcal{N}(d,w,r))&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;s.t.\ \ \mathcal{N}(d,w,r) = \underset{i=1\cdots s}{\bigodot}\hat{\mathcal{F}}^{d\cdot \hat{L}_i}_i(X\langle r\cdot \hat{H}_i,r\cdot \hat{W}_i,r\cdot \hat{C}_i \rangle)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\mathrm{Memory}(\mathcal{N}) \le \mathrm{target\_memory}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\mathrm{FLOPS}(\mathcal{N}) \le \mathrm{target\_flops}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where w,d,r are coefficients for scaling network width, depth, and resolution; $\hat{\mathcal{F}}_i,\hat{L}_i,\hat{H}_i,\hat{W}_i,\hat{C}_i$ are predefined parameters in baseline network(see Table 1 as an example)&lt;br /&gt;
wdr은 네트워크의 스케일 계수이고 위 다섯 변수는 베이스라인 네트워크에서 사전 정의된 파라메터이다&lt;/p&gt;

&lt;h2 id=&quot;32-scaling-dimensions&quot;&gt;3.2. Scaling Dimensions&lt;/h2&gt;
&lt;p&gt;The main difficulty of problem 2 is that the optimal $d,w,r$ depend on each other and the values change under different resource constraints. &lt;br /&gt;
(2)번 문제의 가장 어려운점은 dwr의 최적화가 각각 의존하고있고 다른 자원 제약하에서 값이 바뀌는 것이다&lt;/p&gt;

&lt;p&gt;Due to this difficulty, conventional methods mostly scale ConvNets in one of these dimensions :&lt;br /&gt;
이 문제 때문에 관습적인 방법이 대부분 이런 차원중 하나로 확장을 한다&lt;/p&gt;

&lt;h3 id=&quot;depth-d&quot;&gt;Depth ($d$)&lt;/h3&gt;
&lt;p&gt;Scaling network depth is the most common way used by many ConvNets.&lt;br /&gt;
깊이를 조정하는것은 많은 콘브넷에서 가장 흔하게 사용되는 방법이다.&lt;/p&gt;

&lt;p&gt;The intuition is that deeper ConvNet can capture richer and more complex features, and generalize well on new tasks. &lt;br /&gt;
직감적으로 깊은 콘브넷은 많고 더 복잡한 피쳐를 잡아낼 수 있고 새로운 작업에 대한 일반화가 잘된다&lt;/p&gt;

&lt;p&gt;However, deeper networks ard also more difficult to train due to the vanishing gradient problem.&lt;br /&gt;
하지만 깊은 네트워크는 그라디언트 소실 문제로 학습하기 더 어렵다&lt;/p&gt;

&lt;p&gt;Although several techniques, such as skip connections and batch normalization, alleviate the training problem, the accuracy gain of very deep network diminishes : for example, ResNet-1000 has similar accuracy as ResNet-101 even though it has much more layers.&lt;br /&gt;
스킵커넥션이나 배치 노말같은 이 문제를 완화시키는 몇몇 테크닉이 있지만 정확도는 매우깊은 네트워크에선 감소한다. 예를들면 레즈넷1000은 레즈넨 101과 비슷한 정확도를 가진다 매우 많은 레이어를 가졌음에도 불구하고.&lt;/p&gt;

&lt;p&gt;Figure 3(middle) shows our empirical study on scaling a baseline model with different depth coefficient $d$, further suggesting the diminishing accuracy return for very deep ConvNets.&lt;br /&gt;
그림3은 베이스모델을 d에 따른 확장한 실질적인 연구를 보여주고 매우 깊은 컨브넷에서의 정확도 감소를 잘 보여준다&lt;/p&gt;

&lt;h3 id=&quot;width&quot;&gt;Width&lt;/h3&gt;
&lt;p&gt;Scaling network width is commonly used for small size models.&lt;br /&gt;
네트워크의 너비를 조정하는것은 작은 사이즈 모델에서 매우 흔하게 사용된다&lt;/p&gt;

&lt;p&gt;As discussed in (), wider networks tend to be able to capture more fine-grained features and are easier to train. &lt;br /&gt;
넓은 네트워크는 잘 정제된 피쳐를 캐치하는 경향이 있고 학습이 쉬워지게 한다&lt;/p&gt;

&lt;p&gt;However, extremly wide but shallow networks tend to have difficulties in caputring higher level features.&lt;br /&gt;
그러나 과하게 넓어넓지만 얕은 네트워크는 고수준 피쳐를캡쳐하기 어려운 경향이 생긴다.&lt;/p&gt;

&lt;p&gt;Our empirical results in Figure 3 (left) show that the accuracy quickly saturates when networks become much wider with lager $w$&lt;br /&gt;
우리의 실증적 결과는 그림 3에서 보여준다
넓이가 w보다 많이 넓어질때 정확도를 빠르게 포화시키는것을&lt;/p&gt;

&lt;h3 id=&quot;resolution&quot;&gt;Resolution&lt;/h3&gt;

&lt;p&gt;With higher resulution input images, ConvNets can potentially capture more fine-grained patterns.
더 높은 해상도의 이미지와 함께하면 콘브넷은 더 좋은 패턴을 잘 캡쳐할것이다.&lt;/p&gt;

&lt;p&gt;Starting from 224x224 in ConvNets, modern Convnets tend to use 299x299 or 331x331 for better accuracy.&lt;br /&gt;
224에서 시작하여 현재는 더높은정확도를 위해 299 혹은 331 도 사용하는 경향이 있다&lt;/p&gt;

&lt;p&gt;Recently, GPipe achieves state-of-the-art ImageNet accuracy with 480x480 resulution.&lt;br /&gt;
최근 GPipe는 최고기록 달성을 위해 480을 적용했다&lt;/p&gt;

&lt;p&gt;Higher resulutions, such as 600x600, are also widely used in object detection ConvNets.&lt;br /&gt;
600처럼 더높은 해상도 또한 오브젝트 디텍션에서 널리 사용된다&lt;/p&gt;

&lt;p&gt;Figure 3 shows the results of scaling network resolusions, where indeed higher resolutions improve accuracy, but the accuracy gain diminishes for very high resolutions ($r=1.0$ denotes resolution 224x224 and $r=2.5$ denotes resolution 560x560)&lt;br /&gt;
그림3은 해상도의 조절 결과를 보여준다
실제로 높은 해상도는 정확도를 향상시켰으나 매우 높은 해상도에서는 정확도의 증가가 줄어들었다.
(1은 224, 2.5는 560을 의미한다)&lt;/p&gt;

&lt;h1&gt;#&lt;/h1&gt;

&lt;p&gt;The above analyses lead us to the first observation&lt;br /&gt;
위의 관측은 우리를 첫번째 관찰로 이끈다&lt;/p&gt;

&lt;h3 id=&quot;observation-1&quot;&gt;Observation 1&lt;/h3&gt;
&lt;p&gt;Scaling up any dimension of network width, depth, or resolution improves accuracy, but the accuracy gain diminishes for bigger models.
3가지 차원을 확장하는것은 정확도를 향상시키지만
모델이 커질수록 정확도 증가량은 감소한다.&lt;/p&gt;

&lt;h2 id=&quot;33-compound-scaling&quot;&gt;3.3. Compound Scaling&lt;/h2&gt;
&lt;p&gt;We empirically observe that different scaling dimensions are not independent.&lt;/p&gt;

&lt;p&gt;Intuitively, for higher resolution images, we should increase network depth, such that the larger receptive fields can help capture similar features that include more pixels in bigger images.&lt;br /&gt;
직관적으로 높은 해상도의 이미지에서 네트워크의 깊이를 증가시켜야한다 넓은 수용 공간은 큰 이미지의 많은픽셀을 가진 비슷한 피쳐들을 잘 캐치하도록 도와줄수있다&lt;/p&gt;

&lt;p&gt;Correspondingly, we should also increase network width when resolution is higher, in order to capture more fine-grained patterns with more pixels in high resolution images.&lt;br /&gt;
비슷하게, 해상도가 올라갈땐 네트워크의 너비또한  증가시켜야 하는데, 고생도 이미지에에 많은 픽셀에서 더 좋은 정제된 패턴을 캐치하기 위해서이다&lt;/p&gt;

&lt;p&gt;These intuitions suggest that we need to coordinate and balance different scaling dimensions rather than conventional single-dimension scaling.&lt;br /&gt;
이 직관은 우리가 기존의 1개차원을 스케일하는것보단 차원들의 균형있게 조정을 해야하는것을 제안한다.&lt;/p&gt;

&lt;p&gt;To validate our intuitions, we compare width scaling under different network depths and resolutions, as shown in Figure 4. &lt;br /&gt;
우리의 직관을입증하기위해 그림 4에서 보여주듯이 다양한 네트워크에서 너비를 조정하는것을 비교한다.&lt;/p&gt;

&lt;p&gt;If we only scale network width $w$ without changing depth ($d=1.0$) and resolution ($r=1.0$), the accuracy saturates quickly.
만약 w를 d와 r을 고정하고 변화시키면 정확도는 빠르게 수렴한다&lt;/p&gt;

&lt;p&gt;With deeper ($d=2.0$) and higher resolution ($r=2.0$), width scaling achieves much better accuracy under the same FLOPS cost. 
깊은모델과 고해상도와 함께하면, 같은 FLOPS 비용으로 더 높은 정확도를 달성한다.&lt;/p&gt;

&lt;p&gt;These results lead us to the second observation :
이결과는 우리를 두번째 관측으로 이끈다.&lt;/p&gt;

&lt;h3 id=&quot;observation-2&quot;&gt;Observation 2&lt;/h3&gt;
&lt;p&gt;In order to pursue better accuracy and efficiency, it is critical to balance all dimensions of network width, depth, and resolution during ConvNet scaling.&lt;br /&gt;
더 좋은 정확도와 효율성을 추구하기 위해서는 컨브넷을 확장할때 3가지 차원의 밸런스가 매우 중요하다.&lt;/p&gt;

&lt;p&gt;In fact, a few prior work have already tried to arbitrarily balance network width and depth, but they all require tedious manual tuning
사실상 몇몇 기존의 작업들은 이미 임의로 깊이와 뎁스의 균형을 시도하였지만 지루한 정해진 튜닝이 필요하다.&lt;/p&gt;

&lt;p&gt;In this paper, we propose a new compound scaling method, which use a compound coefficient $\phi$ to uniformly scales network width, depth, and resolution in a principled way :&lt;br /&gt;
이 논문에서 새로운 복합 스케일 메소드를 제안한다
합성계수 $phi$를 사용해 균일하게 네트워크의 차원을 조정하는. 아래 의 법칙에 의하여.&lt;/p&gt;

&lt;p&gt;(3)
&lt;script type=&quot;math/tex&quot;&gt;\mathrm{depth:} d =\alpha^\phi \\
 \mathrm{width:} w =\beta^\phi \\
 \mathrm{resolution:} r =\gamma^\phi \\
 \mathrm{s.t.}\quad\alpha\ \cdot\ \beta^2\ \cdot\ \gamma^2 \approx 2\\
\alpha\ge1,\ \beta\ge1,\ \gamma\ge1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where $\alpha, \beta, \gamma$ are constants that can be determined by a small grid search.&lt;br /&gt;
세 변수는 작은 그리드 서치로 결정할수 있다.&lt;/p&gt;

&lt;p&gt;Intuitively, $phi$ is a user-specified coefficient that controls how many more resources are available for model scaling, while $\alpha, \beta, \gamma$ specify how to assign these extra resources to network width, depth, and resolution respectively.
직관적으로 파이는 사용자지정 계수로 모델을 스케일하기위해 추가 리소스를 각각 3가지에 할당하는 방법을 명시한다&lt;/p&gt;

&lt;p&gt;Notably, the FLOPS of a regular covolution ops is proportional to $d, w^2, r^2$, i.e., doubling network depth will double FLOPS, but doubling network width or resolution woll increase FLOPS by four times.
특히 규칙적인 컨볼루션 에서의 FLOPS는 3가지 변수에 비례한다 다시말하면 두배의 깊이의 네트워크는 FLOPS를 두배로 올리고, 두배의 너비와 해상도는 FLOPS를 네배로 올린다.&lt;/p&gt;

&lt;p&gt;Since convolution ops usually dominate the computation cost in ConvNets, scaling a ConvNet with equation 3 will approximately increase total FLOPS by $(\alpha\ \beta^2\ \gamma^2)^\phi$.&lt;br /&gt;
컨볼루션 부분이 대부분의 계산비용을 지불하므로 방정식3을 사용하여 컨브넷을 확장하면 토탈 FPOPS가 약 $(\alpha\ \beta^2\ \gamma^2)^\phi$ 만큼 오른다&lt;/p&gt;

&lt;p&gt;In this paper, we constraint $\alpha\ \cdot\  \beta^2\ \cdot\ \gamma^2 \approx 2$ such that for any new $\phi$, the total FLOPS will approximately incrtease by $2^\phi$.
이 논문에서 우리는 새로운 어떤 파이에서도 위 방정식으로 제약을하면 총 FLOPS는 약 $2^\phi$ 증가한다&lt;/p&gt;

&lt;h1 id=&quot;4-efficientnet-acchitecture&quot;&gt;4. EfficientNet Acchitecture&lt;/h1&gt;

&lt;p&gt;Since model scaling does not change layer operations $\hat{\mathcal{F}}_i$ in baseline network, having a good baseline network is also critical. &lt;br /&gt;
모델확장은 베이스라인 네트워크에 있는 레이어는 바꾸지 않기 때문에 좋은 베이스라인 모델을 고르는것 또한 매우 중요하다&lt;/p&gt;

&lt;p&gt;We will evaluate our scaling method using existing Convnets, but in order to better demonstrate the effectiveness of our scaling method, we have also developed a new mobile-size baseline, called EfficientNet.&lt;br /&gt;
우리의 확장 메소드를 현존하는 컨브넷을 통해 평가할 것인데 스키일링 방법의 효과를 더 잘 보여주기위해 에피션넷이라 불리는 새로운 모바일 사이즈 베이스라인을 개발하였다.&lt;/p&gt;

&lt;p&gt;Inspired by (Tan et al., 2019), we develop our baseline network by leveraging a multi-objective neural architecture search that optimizes both accuracy and FLOPS. 
()에서 영감을 받았는데, 베이스라인 네트워크는 정확도와 FLOPS 양쪽의 최적화를 찾는 다목적 신경 구조를 활용하였다&lt;/p&gt;

&lt;p&gt;Specifically, we use the same search space as (Tan et al., 2019), and use $ACC(m) \times [FLOPS(m)/T]^w$ as the optimization goal, where $ACC(m)$ and $FLOPS(m)$ denote the accuracy and FLOPS of model $m$, $T$ is the target FLOPS and $w=-0.07$ is a hyperparameter for controlling the trade-off between accuracy and FLOPS.&lt;br /&gt;
구체적으로 우린 위 논문과 같은 검색 공간을 사용하였다
위 식에서 $ACC(m)$ 와 $FLOPS(m)$는 우리 모델의 정확도와 FLOPS, T 는 목표 FLOPS이고 $w$는 정확도와 FLOPS를 Trade-off 하는 하이퍼 파라미터이다&lt;/p&gt;

&lt;p&gt;Unlike(Tan et al., 2019; Cai et al., 2019), here we optimize FLOPS rather than latency since we are not targeting any specific hardware device.&lt;br /&gt;
위 논문과는 다르게 루린 특정한 하드웨어장치를 대상으로 하지 않기 때문에 대기시간보단 FLOPS를 최적화 하였다.&lt;/p&gt;

&lt;p&gt;Our search produces an efficient network, which we name EfficientNet-B0.&lt;/p&gt;

&lt;p&gt;Since we use the same search space as (Tan et al., 2019), the architecture is similar to MnasNet, except our EfficientNet-B0 is slightly bigger due to the larger FLOPS target (our FLOPS target is 400M)
같은 검색공간을 사용했기 때문에 MnasNet과 비슷한 아키텍쳐를 가지고 있고, 에피션넷을 제외하고는 점차적으로 커지는&lt;/p&gt;

&lt;p&gt;Table 1 shows the architecture of EfficientNet-B0.&lt;br /&gt;
테이블1은 에피넷B0의 구조를 보여준다&lt;/p&gt;

&lt;p&gt;Its main building block is mobile inverted bottleneck MBConv, to which we also add squeeze-and-excitation optimization.&lt;br /&gt;
메인 블록의 구성은 인버트바틀넥을 사용한 BMConv, 그리고 Squeeze-and-excitation 최적화를 더하였다.&lt;/p&gt;

&lt;p&gt;Starting from the baseline EfficientNet-B0, we apply our compound scaling method to scale it up with two steps :&lt;br /&gt;
베이스라인 에피션넷으로 시작해 우리의 메소드를  두가지 스텝에 따라 적용하였다&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;STEP 1 : we first fix $\phi=1$, assuming twice more resources available, and do a smamll grid sarch of $\alpha,\beta,\gamma$ based of Equation 2 and 3. In particular, we fine the best values for EfficientNet-B0 are $\alpha=1.2, \beta=1.1, \gamma=1.15, under constraint of $\alpha\ \cdot\ \beta^2\ \cdot\ \gamma^2 \approx 2$.&lt;br /&gt;
우선 파이를 1로 고정하고 두배의 자원이 더있다고 가정하였다, 그리고 방정식 2와 3을따라 작은 그리드서치를 하였다. 특히 우리는 위와같은 최고의 값을 찾았다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;STEP 2 : we then fix $\alpha, \beta,\gamma$ as constants and scale up baseline network with different $\phi$ using Equation 3, to obtain EfficientNet-B1 to B7 (Details in Table 2)&lt;br /&gt;
다음으론 알벳감을 고정하고 방정식 3을따라 다른 파이를주면서 스케일을 확장했다. 그래서 B1부터 B7까지 얻었다(표2에 설명)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notably, it is possible to achieve even better perfomance by searching for $\alpha,\beta,\gamma$ directly around a large model, but the search cost becomes prohibitively more expensive on larger models.
특히 더 좋은 퍼포먼스를 이루기 위해 큰모델에서 직접 알벳감을 찾는것도 가능하지만 큰 모델에서는 하면 안될정도로 큰 비용이 들어간다&lt;/p&gt;

&lt;p&gt;Our method solves this issue by only doing search once on the small baseline network(step1), and then user the same scaling coefficients for all other models(step2)&lt;br /&gt;
우리는 이 방법을 작은 베이스라인 네트워크에서 찾는것으로 이 문제를 해결했고 같은 확장계수를 이용해 다른모델을 만들었다.&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">Abstract</summary></entry><entry><title type="html">GAN 시작</title><link href="http://localhost:4000/deeplearning/2019/06/28/gan.html" rel="alternate" type="text/html" title="GAN 시작" /><published>2019-06-28T10:05:05+09:00</published><updated>2019-06-28T10:05:05+09:00</updated><id>http://localhost:4000/deeplearning/2019/06/28/gan</id><content type="html" xml:base="http://localhost:4000/deeplearning/2019/06/28/gan.html">&lt;h1 id=&quot;gan&quot;&gt;GAN?&lt;/h1&gt;

&lt;p&gt;Generative Adversarial Network&lt;/p&gt;

&lt;p&gt;적대적 생성 네트워크?&lt;br /&gt;
한마디로 생성기 Generator 와 판별기 Discriminator 가 서로 배틀을 하며 서로의 능력을키워가는….? 
뭐 이런 네트워크다.
들어보면 이게 뭔가 하겠지만 막상 내부구조를보면 아 이런거구나 하는 느낌이 오는?&lt;/p&gt;

&lt;p&gt;일단 봅시다&lt;/p&gt;

&lt;p&gt;Generator 는 1차원벡터인 Noise(균등분포로 랜덤생성)를 입력받아 해당 이미지를 출력하고 Discriminator는 이미지를 입력받아 그것이 진짜인지 가짜인지 여부를 Sigmoid로 출력한다&lt;/p&gt;

&lt;p&gt;간단히하자면 Generator에 노이즈를 입력해서 만든 Fake Image 를 Discriminator에 넣고 이것의 결과가 0이되도록 , 그리고 실제 이미지를 Discriminator 에 넣고 이것의 결과는 1이 되도록 학습시키는것이다.&lt;/p&gt;

&lt;p&gt;이 학습이 반복되면 Generator는 점점 실제에 가까운 이미지를 생성해내게 되고 Discriminator는 두 그림의 차이를 점점더 세밀하게 찾아내게 된다. 하지만 점점 가짜 이미지는 실제 이미지와 유사해지고 결국은 판별을 못하게 되는것이다.&lt;/p&gt;

&lt;p&gt;궁극적 목표는 바로 이 Discriminator 가 가짜 이미지와 진짜 이미지를 판별할 능력이 0.5 (찍는것과 동일)로 수렴하게 만드는것!&lt;/p&gt;

&lt;p&gt;실제 이 방식이 위와같은 수렴을 하게 만든다는것도 증명을 해놨는데 수식이 어렵다. 나중에 시간나면 봐야지…
생각보다 구조는 간단하다
문제는 엄청난수의 하이퍼파라미터들에 정확한 값을 넣지않으면 학습이 잘 되지않는다……&lt;/p&gt;

&lt;h1 id=&quot;dcgan&quot;&gt;DCGAN&lt;/h1&gt;
&lt;p&gt;Deep Convolution GAN &lt;br /&gt;
이름만 봐선 뭔지…&lt;/p&gt;

&lt;p&gt;위에 GAN의경우 불안정함을 많이 보였다고 한다.
실제로 해봐도 진짜 학습하기 어렵…
이것의 안정화를 위해 나온 방식이라 보면되는데
현재 대부분 모델은 대부분 다 이것을 기반으로 한다고 한다.&lt;/p&gt;

&lt;p&gt;자 그럼 살펴보자.&lt;/p&gt;

&lt;p&gt;일단 특징?&lt;br /&gt;
어떤 상황에서도 정말 안정적으로 학습이됨.&lt;br /&gt;
생성된 벡터가 산술연산이 가능
필터 시각화 가능
이미지 분류 성능이 매우 높음&lt;/p&gt;

&lt;p&gt;외워서 만든 이미지가 아님을 보여줘야함
latent space 에서 움직일때 변화가 부드러워야함&lt;/p&gt;

&lt;p&gt;아키텍쳐 가이드라인&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Discriminator에서 모든 풀링 레이어는 스트라이드된 컨볼루션으로 교체&lt;/li&gt;
  &lt;li&gt;Generator에선 Fractinal 스트라이드 컨볼루션으로 교체&lt;/li&gt;
  &lt;li&gt;양쪽에 전부 배치노말 사용, Generator의 output과 discriminator의 인풋은 제외&lt;/li&gt;
  &lt;li&gt;FCL 제거&lt;/li&gt;
  &lt;li&gt;Generator에선 Relu 사용&lt;/li&gt;
  &lt;li&gt;Discriminator에 모든 레이어에 LeakyRelu 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Maxpooling 을 제거하여 모두 미분가능하게 변화&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[http://jaejunyoo.blogspot.com/]&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">GAN?</summary></entry><entry><title type="html">새 커널 처음부터 해보기</title><link href="http://localhost:4000/kaggle/2019/06/28/kaggle-new-kernel.html" rel="alternate" type="text/html" title="새 커널 처음부터 해보기" /><published>2019-06-28T10:05:05+09:00</published><updated>2019-06-28T10:05:05+09:00</updated><id>http://localhost:4000/kaggle/2019/06/28/kaggle-new-kernel</id><content type="html" xml:base="http://localhost:4000/kaggle/2019/06/28/kaggle-new-kernel.html">&lt;p&gt;#&lt;/p&gt;</content><author><name>Aru</name></author><summary type="html">#</summary></entry></feed>